{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU via MPS (Apple Silicon)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU via CUDA:', torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('Using GPU via MPS (Apple Silicon)')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')\n",
    "\n",
    "# Use device like this:\n",
    "# model.to(device)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Load miditok tokenizer\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI.from_pretrained(\"tokenizer.json\")\n",
    "\n",
    "# ----- Dataset -----\n",
    "class PairedMIDIDataset(Dataset):\n",
    "    def __init__(self, right_dir: Path, left_dir: Path, max_len=1024):\n",
    "        self.right_files = sorted(right_dir.glob(\"*.json\"))\n",
    "        self.left_files = sorted(left_dir.glob(\"*.json\"))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.right_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.right_files[idx]) as f:\n",
    "            right = json.load(f)[:self.max_len]\n",
    "        with open(self.left_files[idx]) as f:\n",
    "            left = json.load(f)[:self.max_len]\n",
    "\n",
    "        return torch.tensor(right), torch.tensor(left)\n",
    "\n",
    "# ----- Collate function -----\n",
    "def collate_fn(batch):\n",
    "    right_batch, left_batch = zip(*batch)\n",
    "    #right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
    "    #left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n",
    "    right_batch = [\n",
    "        seq.clone().detach() if isinstance(seq, torch.Tensor) else torch.tensor(seq, dtype=torch.long)\n",
    "        for seq in right_batch\n",
    "    ]\n",
    "    left_batch = [\n",
    "        seq.clone().detach() if isinstance(seq, torch.Tensor) else torch.tensor(seq, dtype=torch.long)\n",
    "        for seq in left_batch\n",
    "    ]\n",
    "\n",
    "    pad_token_id = tokenizer[\"PAD_None\"]  # Use string-based access here\n",
    "    right_padded = nn.utils.rnn.pad_sequence(right_batch, batch_first=True, padding_value=pad_token_id)\n",
    "    left_padded = nn.utils.rnn.pad_sequence(left_batch, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return right_padded, left_padded\n",
    "\n",
    "\n",
    "\n",
    "# ----- Dataloader -----\n",
    "right_json_dir = Path(\"tokenized_json/right_hand\")\n",
    "left_json_dir = Path(\"tokenized_json/left_hand\")\n",
    "\n",
    "dataset = PairedMIDIDataset(right_json_dir, left_json_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ----- Model: Simple Transformer -----\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        memory = self.encoder(src_emb.transpose(0, 1), src_mask)\n",
    "        out = self.decoder(tgt_emb.transpose(0, 1), memory, tgt_mask)\n",
    "        logits = self.fc_out(out.transpose(0, 1))\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID in dataset: 897\n",
      "Vocab size from tokenizer: 898\n",
      "Num valid token IDs: 249\n"
     ]
    }
   ],
   "source": [
    "all_ids = []\n",
    "for right, left in dataloader.dataset:\n",
    "    all_ids.extend(right)\n",
    "    all_ids.extend(left)\n",
    "\n",
    "print(f\"Max token ID in dataset: {max(all_ids)}\")\n",
    "print(f\"Vocab size from tokenizer: {len(tokenizer)}\")\n",
    "\n",
    "valid_token_id_set = set(t.item() for t in all_ids)\n",
    "valid_token_ids = torch.tensor(list(valid_token_id_set), dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Num valid token IDs: {len(valid_token_id_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 4.1180\n",
      "Loss improved from inf to 4.1180. Saving model...\n",
      "Epoch 2 - Loss: 2.5016\n",
      "Loss improved from 4.1180 to 2.5016. Saving model...\n",
      "Epoch 3 - Loss: 2.0830\n",
      "Loss improved from 2.5016 to 2.0830. Saving model...\n",
      "Epoch 4 - Loss: 1.9177\n",
      "Loss improved from 2.0830 to 1.9177. Saving model...\n",
      "Epoch 5 - Loss: 1.8210\n",
      "Loss improved from 1.9177 to 1.8210. Saving model...\n",
      "Epoch 6 - Loss: 1.7517\n",
      "Loss improved from 1.8210 to 1.7517. Saving model...\n",
      "Epoch 7 - Loss: 1.6809\n",
      "Loss improved from 1.7517 to 1.6809. Saving model...\n",
      "Epoch 8 - Loss: 1.6677\n",
      "Loss improved from 1.6809 to 1.6677. Saving model...\n",
      "Epoch 9 - Loss: 1.6440\n",
      "Loss improved from 1.6677 to 1.6440. Saving model...\n",
      "Epoch 10 - Loss: 1.6152\n",
      "Loss improved from 1.6440 to 1.6152. Saving model...\n",
      "Epoch 11 - Loss: 1.5966\n",
      "Loss improved from 1.6152 to 1.5966. Saving model...\n",
      "Epoch 12 - Loss: 1.5770\n",
      "Loss improved from 1.5966 to 1.5770. Saving model...\n",
      "Epoch 13 - Loss: 1.5686\n",
      "Loss improved from 1.5770 to 1.5686. Saving model...\n",
      "Epoch 14 - Loss: 1.5273\n",
      "Loss improved from 1.5686 to 1.5273. Saving model...\n",
      "Epoch 15 - Loss: 1.5477\n",
      "Epoch 16 - Loss: 1.5469\n",
      "Epoch 17 - Loss: 1.5150\n",
      "Loss improved from 1.5273 to 1.5150. Saving model...\n",
      "Epoch 18 - Loss: 1.5078\n",
      "Loss improved from 1.5150 to 1.5078. Saving model...\n",
      "Epoch 19 - Loss: 1.4999\n",
      "Loss improved from 1.5078 to 1.4999. Saving model...\n",
      "Epoch 20 - Loss: 1.4955\n",
      "Loss improved from 1.4999 to 1.4955. Saving model...\n",
      "Epoch 21 - Loss: 1.4859\n",
      "Loss improved from 1.4955 to 1.4859. Saving model...\n",
      "Epoch 22 - Loss: 1.4722\n",
      "Loss improved from 1.4859 to 1.4722. Saving model...\n",
      "Epoch 23 - Loss: 1.4589\n",
      "Loss improved from 1.4722 to 1.4589. Saving model...\n",
      "Epoch 24 - Loss: 1.4587\n",
      "Loss improved from 1.4589 to 1.4587. Saving model...\n",
      "Epoch 25 - Loss: 1.4529\n",
      "Loss improved from 1.4587 to 1.4529. Saving model...\n",
      "Epoch 26 - Loss: 1.4419\n",
      "Loss improved from 1.4529 to 1.4419. Saving model...\n",
      "Epoch 27 - Loss: 1.4268\n",
      "Loss improved from 1.4419 to 1.4268. Saving model...\n",
      "Epoch 28 - Loss: 1.4294\n",
      "Epoch 29 - Loss: 1.4185\n",
      "Loss improved from 1.4268 to 1.4185. Saving model...\n",
      "Epoch 30 - Loss: 1.3984\n",
      "Loss improved from 1.4185 to 1.3984. Saving model...\n",
      "Epoch 31 - Loss: 1.3913\n",
      "Loss improved from 1.3984 to 1.3913. Saving model...\n",
      "Epoch 32 - Loss: 1.3892\n",
      "Loss improved from 1.3913 to 1.3892. Saving model...\n",
      "Epoch 33 - Loss: 1.3762\n",
      "Loss improved from 1.3892 to 1.3762. Saving model...\n",
      "Epoch 34 - Loss: 1.3820\n",
      "Epoch 35 - Loss: 1.3712\n",
      "Loss improved from 1.3762 to 1.3712. Saving model...\n",
      "Epoch 36 - Loss: 1.3417\n",
      "Loss improved from 1.3712 to 1.3417. Saving model...\n",
      "Epoch 37 - Loss: 1.3370\n",
      "Loss improved from 1.3417 to 1.3370. Saving model...\n",
      "Epoch 38 - Loss: 1.3167\n",
      "Loss improved from 1.3370 to 1.3167. Saving model...\n",
      "Epoch 39 - Loss: 1.3047\n",
      "Loss improved from 1.3167 to 1.3047. Saving model...\n",
      "Epoch 40 - Loss: 1.2929\n",
      "Loss improved from 1.3047 to 1.2929. Saving model...\n",
      "Epoch 41 - Loss: 1.2733\n",
      "Loss improved from 1.2929 to 1.2733. Saving model...\n",
      "Epoch 42 - Loss: 1.2593\n",
      "Loss improved from 1.2733 to 1.2593. Saving model...\n",
      "Epoch 43 - Loss: 1.2344\n",
      "Loss improved from 1.2593 to 1.2344. Saving model...\n",
      "Epoch 44 - Loss: 1.2027\n",
      "Loss improved from 1.2344 to 1.2027. Saving model...\n",
      "Epoch 45 - Loss: 1.1912\n",
      "Loss improved from 1.2027 to 1.1912. Saving model...\n",
      "Epoch 46 - Loss: 1.1435\n",
      "Loss improved from 1.1912 to 1.1435. Saving model...\n",
      "Epoch 47 - Loss: 1.1379\n",
      "Loss improved from 1.1435 to 1.1379. Saving model...\n",
      "Epoch 48 - Loss: 1.1211\n",
      "Loss improved from 1.1379 to 1.1211. Saving model...\n",
      "Epoch 49 - Loss: 1.1085\n",
      "Loss improved from 1.1211 to 1.1085. Saving model...\n",
      "Epoch 50 - Loss: 1.0807\n",
      "Loss improved from 1.1085 to 1.0807. Saving model...\n",
      "Epoch 51 - Loss: 1.0526\n",
      "Loss improved from 1.0807 to 1.0526. Saving model...\n",
      "Epoch 52 - Loss: 1.0419\n",
      "Loss improved from 1.0526 to 1.0419. Saving model...\n",
      "Epoch 53 - Loss: 1.0321\n",
      "Loss improved from 1.0419 to 1.0321. Saving model...\n",
      "Epoch 54 - Loss: 1.0098\n",
      "Loss improved from 1.0321 to 1.0098. Saving model...\n",
      "Epoch 55 - Loss: 0.9985\n",
      "Loss improved from 1.0098 to 0.9985. Saving model...\n",
      "Epoch 56 - Loss: 0.9834\n",
      "Loss improved from 0.9985 to 0.9834. Saving model...\n",
      "Epoch 57 - Loss: 0.9669\n",
      "Loss improved from 0.9834 to 0.9669. Saving model...\n",
      "Epoch 58 - Loss: 0.9494\n",
      "Loss improved from 0.9669 to 0.9494. Saving model...\n",
      "Epoch 59 - Loss: 0.9423\n",
      "Loss improved from 0.9494 to 0.9423. Saving model...\n",
      "Epoch 60 - Loss: 0.9331\n",
      "Loss improved from 0.9423 to 0.9331. Saving model...\n",
      "Epoch 61 - Loss: 0.9126\n",
      "Loss improved from 0.9331 to 0.9126. Saving model...\n",
      "Epoch 62 - Loss: 0.9049\n",
      "Loss improved from 0.9126 to 0.9049. Saving model...\n",
      "Epoch 63 - Loss: 0.9010\n",
      "Loss improved from 0.9049 to 0.9010. Saving model...\n",
      "Epoch 64 - Loss: 0.8856\n",
      "Loss improved from 0.9010 to 0.8856. Saving model...\n",
      "Epoch 65 - Loss: 0.8748\n",
      "Loss improved from 0.8856 to 0.8748. Saving model...\n",
      "Epoch 66 - Loss: 0.8682\n",
      "Loss improved from 0.8748 to 0.8682. Saving model...\n",
      "Epoch 67 - Loss: 0.8549\n",
      "Loss improved from 0.8682 to 0.8549. Saving model...\n",
      "Epoch 68 - Loss: 0.8473\n",
      "Loss improved from 0.8549 to 0.8473. Saving model...\n",
      "Epoch 69 - Loss: 0.8317\n",
      "Loss improved from 0.8473 to 0.8317. Saving model...\n",
      "Epoch 70 - Loss: 0.8264\n",
      "Loss improved from 0.8317 to 0.8264. Saving model...\n",
      "Epoch 71 - Loss: 0.8183\n",
      "Loss improved from 0.8264 to 0.8183. Saving model...\n",
      "Epoch 72 - Loss: 0.8047\n",
      "Loss improved from 0.8183 to 0.8047. Saving model...\n",
      "Epoch 73 - Loss: 0.7987\n",
      "Loss improved from 0.8047 to 0.7987. Saving model...\n",
      "Epoch 74 - Loss: 0.7855\n",
      "Loss improved from 0.7987 to 0.7855. Saving model...\n",
      "Epoch 75 - Loss: 0.7779\n",
      "Loss improved from 0.7855 to 0.7779. Saving model...\n",
      "Epoch 76 - Loss: 0.7785\n",
      "Epoch 77 - Loss: 0.7664\n",
      "Loss improved from 0.7779 to 0.7664. Saving model...\n",
      "Epoch 78 - Loss: 0.7546\n",
      "Loss improved from 0.7664 to 0.7546. Saving model...\n",
      "Epoch 79 - Loss: 0.7499\n",
      "Loss improved from 0.7546 to 0.7499. Saving model...\n",
      "Epoch 80 - Loss: 0.7388\n",
      "Loss improved from 0.7499 to 0.7388. Saving model...\n",
      "Epoch 81 - Loss: 0.7379\n",
      "Loss improved from 0.7388 to 0.7379. Saving model...\n",
      "Epoch 82 - Loss: 0.7309\n",
      "Loss improved from 0.7379 to 0.7309. Saving model...\n",
      "Epoch 83 - Loss: 0.7199\n",
      "Loss improved from 0.7309 to 0.7199. Saving model...\n",
      "Epoch 84 - Loss: 0.7170\n",
      "Loss improved from 0.7199 to 0.7170. Saving model...\n",
      "Epoch 85 - Loss: 0.6905\n",
      "Loss improved from 0.7170 to 0.6905. Saving model...\n",
      "Epoch 86 - Loss: 0.6905\n",
      "Loss improved from 0.6905 to 0.6905. Saving model...\n",
      "Epoch 87 - Loss: 0.7060\n",
      "Epoch 88 - Loss: 0.6877\n",
      "Loss improved from 0.6905 to 0.6877. Saving model...\n",
      "Epoch 89 - Loss: 0.6758\n",
      "Loss improved from 0.6877 to 0.6758. Saving model...\n",
      "Epoch 90 - Loss: 0.6720\n",
      "Loss improved from 0.6758 to 0.6720. Saving model...\n",
      "Epoch 91 - Loss: 0.6597\n",
      "Loss improved from 0.6720 to 0.6597. Saving model...\n",
      "Epoch 92 - Loss: 0.6602\n",
      "Epoch 93 - Loss: 0.6482\n",
      "Loss improved from 0.6597 to 0.6482. Saving model...\n",
      "Epoch 94 - Loss: 0.6458\n",
      "Loss improved from 0.6482 to 0.6458. Saving model...\n",
      "Epoch 95 - Loss: 0.6415\n",
      "Loss improved from 0.6458 to 0.6415. Saving model...\n",
      "Epoch 96 - Loss: 0.6425\n",
      "Epoch 97 - Loss: 0.6276\n",
      "Loss improved from 0.6415 to 0.6276. Saving model...\n",
      "Epoch 98 - Loss: 0.6235\n",
      "Loss improved from 0.6276 to 0.6235. Saving model...\n",
      "Epoch 99 - Loss: 0.6229\n",
      "Loss improved from 0.6235 to 0.6229. Saving model...\n",
      "Epoch 100 - Loss: 0.6094\n",
      "Loss improved from 0.6229 to 0.6094. Saving model...\n",
      "Epoch 101 - Loss: 0.6121\n",
      "Epoch 102 - Loss: 0.5988\n",
      "Loss improved from 0.6094 to 0.5988. Saving model...\n",
      "Epoch 103 - Loss: 0.6028\n",
      "Epoch 104 - Loss: 0.5914\n",
      "Loss improved from 0.5988 to 0.5914. Saving model...\n",
      "Epoch 105 - Loss: 0.5915\n",
      "Epoch 106 - Loss: 0.5811\n",
      "Loss improved from 0.5914 to 0.5811. Saving model...\n",
      "Epoch 107 - Loss: 0.5746\n",
      "Loss improved from 0.5811 to 0.5746. Saving model...\n",
      "Epoch 108 - Loss: 0.5706\n",
      "Loss improved from 0.5746 to 0.5706. Saving model...\n",
      "Epoch 109 - Loss: 0.5660\n",
      "Loss improved from 0.5706 to 0.5660. Saving model...\n",
      "Epoch 110 - Loss: 0.5652\n",
      "Loss improved from 0.5660 to 0.5652. Saving model...\n",
      "Epoch 111 - Loss: 0.5545\n",
      "Loss improved from 0.5652 to 0.5545. Saving model...\n",
      "Epoch 112 - Loss: 0.5521\n",
      "Loss improved from 0.5545 to 0.5521. Saving model...\n",
      "Epoch 113 - Loss: 0.5494\n",
      "Loss improved from 0.5521 to 0.5494. Saving model...\n",
      "Epoch 114 - Loss: 0.5436\n",
      "Loss improved from 0.5494 to 0.5436. Saving model...\n",
      "Epoch 115 - Loss: 0.5434\n",
      "Loss improved from 0.5436 to 0.5434. Saving model...\n",
      "Epoch 116 - Loss: 0.5360\n",
      "Loss improved from 0.5434 to 0.5360. Saving model...\n",
      "Epoch 117 - Loss: 0.5345\n",
      "Loss improved from 0.5360 to 0.5345. Saving model...\n",
      "Epoch 118 - Loss: 0.5304\n",
      "Loss improved from 0.5345 to 0.5304. Saving model...\n",
      "Epoch 119 - Loss: 0.5353\n",
      "Epoch 120 - Loss: 0.5279\n",
      "Loss improved from 0.5304 to 0.5279. Saving model...\n",
      "Epoch 121 - Loss: 0.5372\n",
      "Epoch 122 - Loss: 0.5393\n",
      "Epoch 123 - Loss: 0.5147\n",
      "Loss improved from 0.5279 to 0.5147. Saving model...\n",
      "Epoch 124 - Loss: 0.5175\n",
      "Epoch 125 - Loss: 0.5224\n",
      "Epoch 126 - Loss: 0.5019\n",
      "Loss improved from 0.5147 to 0.5019. Saving model...\n",
      "Epoch 127 - Loss: 0.4945\n",
      "Loss improved from 0.5019 to 0.4945. Saving model...\n",
      "Epoch 128 - Loss: 0.4906\n",
      "Loss improved from 0.4945 to 0.4906. Saving model...\n",
      "Epoch 129 - Loss: 0.4831\n",
      "Loss improved from 0.4906 to 0.4831. Saving model...\n",
      "Epoch 130 - Loss: 0.4790\n",
      "Loss improved from 0.4831 to 0.4790. Saving model...\n",
      "Epoch 131 - Loss: 0.4766\n",
      "Loss improved from 0.4790 to 0.4766. Saving model...\n",
      "Epoch 132 - Loss: 0.4770\n",
      "Epoch 133 - Loss: 0.4728\n",
      "Loss improved from 0.4766 to 0.4728. Saving model...\n",
      "Epoch 134 - Loss: 0.4695\n",
      "Loss improved from 0.4728 to 0.4695. Saving model...\n",
      "Epoch 135 - Loss: 0.4644\n",
      "Loss improved from 0.4695 to 0.4644. Saving model...\n",
      "Epoch 136 - Loss: 0.4579\n",
      "Loss improved from 0.4644 to 0.4579. Saving model...\n",
      "Epoch 137 - Loss: 0.4551\n",
      "Loss improved from 0.4579 to 0.4551. Saving model...\n",
      "Epoch 138 - Loss: 0.4502\n",
      "Loss improved from 0.4551 to 0.4502. Saving model...\n",
      "Epoch 139 - Loss: 0.4536\n",
      "Epoch 140 - Loss: 0.4470\n",
      "Loss improved from 0.4502 to 0.4470. Saving model...\n",
      "Epoch 141 - Loss: 0.4431\n",
      "Loss improved from 0.4470 to 0.4431. Saving model...\n",
      "Epoch 142 - Loss: 0.4383\n",
      "Loss improved from 0.4431 to 0.4383. Saving model...\n",
      "Epoch 143 - Loss: 0.4391\n",
      "Epoch 144 - Loss: 0.4497\n",
      "Epoch 145 - Loss: 0.4682\n",
      "Epoch 146 - Loss: 0.4336\n",
      "Loss improved from 0.4383 to 0.4336. Saving model...\n",
      "Epoch 147 - Loss: 0.4204\n",
      "Loss improved from 0.4336 to 0.4204. Saving model...\n",
      "Epoch 148 - Loss: 0.4182\n",
      "Loss improved from 0.4204 to 0.4182. Saving model...\n",
      "Epoch 149 - Loss: 0.4143\n",
      "Loss improved from 0.4182 to 0.4143. Saving model...\n",
      "Epoch 150 - Loss: 0.4089\n",
      "Loss improved from 0.4143 to 0.4089. Saving model...\n",
      "Epoch 151 - Loss: 0.4100\n",
      "Epoch 152 - Loss: 0.4120\n",
      "Epoch 153 - Loss: 0.4057\n",
      "Loss improved from 0.4089 to 0.4057. Saving model...\n",
      "Epoch 154 - Loss: 0.4001\n",
      "Loss improved from 0.4057 to 0.4001. Saving model...\n",
      "Epoch 155 - Loss: 0.4010\n",
      "Epoch 156 - Loss: 0.3945\n",
      "Loss improved from 0.4001 to 0.3945. Saving model...\n",
      "Epoch 157 - Loss: 0.3947\n",
      "Epoch 158 - Loss: 0.3917\n",
      "Loss improved from 0.3945 to 0.3917. Saving model...\n",
      "Epoch 159 - Loss: 0.3885\n",
      "Loss improved from 0.3917 to 0.3885. Saving model...\n",
      "Epoch 160 - Loss: 0.3837\n",
      "Loss improved from 0.3885 to 0.3837. Saving model...\n",
      "Epoch 161 - Loss: 0.3829\n",
      "Loss improved from 0.3837 to 0.3829. Saving model...\n",
      "Epoch 162 - Loss: 0.3793\n",
      "Loss improved from 0.3829 to 0.3793. Saving model...\n",
      "Epoch 163 - Loss: 0.3826\n",
      "Epoch 164 - Loss: 0.3798\n",
      "Epoch 165 - Loss: 0.3780\n",
      "Loss improved from 0.3793 to 0.3780. Saving model...\n",
      "Epoch 166 - Loss: 0.3698\n",
      "Loss improved from 0.3780 to 0.3698. Saving model...\n",
      "Epoch 167 - Loss: 0.3695\n",
      "Loss improved from 0.3698 to 0.3695. Saving model...\n",
      "Epoch 168 - Loss: 0.3718\n",
      "Epoch 169 - Loss: 0.3629\n",
      "Loss improved from 0.3695 to 0.3629. Saving model...\n",
      "Epoch 170 - Loss: 0.3551\n",
      "Loss improved from 0.3629 to 0.3551. Saving model...\n",
      "Epoch 171 - Loss: 0.3523\n",
      "Loss improved from 0.3551 to 0.3523. Saving model...\n",
      "Epoch 172 - Loss: 0.3546\n",
      "Epoch 173 - Loss: 0.3523\n",
      "Epoch 174 - Loss: 0.3614\n",
      "Epoch 175 - Loss: 0.3573\n",
      "Epoch 176 - Loss: 0.3433\n",
      "Loss improved from 0.3523 to 0.3433. Saving model...\n",
      "Epoch 177 - Loss: 0.3395\n",
      "Loss improved from 0.3433 to 0.3395. Saving model...\n",
      "Epoch 178 - Loss: 0.3416\n",
      "Epoch 179 - Loss: 0.3431\n",
      "Epoch 180 - Loss: 0.3374\n",
      "Loss improved from 0.3395 to 0.3374. Saving model...\n",
      "Epoch 181 - Loss: 0.3341\n",
      "Loss improved from 0.3374 to 0.3341. Saving model...\n",
      "Epoch 182 - Loss: 0.3374\n",
      "Epoch 183 - Loss: 0.3300\n",
      "Loss improved from 0.3341 to 0.3300. Saving model...\n",
      "Epoch 184 - Loss: 0.3298\n",
      "Loss improved from 0.3300 to 0.3298. Saving model...\n",
      "Epoch 185 - Loss: 0.3238\n",
      "Loss improved from 0.3298 to 0.3238. Saving model...\n",
      "Epoch 186 - Loss: 0.3203\n",
      "Loss improved from 0.3238 to 0.3203. Saving model...\n",
      "Epoch 187 - Loss: 0.3211\n",
      "Epoch 188 - Loss: 0.3186\n",
      "Loss improved from 0.3203 to 0.3186. Saving model...\n",
      "Epoch 189 - Loss: 0.3164\n",
      "Loss improved from 0.3186 to 0.3164. Saving model...\n",
      "Epoch 190 - Loss: 0.3189\n",
      "Epoch 191 - Loss: 0.3142\n",
      "Loss improved from 0.3164 to 0.3142. Saving model...\n",
      "Epoch 192 - Loss: 0.3121\n",
      "Loss improved from 0.3142 to 0.3121. Saving model...\n",
      "Epoch 193 - Loss: 0.3066\n",
      "Loss improved from 0.3121 to 0.3066. Saving model...\n",
      "Epoch 194 - Loss: 0.3092\n",
      "Epoch 195 - Loss: 0.3024\n",
      "Loss improved from 0.3066 to 0.3024. Saving model...\n",
      "Epoch 196 - Loss: 0.2983\n",
      "Loss improved from 0.3024 to 0.2983. Saving model...\n",
      "Epoch 197 - Loss: 0.2937\n",
      "Loss improved from 0.2983 to 0.2937. Saving model...\n",
      "Epoch 198 - Loss: 0.2929\n",
      "Loss improved from 0.2937 to 0.2929. Saving model...\n",
      "Epoch 199 - Loss: 0.2959\n",
      "Epoch 200 - Loss: 0.2941\n",
      "Epoch 201 - Loss: 0.2927\n",
      "Loss improved from 0.2929 to 0.2927. Saving model...\n",
      "Epoch 202 - Loss: 0.2915\n",
      "Loss improved from 0.2927 to 0.2915. Saving model...\n",
      "Epoch 203 - Loss: 0.2879\n",
      "Loss improved from 0.2915 to 0.2879. Saving model...\n",
      "Epoch 204 - Loss: 0.2812\n",
      "Loss improved from 0.2879 to 0.2812. Saving model...\n",
      "Epoch 205 - Loss: 0.2796\n",
      "Loss improved from 0.2812 to 0.2796. Saving model...\n",
      "Epoch 206 - Loss: 0.2909\n",
      "Epoch 207 - Loss: 0.2925\n",
      "Epoch 208 - Loss: 0.2797\n",
      "Epoch 209 - Loss: 0.2769\n",
      "Loss improved from 0.2796 to 0.2769. Saving model...\n",
      "Epoch 210 - Loss: 0.2752\n",
      "Loss improved from 0.2769 to 0.2752. Saving model...\n",
      "Epoch 211 - Loss: 0.2718\n",
      "Loss improved from 0.2752 to 0.2718. Saving model...\n",
      "Epoch 212 - Loss: 0.2825\n",
      "Epoch 213 - Loss: 0.2817\n",
      "Epoch 214 - Loss: 0.2675\n",
      "Loss improved from 0.2718 to 0.2675. Saving model...\n",
      "Epoch 215 - Loss: 0.2755\n",
      "Epoch 216 - Loss: 0.2711\n",
      "Epoch 217 - Loss: 0.2668\n",
      "Loss improved from 0.2675 to 0.2668. Saving model...\n",
      "Epoch 218 - Loss: 0.2673\n",
      "Epoch 219 - Loss: 0.2616\n",
      "Loss improved from 0.2668 to 0.2616. Saving model...\n",
      "Epoch 220 - Loss: 0.2567\n",
      "Loss improved from 0.2616 to 0.2567. Saving model...\n",
      "Epoch 221 - Loss: 0.2529\n",
      "Loss improved from 0.2567 to 0.2529. Saving model...\n",
      "Epoch 222 - Loss: 0.2584\n",
      "Epoch 223 - Loss: 0.2582\n",
      "Epoch 224 - Loss: 0.2518\n",
      "Loss improved from 0.2529 to 0.2518. Saving model...\n",
      "Epoch 225 - Loss: 0.2470\n",
      "Loss improved from 0.2518 to 0.2470. Saving model...\n",
      "Epoch 226 - Loss: 0.2479\n",
      "Epoch 227 - Loss: 0.2482\n",
      "Epoch 228 - Loss: 0.2507\n",
      "Epoch 229 - Loss: 0.2457\n",
      "Loss improved from 0.2470 to 0.2457. Saving model...\n",
      "Epoch 230 - Loss: 0.2445\n",
      "Loss improved from 0.2457 to 0.2445. Saving model...\n",
      "Epoch 231 - Loss: 0.2410\n",
      "Loss improved from 0.2445 to 0.2410. Saving model...\n",
      "Epoch 232 - Loss: 0.2375\n",
      "Loss improved from 0.2410 to 0.2375. Saving model...\n",
      "Epoch 233 - Loss: 0.2349\n",
      "Loss improved from 0.2375 to 0.2349. Saving model...\n",
      "Epoch 234 - Loss: 0.2433\n",
      "Epoch 235 - Loss: 0.2397\n",
      "Epoch 236 - Loss: 0.2381\n",
      "Epoch 237 - Loss: 0.2399\n",
      "Epoch 238 - Loss: 0.2322\n",
      "Loss improved from 0.2349 to 0.2322. Saving model...\n",
      "Epoch 239 - Loss: 0.2245\n",
      "Loss improved from 0.2322 to 0.2245. Saving model...\n",
      "Epoch 240 - Loss: 0.2229\n",
      "Loss improved from 0.2245 to 0.2229. Saving model...\n",
      "Epoch 241 - Loss: 0.2273\n",
      "Epoch 242 - Loss: 0.2213\n",
      "Loss improved from 0.2229 to 0.2213. Saving model...\n",
      "Epoch 243 - Loss: 0.2209\n",
      "Loss improved from 0.2213 to 0.2209. Saving model...\n",
      "Epoch 244 - Loss: 0.2212\n",
      "Epoch 245 - Loss: 0.2194\n",
      "Loss improved from 0.2209 to 0.2194. Saving model...\n",
      "Epoch 246 - Loss: 0.2202\n",
      "Epoch 247 - Loss: 0.2174\n",
      "Loss improved from 0.2194 to 0.2174. Saving model...\n",
      "Epoch 248 - Loss: 0.2164\n",
      "Loss improved from 0.2174 to 0.2164. Saving model...\n",
      "Epoch 249 - Loss: 0.2223\n",
      "Epoch 250 - Loss: 0.2242\n",
      "Epoch 251 - Loss: 0.2137\n",
      "Loss improved from 0.2164 to 0.2137. Saving model...\n",
      "Epoch 252 - Loss: 0.2147\n",
      "Epoch 253 - Loss: 0.2124\n",
      "Loss improved from 0.2137 to 0.2124. Saving model...\n",
      "Epoch 254 - Loss: 0.2173\n",
      "Epoch 255 - Loss: 0.2128\n",
      "Epoch 256 - Loss: 0.2094\n",
      "Loss improved from 0.2124 to 0.2094. Saving model...\n",
      "Epoch 257 - Loss: 0.2052\n",
      "Loss improved from 0.2094 to 0.2052. Saving model...\n",
      "Epoch 258 - Loss: 0.2094\n",
      "Epoch 259 - Loss: 0.2098\n",
      "Epoch 260 - Loss: 0.2067\n",
      "Epoch 261 - Loss: 0.2214\n",
      "Epoch 262 - Loss: 0.2067\n",
      "Epoch 263 - Loss: 0.2028\n",
      "Loss improved from 0.2052 to 0.2028. Saving model...\n",
      "Epoch 264 - Loss: 0.1981\n",
      "Loss improved from 0.2028 to 0.1981. Saving model...\n",
      "Epoch 265 - Loss: 0.1992\n",
      "Epoch 266 - Loss: 0.2032\n",
      "Epoch 267 - Loss: 0.1970\n",
      "Loss improved from 0.1981 to 0.1970. Saving model...\n",
      "Epoch 268 - Loss: 0.1985\n",
      "Epoch 269 - Loss: 0.1928\n",
      "Loss improved from 0.1970 to 0.1928. Saving model...\n",
      "Epoch 270 - Loss: 0.1936\n",
      "Epoch 271 - Loss: 0.1910\n",
      "Loss improved from 0.1928 to 0.1910. Saving model...\n",
      "Epoch 272 - Loss: 0.1877\n",
      "Loss improved from 0.1910 to 0.1877. Saving model...\n",
      "Epoch 273 - Loss: 0.1925\n",
      "Epoch 274 - Loss: 0.1885\n",
      "Epoch 275 - Loss: 0.1900\n",
      "Epoch 276 - Loss: 0.1963\n",
      "Epoch 277 - Loss: 0.1883\n",
      "Epoch 278 - Loss: 0.1868\n",
      "Loss improved from 0.1877 to 0.1868. Saving model...\n",
      "Epoch 279 - Loss: 0.1890\n",
      "Epoch 280 - Loss: 0.1886\n",
      "Epoch 281 - Loss: 0.1872\n",
      "Epoch 282 - Loss: 0.1846\n",
      "Loss improved from 0.1868 to 0.1846. Saving model...\n",
      "Epoch 283 - Loss: 0.1781\n",
      "Loss improved from 0.1846 to 0.1781. Saving model...\n",
      "Epoch 284 - Loss: 0.1746\n",
      "Loss improved from 0.1781 to 0.1746. Saving model...\n",
      "Epoch 285 - Loss: 0.1763\n",
      "Epoch 286 - Loss: 0.1766\n",
      "Epoch 287 - Loss: 0.1810\n",
      "Epoch 288 - Loss: 0.1842\n",
      "Epoch 289 - Loss: 0.1808\n",
      "Epoch 290 - Loss: 0.1781\n",
      "Epoch 291 - Loss: 0.1746\n",
      "Epoch 292 - Loss: 0.1771\n",
      "Epoch 293 - Loss: 0.1707\n",
      "Loss improved from 0.1746 to 0.1707. Saving model...\n",
      "Epoch 294 - Loss: 0.1658\n",
      "Loss improved from 0.1707 to 0.1658. Saving model...\n",
      "Epoch 295 - Loss: 0.1677\n",
      "Epoch 296 - Loss: 0.1718\n",
      "Epoch 297 - Loss: 0.1681\n",
      "Epoch 298 - Loss: 0.1665\n",
      "Epoch 299 - Loss: 0.1653\n",
      "Loss improved from 0.1658 to 0.1653. Saving model...\n",
      "Epoch 300 - Loss: 0.1724\n",
      "Epoch 301 - Loss: 0.1691\n",
      "Epoch 302 - Loss: 0.1687\n",
      "Epoch 303 - Loss: 0.1814\n",
      "Epoch 304 - Loss: 0.1672\n",
      "Epoch 305 - Loss: 0.1621\n",
      "Loss improved from 0.1653 to 0.1621. Saving model...\n",
      "Epoch 306 - Loss: 0.1630\n",
      "Epoch 307 - Loss: 0.1622\n",
      "Epoch 308 - Loss: 0.1673\n",
      "Epoch 309 - Loss: 0.1619\n",
      "Loss improved from 0.1621 to 0.1619. Saving model...\n",
      "Epoch 310 - Loss: 0.1561\n",
      "Loss improved from 0.1619 to 0.1561. Saving model...\n",
      "Epoch 311 - Loss: 0.1528\n",
      "Loss improved from 0.1561 to 0.1528. Saving model...\n",
      "Epoch 312 - Loss: 0.1533\n",
      "Epoch 313 - Loss: 0.1549\n",
      "Epoch 314 - Loss: 0.1517\n",
      "Loss improved from 0.1528 to 0.1517. Saving model...\n",
      "Epoch 315 - Loss: 0.1519\n",
      "Epoch 316 - Loss: 0.1497\n",
      "Loss improved from 0.1517 to 0.1497. Saving model...\n",
      "Epoch 317 - Loss: 0.1510\n",
      "Epoch 318 - Loss: 0.1523\n",
      "Epoch 319 - Loss: 0.1628\n",
      "Epoch 320 - Loss: 0.1545\n",
      "Epoch 321 - Loss: 0.1558\n",
      "Epoch 322 - Loss: 0.1508\n",
      "Epoch 323 - Loss: 0.1474\n",
      "Loss improved from 0.1497 to 0.1474. Saving model...\n",
      "Epoch 324 - Loss: 0.1489\n",
      "Epoch 325 - Loss: 0.1560\n",
      "Epoch 326 - Loss: 0.1571\n",
      "Epoch 327 - Loss: 0.1523\n",
      "Epoch 328 - Loss: 0.1562\n",
      "Epoch 329 - Loss: 0.1473\n",
      "Loss improved from 0.1474 to 0.1473. Saving model...\n",
      "Epoch 330 - Loss: 0.1422\n",
      "Loss improved from 0.1473 to 0.1422. Saving model...\n",
      "Epoch 331 - Loss: 0.1418\n",
      "Loss improved from 0.1422 to 0.1418. Saving model...\n",
      "Epoch 332 - Loss: 0.1379\n",
      "Loss improved from 0.1418 to 0.1379. Saving model...\n",
      "Epoch 333 - Loss: 0.1412\n",
      "Epoch 334 - Loss: 0.1389\n",
      "Epoch 335 - Loss: 0.1462\n",
      "Epoch 336 - Loss: 0.1455\n",
      "Epoch 337 - Loss: 0.1430\n",
      "Epoch 338 - Loss: 0.1394\n",
      "Epoch 339 - Loss: 0.1381\n",
      "Epoch 340 - Loss: 0.1317\n",
      "Loss improved from 0.1379 to 0.1317. Saving model...\n",
      "Epoch 341 - Loss: 0.1301\n",
      "Loss improved from 0.1317 to 0.1301. Saving model...\n",
      "Epoch 342 - Loss: 0.1339\n",
      "Epoch 343 - Loss: 0.1388\n",
      "Epoch 344 - Loss: 0.1360\n",
      "Epoch 345 - Loss: 0.1342\n",
      "Epoch 346 - Loss: 0.1332\n",
      "Epoch 347 - Loss: 0.1345\n",
      "Epoch 348 - Loss: 0.1310\n",
      "Epoch 349 - Loss: 0.1289\n",
      "Loss improved from 0.1301 to 0.1289. Saving model...\n",
      "Epoch 350 - Loss: 0.1309\n",
      "Epoch 351 - Loss: 0.1307\n",
      "Epoch 352 - Loss: 0.1272\n",
      "Loss improved from 0.1289 to 0.1272. Saving model...\n",
      "Epoch 353 - Loss: 0.1254\n",
      "Loss improved from 0.1272 to 0.1254. Saving model...\n",
      "Epoch 354 - Loss: 0.1271\n",
      "Epoch 355 - Loss: 0.1309\n",
      "Epoch 356 - Loss: 0.1362\n",
      "Epoch 357 - Loss: 0.1251\n",
      "Loss improved from 0.1254 to 0.1251. Saving model...\n",
      "Epoch 358 - Loss: 0.1260\n",
      "Epoch 359 - Loss: 0.1231\n",
      "Loss improved from 0.1251 to 0.1231. Saving model...\n",
      "Epoch 360 - Loss: 0.1234\n",
      "Epoch 361 - Loss: 0.1319\n",
      "Epoch 362 - Loss: 0.1255\n",
      "Epoch 363 - Loss: 0.1253\n",
      "Epoch 364 - Loss: 0.1248\n",
      "Epoch 365 - Loss: 0.1225\n",
      "Loss improved from 0.1231 to 0.1225. Saving model...\n",
      "Epoch 366 - Loss: 0.1235\n",
      "Epoch 367 - Loss: 0.1217\n",
      "Loss improved from 0.1225 to 0.1217. Saving model...\n",
      "Epoch 368 - Loss: 0.1281\n",
      "Epoch 369 - Loss: 0.1326\n",
      "Epoch 370 - Loss: 0.1285\n",
      "Epoch 371 - Loss: 0.1233\n",
      "Epoch 372 - Loss: 0.1168\n",
      "Loss improved from 0.1217 to 0.1168. Saving model...\n",
      "Epoch 373 - Loss: 0.1173\n",
      "Epoch 374 - Loss: 0.1213\n",
      "Epoch 375 - Loss: 0.1222\n",
      "Epoch 376 - Loss: 0.1198\n",
      "Epoch 377 - Loss: 0.1211\n",
      "Epoch 378 - Loss: 0.1175\n",
      "Epoch 379 - Loss: 0.1159\n",
      "Loss improved from 0.1168 to 0.1159. Saving model...\n",
      "Epoch 380 - Loss: 0.1135\n",
      "Loss improved from 0.1159 to 0.1135. Saving model...\n",
      "Epoch 381 - Loss: 0.1164\n",
      "Epoch 382 - Loss: 0.1113\n",
      "Loss improved from 0.1135 to 0.1113. Saving model...\n",
      "Epoch 383 - Loss: 0.1128\n",
      "Epoch 384 - Loss: 0.1155\n",
      "Epoch 385 - Loss: 0.1126\n",
      "Epoch 386 - Loss: 0.1158\n",
      "Epoch 387 - Loss: 0.1104\n",
      "Loss improved from 0.1113 to 0.1104. Saving model...\n",
      "Epoch 388 - Loss: 0.1099\n",
      "Loss improved from 0.1104 to 0.1099. Saving model...\n",
      "Epoch 389 - Loss: 0.1097\n",
      "Loss improved from 0.1099 to 0.1097. Saving model...\n",
      "Epoch 390 - Loss: 0.1116\n",
      "Epoch 391 - Loss: 0.1127\n",
      "Epoch 392 - Loss: 0.1096\n",
      "Loss improved from 0.1097 to 0.1096. Saving model...\n",
      "Epoch 393 - Loss: 0.1120\n",
      "Epoch 394 - Loss: 0.1099\n",
      "Epoch 395 - Loss: 0.1085\n",
      "Loss improved from 0.1096 to 0.1085. Saving model...\n",
      "Epoch 396 - Loss: 0.1109\n",
      "Epoch 397 - Loss: 0.1097\n",
      "Epoch 398 - Loss: 0.1095\n",
      "Epoch 399 - Loss: 0.1090\n",
      "Epoch 400 - Loss: 0.1068\n",
      "Loss improved from 0.1085 to 0.1068. Saving model...\n",
      "Epoch 401 - Loss: 0.1189\n",
      "Epoch 402 - Loss: 0.1094\n",
      "Epoch 403 - Loss: 0.1076\n",
      "Epoch 404 - Loss: 0.1041\n",
      "Loss improved from 0.1068 to 0.1041. Saving model...\n",
      "Epoch 405 - Loss: 0.1150\n",
      "Epoch 406 - Loss: 0.1095\n",
      "Epoch 407 - Loss: 0.1091\n",
      "Epoch 408 - Loss: 0.1065\n",
      "Epoch 409 - Loss: 0.1031\n",
      "Loss improved from 0.1041 to 0.1031. Saving model...\n",
      "Epoch 410 - Loss: 0.1055\n",
      "Epoch 411 - Loss: 0.1084\n",
      "Epoch 412 - Loss: 0.1025\n",
      "Loss improved from 0.1031 to 0.1025. Saving model...\n",
      "Epoch 413 - Loss: 0.0996\n",
      "Loss improved from 0.1025 to 0.0996. Saving model...\n",
      "Epoch 414 - Loss: 0.0981\n",
      "Loss improved from 0.0996 to 0.0981. Saving model...\n",
      "Epoch 415 - Loss: 0.1015\n",
      "Epoch 416 - Loss: 0.0983\n",
      "Epoch 417 - Loss: 0.1004\n",
      "Epoch 418 - Loss: 0.0972\n",
      "Loss improved from 0.0981 to 0.0972. Saving model...\n",
      "Epoch 419 - Loss: 0.1040\n",
      "Epoch 420 - Loss: 0.1045\n",
      "Epoch 421 - Loss: 0.0993\n",
      "Epoch 422 - Loss: 0.1009\n",
      "Epoch 423 - Loss: 0.0963\n",
      "Loss improved from 0.0972 to 0.0963. Saving model...\n",
      "Epoch 424 - Loss: 0.0955\n",
      "Loss improved from 0.0963 to 0.0955. Saving model...\n",
      "Epoch 425 - Loss: 0.0983\n",
      "Epoch 426 - Loss: 0.0950\n",
      "Loss improved from 0.0955 to 0.0950. Saving model...\n",
      "Epoch 427 - Loss: 0.0941\n",
      "Loss improved from 0.0950 to 0.0941. Saving model...\n",
      "Epoch 428 - Loss: 0.0935\n",
      "Loss improved from 0.0941 to 0.0935. Saving model...\n",
      "Epoch 429 - Loss: 0.0959\n",
      "Epoch 430 - Loss: 0.0977\n",
      "Epoch 431 - Loss: 0.0939\n",
      "Epoch 432 - Loss: 0.0893\n",
      "Loss improved from 0.0935 to 0.0893. Saving model...\n",
      "Epoch 433 - Loss: 0.0917\n",
      "Epoch 434 - Loss: 0.0929\n",
      "Epoch 435 - Loss: 0.1025\n",
      "Epoch 436 - Loss: 0.1080\n",
      "Epoch 437 - Loss: 0.0980\n",
      "Epoch 438 - Loss: 0.0978\n",
      "Epoch 439 - Loss: 0.1068\n",
      "Epoch 440 - Loss: 0.0944\n",
      "Epoch 441 - Loss: 0.0923\n",
      "Epoch 442 - Loss: 0.0928\n",
      "Epoch 443 - Loss: 0.0894\n",
      "Epoch 444 - Loss: 0.0873\n",
      "Loss improved from 0.0893 to 0.0873. Saving model...\n",
      "Epoch 445 - Loss: 0.0890\n",
      "Epoch 446 - Loss: 0.0903\n",
      "Epoch 447 - Loss: 0.0920\n",
      "Epoch 448 - Loss: 0.0927\n",
      "Epoch 449 - Loss: 0.0885\n",
      "Epoch 450 - Loss: 0.0866\n",
      "Loss improved from 0.0873 to 0.0866. Saving model...\n",
      "Epoch 451 - Loss: 0.0912\n",
      "Epoch 452 - Loss: 0.0945\n",
      "Epoch 453 - Loss: 0.0903\n",
      "Epoch 454 - Loss: 0.0990\n",
      "Epoch 455 - Loss: 0.0945\n",
      "Epoch 456 - Loss: 0.0868\n",
      "Epoch 457 - Loss: 0.0888\n",
      "Epoch 458 - Loss: 0.0837\n",
      "Loss improved from 0.0866 to 0.0837. Saving model...\n",
      "Epoch 459 - Loss: 0.0841\n",
      "Epoch 460 - Loss: 0.0830\n",
      "Loss improved from 0.0837 to 0.0830. Saving model...\n",
      "Epoch 461 - Loss: 0.0828\n",
      "Loss improved from 0.0830 to 0.0828. Saving model...\n",
      "Epoch 462 - Loss: 0.0811\n",
      "Loss improved from 0.0828 to 0.0811. Saving model...\n",
      "Epoch 463 - Loss: 0.0830\n",
      "Epoch 464 - Loss: 0.0847\n",
      "Epoch 465 - Loss: 0.0862\n",
      "Epoch 466 - Loss: 0.1027\n",
      "Epoch 467 - Loss: 0.0958\n",
      "Epoch 468 - Loss: 0.0890\n",
      "Epoch 469 - Loss: 0.0830\n",
      "Epoch 470 - Loss: 0.0840\n",
      "Epoch 471 - Loss: 0.0799\n",
      "Loss improved from 0.0811 to 0.0799. Saving model...\n",
      "Epoch 472 - Loss: 0.0836\n",
      "Epoch 473 - Loss: 0.0820\n",
      "Epoch 474 - Loss: 0.0808\n",
      "Epoch 475 - Loss: 0.0792\n",
      "Loss improved from 0.0799 to 0.0792. Saving model...\n",
      "Epoch 476 - Loss: 0.0773\n",
      "Loss improved from 0.0792 to 0.0773. Saving model...\n",
      "Epoch 477 - Loss: 0.0794\n",
      "Epoch 478 - Loss: 0.0849\n",
      "Epoch 479 - Loss: 0.0959\n",
      "Epoch 480 - Loss: 0.0915\n",
      "Epoch 481 - Loss: 0.1001\n",
      "Epoch 482 - Loss: 0.0824\n",
      "Epoch 483 - Loss: 0.0786\n",
      "Epoch 484 - Loss: 0.0762\n",
      "Loss improved from 0.0773 to 0.0762. Saving model...\n",
      "Epoch 485 - Loss: 0.0774\n",
      "Epoch 486 - Loss: 0.0749\n",
      "Loss improved from 0.0762 to 0.0749. Saving model...\n",
      "Epoch 487 - Loss: 0.0750\n",
      "Epoch 488 - Loss: 0.0761\n",
      "Epoch 489 - Loss: 0.0754\n",
      "Epoch 490 - Loss: 0.0755\n",
      "Epoch 491 - Loss: 0.0763\n",
      "Epoch 492 - Loss: 0.0792\n",
      "Epoch 493 - Loss: 0.0745\n",
      "Loss improved from 0.0749 to 0.0745. Saving model...\n",
      "Epoch 494 - Loss: 0.0745\n",
      "Loss improved from 0.0745 to 0.0745. Saving model...\n",
      "Epoch 495 - Loss: 0.0745\n",
      "Loss improved from 0.0745 to 0.0745. Saving model...\n",
      "Epoch 496 - Loss: 0.0729\n",
      "Loss improved from 0.0745 to 0.0729. Saving model...\n",
      "Epoch 497 - Loss: 0.0749\n",
      "Epoch 498 - Loss: 0.0740\n",
      "Epoch 499 - Loss: 0.0750\n",
      "Epoch 500 - Loss: 0.0755\n",
      "Epoch 501 - Loss: 0.0751\n",
      "Epoch 502 - Loss: 0.0785\n",
      "Epoch 503 - Loss: 0.0763\n",
      "Epoch 504 - Loss: 0.0795\n",
      "Epoch 505 - Loss: 0.0781\n",
      "Epoch 506 - Loss: 0.0732\n",
      "Epoch 507 - Loss: 0.0738\n",
      "Epoch 508 - Loss: 0.0800\n",
      "Epoch 509 - Loss: 0.0806\n",
      "Epoch 510 - Loss: 0.0787\n",
      "Epoch 511 - Loss: 0.0784\n",
      "Epoch 512 - Loss: 0.0732\n",
      "Epoch 513 - Loss: 0.0693\n",
      "Loss improved from 0.0729 to 0.0693. Saving model...\n",
      "Epoch 514 - Loss: 0.0709\n",
      "Epoch 515 - Loss: 0.0692\n",
      "Loss improved from 0.0693 to 0.0692. Saving model...\n",
      "Epoch 516 - Loss: 0.0701\n",
      "Epoch 517 - Loss: 0.0700\n",
      "Epoch 518 - Loss: 0.0750\n",
      "Epoch 519 - Loss: 0.0878\n",
      "Epoch 520 - Loss: 0.0792\n",
      "Epoch 521 - Loss: 0.0855\n",
      "Epoch 522 - Loss: 0.0789\n",
      "Epoch 523 - Loss: 0.0794\n",
      "Epoch 524 - Loss: 0.0790\n",
      "Epoch 525 - Loss: 0.0779\n",
      "Epoch 526 - Loss: 0.0694\n",
      "Epoch 527 - Loss: 0.0685\n",
      "Loss improved from 0.0692 to 0.0685. Saving model...\n",
      "Epoch 528 - Loss: 0.0663\n",
      "Loss improved from 0.0685 to 0.0663. Saving model...\n",
      "Epoch 529 - Loss: 0.0652\n",
      "Loss improved from 0.0663 to 0.0652. Saving model...\n",
      "Epoch 530 - Loss: 0.0642\n",
      "Loss improved from 0.0652 to 0.0642. Saving model...\n",
      "Epoch 531 - Loss: 0.0678\n",
      "Epoch 532 - Loss: 0.0658\n",
      "Epoch 533 - Loss: 0.0662\n",
      "Epoch 534 - Loss: 0.0683\n",
      "Epoch 535 - Loss: 0.0705\n",
      "Epoch 536 - Loss: 0.0718\n",
      "Epoch 537 - Loss: 0.0680\n",
      "Epoch 538 - Loss: 0.0642\n",
      "Loss improved from 0.0642 to 0.0642. Saving model...\n",
      "Epoch 539 - Loss: 0.0616\n",
      "Loss improved from 0.0642 to 0.0616. Saving model...\n",
      "Epoch 540 - Loss: 0.0649\n",
      "Epoch 541 - Loss: 0.0632\n",
      "Epoch 542 - Loss: 0.0639\n",
      "Epoch 543 - Loss: 0.0627\n",
      "Epoch 544 - Loss: 0.0642\n",
      "Epoch 545 - Loss: 0.0629\n",
      "Epoch 546 - Loss: 0.0641\n",
      "Epoch 547 - Loss: 0.0634\n",
      "Epoch 548 - Loss: 0.0680\n",
      "Epoch 549 - Loss: 0.0662\n",
      "Epoch 550 - Loss: 0.0708\n",
      "Epoch 551 - Loss: 0.0650\n",
      "Epoch 552 - Loss: 0.0666\n",
      "Epoch 553 - Loss: 0.0830\n",
      "Epoch 554 - Loss: 0.0688\n",
      "Epoch 555 - Loss: 0.0623\n",
      "Epoch 556 - Loss: 0.0603\n",
      "Loss improved from 0.0616 to 0.0603. Saving model...\n",
      "Epoch 557 - Loss: 0.0615\n",
      "Epoch 558 - Loss: 0.0642\n",
      "Epoch 559 - Loss: 0.0650\n",
      "Epoch 560 - Loss: 0.0619\n",
      "Epoch 561 - Loss: 0.0609\n",
      "Epoch 562 - Loss: 0.0590\n",
      "Loss improved from 0.0603 to 0.0590. Saving model...\n",
      "Epoch 563 - Loss: 0.0590\n",
      "Epoch 564 - Loss: 0.0630\n",
      "Epoch 565 - Loss: 0.0602\n",
      "Epoch 566 - Loss: 0.0593\n",
      "Epoch 567 - Loss: 0.0586\n",
      "Loss improved from 0.0590 to 0.0586. Saving model...\n",
      "Epoch 568 - Loss: 0.0593\n",
      "Epoch 569 - Loss: 0.0616\n",
      "Epoch 570 - Loss: 0.0583\n",
      "Loss improved from 0.0586 to 0.0583. Saving model...\n",
      "Epoch 571 - Loss: 0.0606\n",
      "Epoch 572 - Loss: 0.0586\n",
      "Epoch 573 - Loss: 0.0569\n",
      "Loss improved from 0.0583 to 0.0569. Saving model...\n",
      "Epoch 574 - Loss: 0.0613\n",
      "Epoch 575 - Loss: 0.0602\n",
      "Epoch 576 - Loss: 0.0635\n",
      "Epoch 577 - Loss: 0.0650\n",
      "Epoch 578 - Loss: 0.0659\n",
      "Epoch 579 - Loss: 0.0626\n",
      "Epoch 580 - Loss: 0.0652\n",
      "Epoch 581 - Loss: 0.0592\n",
      "Epoch 582 - Loss: 0.0586\n",
      "Epoch 583 - Loss: 0.0591\n",
      "Epoch 584 - Loss: 0.0588\n",
      "Epoch 585 - Loss: 0.0608\n",
      "Epoch 586 - Loss: 0.0620\n",
      "Epoch 587 - Loss: 0.0678\n",
      "Epoch 588 - Loss: 0.0752\n",
      "Epoch 589 - Loss: 0.0658\n",
      "Epoch 590 - Loss: 0.0601\n",
      "Epoch 591 - Loss: 0.0592\n",
      "Epoch 592 - Loss: 0.0567\n",
      "Loss improved from 0.0569 to 0.0567. Saving model...\n",
      "Epoch 593 - Loss: 0.0558\n",
      "Loss improved from 0.0567 to 0.0558. Saving model...\n",
      "Epoch 594 - Loss: 0.0630\n",
      "Epoch 595 - Loss: 0.0576\n",
      "Epoch 596 - Loss: 0.0556\n",
      "Loss improved from 0.0558 to 0.0556. Saving model...\n",
      "Epoch 597 - Loss: 0.0573\n",
      "Epoch 598 - Loss: 0.0553\n",
      "Loss improved from 0.0556 to 0.0553. Saving model...\n",
      "Epoch 599 - Loss: 0.0620\n",
      "Epoch 600 - Loss: 0.0647\n",
      "Epoch 601 - Loss: 0.0595\n",
      "Epoch 602 - Loss: 0.0574\n",
      "Epoch 603 - Loss: 0.0583\n",
      "Epoch 604 - Loss: 0.0518\n",
      "Loss improved from 0.0553 to 0.0518. Saving model...\n",
      "Epoch 605 - Loss: 0.0508\n",
      "Loss improved from 0.0518 to 0.0508. Saving model...\n",
      "Epoch 606 - Loss: 0.0524\n",
      "Epoch 607 - Loss: 0.0540\n",
      "Epoch 608 - Loss: 0.0516\n",
      "Epoch 609 - Loss: 0.0520\n",
      "Epoch 610 - Loss: 0.0518\n",
      "Epoch 611 - Loss: 0.0484\n",
      "Loss improved from 0.0508 to 0.0484. Saving model...\n",
      "Epoch 612 - Loss: 0.0495\n",
      "Epoch 613 - Loss: 0.0519\n",
      "Epoch 614 - Loss: 0.0509\n",
      "Epoch 615 - Loss: 0.0532\n",
      "Epoch 616 - Loss: 0.0552\n",
      "Epoch 617 - Loss: 0.0540\n",
      "Epoch 618 - Loss: 0.0528\n",
      "Epoch 619 - Loss: 0.0525\n",
      "Epoch 620 - Loss: 0.0541\n",
      "Epoch 621 - Loss: 0.0534\n",
      "Epoch 622 - Loss: 0.0544\n",
      "Epoch 623 - Loss: 0.0540\n",
      "Epoch 624 - Loss: 0.0548\n",
      "Epoch 625 - Loss: 0.0581\n",
      "Epoch 626 - Loss: 0.0559\n",
      "Epoch 627 - Loss: 0.0499\n",
      "Epoch 628 - Loss: 0.0484\n",
      "Loss improved from 0.0484 to 0.0484. Saving model...\n",
      "Epoch 629 - Loss: 0.0486\n",
      "Epoch 630 - Loss: 0.0469\n",
      "Loss improved from 0.0484 to 0.0469. Saving model...\n",
      "Epoch 631 - Loss: 0.0509\n",
      "Epoch 632 - Loss: 0.0497\n",
      "Epoch 633 - Loss: 0.0492\n",
      "Epoch 634 - Loss: 0.0468\n",
      "Loss improved from 0.0469 to 0.0468. Saving model...\n",
      "Epoch 635 - Loss: 0.0485\n",
      "Epoch 636 - Loss: 0.0494\n",
      "Epoch 637 - Loss: 0.0496\n",
      "Epoch 638 - Loss: 0.0504\n",
      "Epoch 639 - Loss: 0.0520\n",
      "Epoch 640 - Loss: 0.0513\n",
      "Epoch 641 - Loss: 0.0505\n",
      "Epoch 642 - Loss: 0.0502\n",
      "Epoch 643 - Loss: 0.0494\n",
      "Epoch 644 - Loss: 0.0504\n",
      "Epoch 645 - Loss: 0.0503\n",
      "Epoch 646 - Loss: 0.0493\n",
      "Epoch 647 - Loss: 0.0506\n",
      "Epoch 648 - Loss: 0.0509\n",
      "Epoch 649 - Loss: 0.0486\n",
      "Epoch 650 - Loss: 0.0474\n",
      "Epoch 651 - Loss: 0.0465\n",
      "Loss improved from 0.0468 to 0.0465. Saving model...\n",
      "Epoch 652 - Loss: 0.0454\n",
      "Loss improved from 0.0465 to 0.0454. Saving model...\n",
      "Epoch 653 - Loss: 0.0464\n",
      "Epoch 654 - Loss: 0.0469\n",
      "Epoch 655 - Loss: 0.0467\n",
      "Epoch 656 - Loss: 0.0468\n",
      "Epoch 657 - Loss: 0.0492\n",
      "Epoch 658 - Loss: 0.0464\n",
      "Epoch 659 - Loss: 0.0457\n",
      "Epoch 660 - Loss: 0.0471\n",
      "Epoch 661 - Loss: 0.0474\n",
      "Epoch 662 - Loss: 0.0437\n",
      "Loss improved from 0.0454 to 0.0437. Saving model...\n",
      "Epoch 663 - Loss: 0.0455\n",
      "Epoch 664 - Loss: 0.0486\n",
      "Epoch 665 - Loss: 0.0506\n",
      "Epoch 666 - Loss: 0.0483\n",
      "Epoch 667 - Loss: 0.0478\n",
      "Epoch 668 - Loss: 0.0470\n",
      "Epoch 669 - Loss: 0.0474\n",
      "Epoch 670 - Loss: 0.0490\n",
      "Epoch 671 - Loss: 0.0487\n",
      "Epoch 672 - Loss: 0.0466\n",
      "Epoch 673 - Loss: 0.0607\n",
      "Epoch 674 - Loss: 0.0557\n",
      "Epoch 675 - Loss: 0.0531\n",
      "Epoch 676 - Loss: 0.0551\n",
      "Epoch 677 - Loss: 0.0848\n",
      "Epoch 678 - Loss: 0.0561\n",
      "Epoch 679 - Loss: 0.0479\n",
      "Epoch 680 - Loss: 0.0449\n",
      "Epoch 681 - Loss: 0.0457\n",
      "Epoch 682 - Loss: 0.0427\n",
      "Loss improved from 0.0437 to 0.0427. Saving model...\n",
      "Epoch 683 - Loss: 0.0407\n",
      "Loss improved from 0.0427 to 0.0407. Saving model...\n",
      "Epoch 684 - Loss: 0.0400\n",
      "Loss improved from 0.0407 to 0.0400. Saving model...\n",
      "Epoch 685 - Loss: 0.0392\n",
      "Loss improved from 0.0400 to 0.0392. Saving model...\n",
      "Epoch 686 - Loss: 0.0412\n",
      "Epoch 687 - Loss: 0.0423\n",
      "Epoch 688 - Loss: 0.0449\n",
      "Epoch 689 - Loss: 0.0433\n",
      "Epoch 690 - Loss: 0.0456\n",
      "Epoch 691 - Loss: 0.0633\n",
      "Epoch 692 - Loss: 0.0508\n",
      "Epoch 693 - Loss: 0.0467\n",
      "Epoch 694 - Loss: 0.0413\n",
      "Epoch 695 - Loss: 0.0411\n",
      "Epoch 696 - Loss: 0.0403\n",
      "Epoch 697 - Loss: 0.0407\n",
      "Epoch 698 - Loss: 0.0402\n",
      "Epoch 699 - Loss: 0.0400\n",
      "Epoch 700 - Loss: 0.0394\n",
      "Epoch 701 - Loss: 0.0429\n",
      "Epoch 702 - Loss: 0.0434\n",
      "Epoch 703 - Loss: 0.0449\n",
      "Epoch 704 - Loss: 0.0435\n",
      "Epoch 705 - Loss: 0.0429\n",
      "Epoch 706 - Loss: 0.0432\n",
      "Epoch 707 - Loss: 0.0407\n",
      "Epoch 708 - Loss: 0.0401\n",
      "Epoch 709 - Loss: 0.0399\n",
      "Epoch 710 - Loss: 0.0408\n",
      "Epoch 711 - Loss: 0.0466\n",
      "Epoch 712 - Loss: 0.0406\n",
      "Epoch 713 - Loss: 0.0406\n",
      "Epoch 714 - Loss: 0.0413\n",
      "Epoch 715 - Loss: 0.0420\n",
      "Epoch 716 - Loss: 0.0441\n",
      "Epoch 717 - Loss: 0.0421\n",
      "Epoch 718 - Loss: 0.0448\n",
      "Epoch 719 - Loss: 0.0444\n",
      "Epoch 720 - Loss: 0.0463\n",
      "Epoch 721 - Loss: 0.0445\n",
      "Epoch 722 - Loss: 0.0404\n",
      "Epoch 723 - Loss: 0.0384\n",
      "Loss improved from 0.0392 to 0.0384. Saving model...\n",
      "Epoch 724 - Loss: 0.0373\n",
      "Loss improved from 0.0384 to 0.0373. Saving model...\n",
      "Epoch 725 - Loss: 0.0366\n",
      "Loss improved from 0.0373 to 0.0366. Saving model...\n",
      "Epoch 726 - Loss: 0.0379\n",
      "Epoch 727 - Loss: 0.0396\n",
      "Epoch 728 - Loss: 0.0440\n",
      "Epoch 729 - Loss: 0.0438\n",
      "Epoch 730 - Loss: 0.0416\n",
      "Epoch 731 - Loss: 0.0403\n",
      "Epoch 732 - Loss: 0.0393\n",
      "Epoch 733 - Loss: 0.0403\n",
      "Epoch 734 - Loss: 0.0391\n",
      "Epoch 735 - Loss: 0.0409\n",
      "Epoch 736 - Loss: 0.0402\n",
      "Epoch 737 - Loss: 0.0417\n",
      "Epoch 738 - Loss: 0.0417\n",
      "Epoch 739 - Loss: 0.0386\n",
      "Epoch 740 - Loss: 0.0409\n",
      "Epoch 741 - Loss: 0.0436\n",
      "Epoch 742 - Loss: 0.0431\n",
      "Epoch 743 - Loss: 0.0449\n",
      "Epoch 744 - Loss: 0.0399\n",
      "Epoch 745 - Loss: 0.0388\n",
      "Epoch 746 - Loss: 0.0442\n",
      "Epoch 747 - Loss: 0.0443\n",
      "Epoch 748 - Loss: 0.0399\n",
      "Epoch 749 - Loss: 0.0385\n",
      "Epoch 750 - Loss: 0.0351\n",
      "Loss improved from 0.0366 to 0.0351. Saving model...\n",
      "Epoch 751 - Loss: 0.0340\n",
      "Loss improved from 0.0351 to 0.0340. Saving model...\n",
      "Epoch 752 - Loss: 0.0359\n",
      "Epoch 753 - Loss: 0.0361\n",
      "Epoch 754 - Loss: 0.0345\n",
      "Epoch 755 - Loss: 0.0371\n",
      "Epoch 756 - Loss: 0.0355\n",
      "Epoch 757 - Loss: 0.0360\n",
      "Epoch 758 - Loss: 0.0373\n",
      "Epoch 759 - Loss: 0.0365\n",
      "Epoch 760 - Loss: 0.0367\n",
      "Epoch 761 - Loss: 0.0460\n",
      "Epoch 762 - Loss: 0.0410\n",
      "Epoch 763 - Loss: 0.0451\n",
      "Epoch 764 - Loss: 0.0468\n",
      "Epoch 765 - Loss: 0.0401\n",
      "Epoch 766 - Loss: 0.0383\n",
      "Epoch 767 - Loss: 0.0353\n",
      "Epoch 768 - Loss: 0.0365\n",
      "Epoch 769 - Loss: 0.0360\n",
      "Epoch 770 - Loss: 0.0355\n",
      "Epoch 771 - Loss: 0.0351\n",
      "Epoch 772 - Loss: 0.0334\n",
      "Loss improved from 0.0340 to 0.0334. Saving model...\n",
      "Epoch 773 - Loss: 0.0365\n",
      "Epoch 774 - Loss: 0.0360\n",
      "Epoch 775 - Loss: 0.0381\n",
      "Epoch 776 - Loss: 0.0372\n",
      "Epoch 777 - Loss: 0.0365\n",
      "Epoch 778 - Loss: 0.0345\n",
      "Epoch 779 - Loss: 0.0370\n",
      "Epoch 780 - Loss: 0.0447\n",
      "Epoch 781 - Loss: 0.0407\n",
      "Epoch 782 - Loss: 0.0356\n",
      "Epoch 783 - Loss: 0.0351\n",
      "Epoch 784 - Loss: 0.0374\n",
      "Epoch 785 - Loss: 0.0406\n",
      "Epoch 786 - Loss: 0.0364\n",
      "Epoch 787 - Loss: 0.0391\n",
      "Epoch 788 - Loss: 0.0428\n",
      "Epoch 789 - Loss: 0.0454\n",
      "Epoch 790 - Loss: 0.0402\n",
      "Epoch 791 - Loss: 0.0368\n",
      "Epoch 792 - Loss: 0.0379\n",
      "Epoch 793 - Loss: 0.0406\n",
      "Epoch 794 - Loss: 0.0405\n",
      "Epoch 795 - Loss: 0.0351\n",
      "Epoch 796 - Loss: 0.0318\n",
      "Loss improved from 0.0334 to 0.0318. Saving model...\n",
      "Epoch 797 - Loss: 0.0312\n",
      "Loss improved from 0.0318 to 0.0312. Saving model...\n",
      "Epoch 798 - Loss: 0.0301\n",
      "Loss improved from 0.0312 to 0.0301. Saving model...\n",
      "Epoch 799 - Loss: 0.0311\n",
      "Epoch 800 - Loss: 0.0323\n",
      "Epoch 801 - Loss: 0.0328\n",
      "Epoch 802 - Loss: 0.0321\n",
      "Epoch 803 - Loss: 0.0345\n",
      "Epoch 804 - Loss: 0.0371\n",
      "Epoch 805 - Loss: 0.0368\n",
      "Epoch 806 - Loss: 0.0429\n",
      "Epoch 807 - Loss: 0.0358\n",
      "Epoch 808 - Loss: 0.0354\n",
      "Epoch 809 - Loss: 0.0337\n",
      "Epoch 810 - Loss: 0.0352\n",
      "Epoch 811 - Loss: 0.0334\n",
      "Epoch 812 - Loss: 0.0329\n",
      "Epoch 813 - Loss: 0.0340\n",
      "Epoch 814 - Loss: 0.0316\n",
      "Epoch 815 - Loss: 0.0327\n",
      "Epoch 816 - Loss: 0.0309\n",
      "Epoch 817 - Loss: 0.0331\n",
      "Epoch 818 - Loss: 0.0312\n",
      "Epoch 819 - Loss: 0.0303\n",
      "Epoch 820 - Loss: 0.0307\n",
      "Epoch 821 - Loss: 0.0316\n",
      "Epoch 822 - Loss: 0.0329\n",
      "Epoch 823 - Loss: 0.0342\n",
      "Epoch 824 - Loss: 0.0362\n",
      "Epoch 825 - Loss: 0.0315\n",
      "Epoch 826 - Loss: 0.0335\n",
      "Epoch 827 - Loss: 0.0322\n",
      "Epoch 828 - Loss: 0.0320\n",
      "Epoch 829 - Loss: 0.0301\n",
      "Loss improved from 0.0301 to 0.0301. Saving model...\n",
      "Epoch 830 - Loss: 0.0312\n",
      "Epoch 831 - Loss: 0.0350\n",
      "Epoch 832 - Loss: 0.0362\n",
      "Epoch 833 - Loss: 0.0523\n",
      "Epoch 834 - Loss: 0.0457\n",
      "Epoch 835 - Loss: 0.0383\n",
      "Epoch 836 - Loss: 0.0341\n",
      "Epoch 837 - Loss: 0.0343\n",
      "Epoch 838 - Loss: 0.0341\n",
      "Epoch 839 - Loss: 0.0316\n",
      "Epoch 840 - Loss: 0.0299\n",
      "Loss improved from 0.0301 to 0.0299. Saving model...\n",
      "Epoch 841 - Loss: 0.0292\n",
      "Loss improved from 0.0299 to 0.0292. Saving model...\n",
      "Epoch 842 - Loss: 0.0287\n",
      "Loss improved from 0.0292 to 0.0287. Saving model...\n",
      "Epoch 843 - Loss: 0.0313\n",
      "Epoch 844 - Loss: 0.0295\n",
      "Epoch 845 - Loss: 0.0313\n",
      "Epoch 846 - Loss: 0.0284\n",
      "Loss improved from 0.0287 to 0.0284. Saving model...\n",
      "Epoch 847 - Loss: 0.0285\n",
      "Epoch 848 - Loss: 0.0307\n",
      "Epoch 849 - Loss: 0.0310\n",
      "Epoch 850 - Loss: 0.0293\n",
      "Epoch 851 - Loss: 0.0295\n",
      "Epoch 852 - Loss: 0.0308\n",
      "Epoch 853 - Loss: 0.0342\n",
      "Epoch 854 - Loss: 0.0308\n",
      "Epoch 855 - Loss: 0.0320\n",
      "Epoch 856 - Loss: 0.0326\n",
      "Epoch 857 - Loss: 0.0314\n",
      "Epoch 858 - Loss: 0.0328\n",
      "Epoch 859 - Loss: 0.0296\n",
      "Epoch 860 - Loss: 0.0295\n",
      "Epoch 861 - Loss: 0.0368\n",
      "Epoch 862 - Loss: 0.0351\n",
      "Epoch 863 - Loss: 0.0324\n",
      "Epoch 864 - Loss: 0.0310\n",
      "Epoch 865 - Loss: 0.0303\n",
      "Epoch 866 - Loss: 0.0306\n",
      "Epoch 867 - Loss: 0.0342\n",
      "Epoch 868 - Loss: 0.0364\n",
      "Epoch 869 - Loss: 0.0346\n",
      "Epoch 870 - Loss: 0.0322\n",
      "Epoch 871 - Loss: 0.0310\n",
      "Epoch 872 - Loss: 0.0306\n",
      "Epoch 873 - Loss: 0.0324\n",
      "Epoch 874 - Loss: 0.0286\n",
      "Epoch 875 - Loss: 0.0279\n",
      "Loss improved from 0.0284 to 0.0279. Saving model...\n",
      "Epoch 876 - Loss: 0.0270\n",
      "Loss improved from 0.0279 to 0.0270. Saving model...\n",
      "Epoch 877 - Loss: 0.0257\n",
      "Loss improved from 0.0270 to 0.0257. Saving model...\n",
      "Epoch 878 - Loss: 0.0256\n",
      "Loss improved from 0.0257 to 0.0256. Saving model...\n",
      "Epoch 879 - Loss: 0.0274\n",
      "Epoch 880 - Loss: 0.0295\n",
      "Epoch 881 - Loss: 0.0269\n",
      "Epoch 882 - Loss: 0.0307\n",
      "Epoch 883 - Loss: 0.0301\n",
      "Epoch 884 - Loss: 0.0288\n",
      "Epoch 885 - Loss: 0.0379\n",
      "Epoch 886 - Loss: 0.0361\n",
      "Epoch 887 - Loss: 0.0307\n",
      "Epoch 888 - Loss: 0.0289\n",
      "Epoch 889 - Loss: 0.0278\n",
      "Epoch 890 - Loss: 0.0268\n",
      "Epoch 891 - Loss: 0.0297\n",
      "Epoch 892 - Loss: 0.0284\n",
      "Epoch 893 - Loss: 0.0267\n",
      "Epoch 894 - Loss: 0.0285\n",
      "Epoch 895 - Loss: 0.0289\n",
      "Epoch 896 - Loss: 0.0279\n",
      "Epoch 897 - Loss: 0.0263\n",
      "Epoch 898 - Loss: 0.0282\n",
      "Epoch 899 - Loss: 0.0292\n",
      "Epoch 900 - Loss: 0.0299\n",
      "Epoch 901 - Loss: 0.0319\n",
      "Epoch 902 - Loss: 0.0331\n",
      "Epoch 903 - Loss: 0.0362\n",
      "Epoch 904 - Loss: 0.0308\n",
      "Epoch 905 - Loss: 0.0295\n",
      "Epoch 906 - Loss: 0.0274\n",
      "Epoch 907 - Loss: 0.0252\n",
      "Loss improved from 0.0256 to 0.0252. Saving model...\n",
      "Epoch 908 - Loss: 0.0259\n",
      "Epoch 909 - Loss: 0.0258\n",
      "Epoch 910 - Loss: 0.0242\n",
      "Loss improved from 0.0252 to 0.0242. Saving model...\n",
      "Epoch 911 - Loss: 0.0261\n",
      "Epoch 912 - Loss: 0.0259\n",
      "Epoch 913 - Loss: 0.0286\n",
      "Epoch 914 - Loss: 0.0284\n",
      "Epoch 915 - Loss: 0.0260\n",
      "Epoch 916 - Loss: 0.0250\n",
      "Epoch 917 - Loss: 0.0277\n",
      "Epoch 918 - Loss: 0.0274\n",
      "Epoch 919 - Loss: 0.0283\n",
      "Epoch 920 - Loss: 0.0323\n",
      "Epoch 921 - Loss: 0.0287\n",
      "Epoch 922 - Loss: 0.0288\n",
      "Epoch 923 - Loss: 0.0281\n",
      "Epoch 924 - Loss: 0.0260\n",
      "Epoch 925 - Loss: 0.0256\n",
      "Epoch 926 - Loss: 0.0272\n",
      "Epoch 927 - Loss: 0.0277\n",
      "Epoch 928 - Loss: 0.0266\n",
      "Epoch 929 - Loss: 0.0309\n",
      "Epoch 930 - Loss: 0.0274\n",
      "Epoch 931 - Loss: 0.0261\n",
      "Epoch 932 - Loss: 0.0287\n",
      "Epoch 933 - Loss: 0.0254\n",
      "Epoch 934 - Loss: 0.0258\n",
      "Epoch 935 - Loss: 0.0281\n",
      "Epoch 936 - Loss: 0.0276\n",
      "Epoch 937 - Loss: 0.0270\n",
      "Epoch 938 - Loss: 0.0246\n",
      "Epoch 939 - Loss: 0.0249\n",
      "Epoch 940 - Loss: 0.0257\n",
      "Epoch 941 - Loss: 0.0273\n",
      "Epoch 942 - Loss: 0.0312\n",
      "Epoch 943 - Loss: 0.0329\n",
      "Epoch 944 - Loss: 0.0288\n",
      "Epoch 945 - Loss: 0.0267\n",
      "Epoch 946 - Loss: 0.0307\n",
      "Epoch 947 - Loss: 0.0280\n",
      "Epoch 948 - Loss: 0.0262\n",
      "Epoch 949 - Loss: 0.0252\n",
      "Epoch 950 - Loss: 0.0259\n",
      "Epoch 951 - Loss: 0.0255\n",
      "Epoch 952 - Loss: 0.0234\n",
      "Loss improved from 0.0242 to 0.0234. Saving model...\n",
      "Epoch 953 - Loss: 0.0233\n",
      "Loss improved from 0.0234 to 0.0233. Saving model...\n",
      "Epoch 954 - Loss: 0.0263\n",
      "Epoch 955 - Loss: 0.0249\n",
      "Epoch 956 - Loss: 0.0252\n",
      "Epoch 957 - Loss: 0.0241\n",
      "Epoch 958 - Loss: 0.0251\n",
      "Epoch 959 - Loss: 0.0248\n",
      "Epoch 960 - Loss: 0.0265\n",
      "Epoch 961 - Loss: 0.0260\n",
      "Epoch 962 - Loss: 0.0252\n",
      "Epoch 963 - Loss: 0.0232\n",
      "Loss improved from 0.0233 to 0.0232. Saving model...\n",
      "Epoch 964 - Loss: 0.0265\n",
      "Epoch 965 - Loss: 0.0261\n",
      "Epoch 966 - Loss: 0.0275\n",
      "Epoch 967 - Loss: 0.0253\n",
      "Epoch 968 - Loss: 0.0322\n",
      "Epoch 969 - Loss: 0.0276\n",
      "Epoch 970 - Loss: 0.0284\n",
      "Epoch 971 - Loss: 0.0304\n",
      "Epoch 972 - Loss: 0.0291\n",
      "Epoch 973 - Loss: 0.0307\n",
      "Epoch 974 - Loss: 0.0314\n",
      "Epoch 975 - Loss: 0.0301\n",
      "Epoch 976 - Loss: 0.0270\n",
      "Epoch 977 - Loss: 0.0240\n",
      "Epoch 978 - Loss: 0.0218\n",
      "Loss improved from 0.0232 to 0.0218. Saving model...\n",
      "Epoch 979 - Loss: 0.0260\n",
      "Epoch 980 - Loss: 0.0233\n",
      "Epoch 981 - Loss: 0.0248\n",
      "Epoch 982 - Loss: 0.0247\n",
      "Epoch 983 - Loss: 0.0258\n",
      "Epoch 984 - Loss: 0.0226\n",
      "Epoch 985 - Loss: 0.0218\n",
      "Loss improved from 0.0218 to 0.0218. Saving model...\n",
      "Epoch 986 - Loss: 0.0217\n",
      "Loss improved from 0.0218 to 0.0217. Saving model...\n",
      "Epoch 987 - Loss: 0.0223\n",
      "Epoch 988 - Loss: 0.0238\n",
      "Epoch 989 - Loss: 0.0260\n",
      "Epoch 990 - Loss: 0.0438\n",
      "Epoch 991 - Loss: 0.0383\n",
      "Epoch 992 - Loss: 0.0304\n",
      "Epoch 993 - Loss: 0.0288\n",
      "Epoch 994 - Loss: 0.0251\n",
      "Epoch 995 - Loss: 0.0230\n",
      "Epoch 996 - Loss: 0.0223\n",
      "Epoch 997 - Loss: 0.0223\n",
      "Epoch 998 - Loss: 0.0217\n",
      "Loss improved from 0.0217 to 0.0217. Saving model...\n",
      "Epoch 999 - Loss: 0.0217\n",
      "Epoch 1000 - Loss: 0.0223\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#vocab_size = len(valid_token_id_set)\n",
    "vocab_size = len(tokenizer)\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "pad_token = tokenizer.vocab.get(\"PAD_None\", -100)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "\n",
    "best_loss = float(\"inf\")  # Initialize best loss to very large value\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])  # teacher forcing\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model if there is significant improvement\n",
    "    if avg_loss < best_loss:\n",
    "        print(f\"Loss improved from {best_loss:.4f} to {avg_loss:.4f}. Saving model...\")\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), \"music_transformer_weights.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"music_transformer_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "def score_to_midi(score_tick):\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Correctly access instruments/tracks from ScoreTick\n",
    "    try:\n",
    "        for track in score_tick.tracks:  #  this is the fix\n",
    "            midi_instr = Instrument(\n",
    "                program=track.program,\n",
    "                is_drum=track.is_drum,\n",
    "                name=track.name\n",
    "            )\n",
    "            for note in track.notes:\n",
    "                midi_instr.notes.append(Note(\n",
    "                    pitch=note.pitch,\n",
    "                    start=note.start,\n",
    "                    end=note.end,\n",
    "                    velocity=note.velocity\n",
    "                ))\n",
    "            midi.instruments.append(midi_instr)\n",
    "    except AttributeError as e:\n",
    "        raise ValueError(\"Provided object does not contain valid MIDI track info\") from e\n",
    "\n",
    "    return midi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    output_path,\n",
    "    max_len=1024,\n",
    "    device=\"cpu\",\n",
    "    valid_token_ids=None,  #  NEW: pass a list or set of allowed IDs\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Ensure right_hand_tokens is a batched tensor\n",
    "    if isinstance(right_hand_tokens, list):\n",
    "        input_ids = torch.tensor([right_hand_tokens], dtype=torch.long, device=device)\n",
    "    elif isinstance(right_hand_tokens, torch.Tensor):\n",
    "        if right_hand_tokens.ndim == 1:\n",
    "            input_ids = right_hand_tokens.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            input_ids = right_hand_tokens.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"right_hand_tokens must be a list of ints or a torch.Tensor\")\n",
    "\n",
    "    bos_token_id = tokenizer.vocab.get(\"BOS_None\", tokenizer.vocab.get(\"BOS\", 0))\n",
    "    eos_token_id = tokenizer.vocab.get(\"EOS_None\", tokenizer.vocab.get(\"EOS\", -1))\n",
    "\n",
    "    decoder_input = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    #vocab_size = len(valid_token_id_set)\n",
    "\n",
    "    mask_tensor = torch.full((vocab_size,), float('-inf'), device=device)\n",
    "\n",
    "    mask_tensor[valid_token_ids] = 0.0\n",
    "\n",
    "\n",
    "    # Autoregressive generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_ids, decoder_input)  # (batch, seq, vocab)\n",
    "            next_token_logits = output[:, -1, :]      # (batch, vocab)\n",
    "\n",
    "            # Apply mask\n",
    "            next_token_logits = next_token_logits + mask_tensor\n",
    "\n",
    "            # Top-k sampling (top 50)\n",
    "            top_k = 50\n",
    "            top_logits, top_indices = torch.topk(next_token_logits, k=top_k, dim=-1)  # (batch, top_k)\n",
    "            probs = torch.softmax(top_logits, dim=-1)\n",
    "            sampled = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
    "            next_token = top_indices.gather(1, sampled)        # (batch, 1)\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    left_hand_tokens = decoder_input.squeeze(0).tolist()\n",
    "    left_hand_tokens[0:2] = input_ids.squeeze(0).tolist()[0:2]\n",
    "\n",
    "    # Decode to Score objects\n",
    "    print('right tokens', input_ids.squeeze(0).tolist())\n",
    "    print('left tokens', left_hand_tokens)\n",
    "    \n",
    "\n",
    "    right_score = tokenizer.decode(input_ids.squeeze(0).tolist())\n",
    "    left_score = tokenizer.decode(left_hand_tokens)\n",
    "    \n",
    "    print('right score:', right_score)\n",
    "    print('left score:', left_score)\n",
    "\n",
    "    # Convert to MIDI\n",
    "    right_midi = score_to_midi(right_score)\n",
    "    left_midi = score_to_midi(left_score)\n",
    "\n",
    "    #print(right_midi.instruments[0])\n",
    "\n",
    "\n",
    "    print('left midi', left_midi)\n",
    "    # Create MIDI\n",
    "    # Create new MIDI and combine tracks\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Append notes from decoded MIDI objects\n",
    "    for track, program, name in zip([right_midi, left_midi], [0, 0], [\"RH-1\", \"LH-1\"]):\n",
    "        inst = Instrument(program=program, is_drum=False, name=name)\n",
    "        # Take notes from the first instrument in the decoded track\n",
    "        decoded_inst = track.instruments[0]\n",
    "        inst.notes.extend(decoded_inst.notes)\n",
    "        midi.instruments.append(inst)\n",
    "    midi.ticks_per_beat=32\n",
    "\n",
    "    midi.dump(str(output_path))\n",
    "\n",
    "    print(f\"Saved MIDI to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right tokens [4, 897, 557, 868, 888, 36, 149, 157, 562, 888, 37, 149, 157, 824, 568, 888, 38, 149, 157, 573, 888, 39, 149, 158, 824, 584, 888, 38, 149, 157, 589, 888, 39, 149, 158, 824, 599, 888, 48, 149, 157, 824, 605, 888, 49, 149, 157, 610, 888, 50, 149, 157, 615, 888, 51, 149, 158, 824, 561, 888, 50, 149, 157, 824, 567, 888, 51, 149, 158, 824, 577, 888, 48, 149, 158, 888, 54, 149, 158, 888, 56, 149, 158, 824, 587, 888, 48, 149, 157, 888, 54, 149, 157, 888, 57, 149, 157, 592, 888, 48, 149, 158, 888, 54, 149, 158, 888, 58, 149, 158, 824, 602, 888, 48, 149, 157, 888, 54, 149, 157, 888, 57, 149, 157, 607, 888, 48, 149, 158, 888, 54, 149, 158, 888, 58, 149, 158, 824, 618, 888, 48, 149, 158, 888, 54, 149, 158, 888, 56, 149, 158, 829, 574, 888, 56, 156, 157, 888, 60, 156, 157, 888, 68, 156, 157, 579, 888, 56, 149, 157, 888, 60, 149, 157, 888, 68, 149, 157, 583, 888, 58, 149, 157, 888, 61, 149, 157, 888, 70, 149, 157, 588, 888, 60, 149, 157, 888, 63, 149, 157, 888, 72, 149, 157, 593, 888, 61, 138, 158, 888, 65, 138, 158, 888, 73, 138, 158, 824, 603, 888, 58, 138, 157, 888, 61, 138, 157, 888, 70, 138, 157, 608, 888, 56, 138, 158, 888, 61, 138, 158, 888, 68, 138, 158, 824, 618, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 4, 897, 559, 888, 52, 138, 157, 888, 55, 138, 157, 888, 64, 138, 157, 564, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 569, 888, 56, 138, 158, 888, 60, 138, 158, 888, 68, 138, 158, 824, 579, 888, 54, 138, 157, 888, 66, 138, 157, 584, 888, 51, 138, 161, 888, 63, 138, 161, 830, 618, 888, 56, 138, 157, 888, 66, 138, 157, 888, 68, 138, 157, 4, 897, 559, 888, 58, 138, 158, 888, 66, 138, 158, 888, 70, 138, 158, 824, 569, 888, 56, 138, 157, 888, 66, 138, 157, 888, 68, 138, 157, 574, 888, 58, 138, 158, 888, 66, 138, 158, 888, 70, 138, 158, 824, 584, 888, 53, 138, 162, 888, 61, 138, 162, 888, 65, 138, 162, 608, 888, 56, 138, 157, 888, 60, 138, 157, 888, 68, 138, 157, 613, 888, 58, 138, 157, 888, 61, 138, 157, 888, 70, 138, 157, 618, 888, 60, 138, 157, 888, 63, 138, 157, 888, 72, 138, 157, 4, 897, 559, 888, 61, 138, 158, 888, 65, 138, 158, 888, 73, 138, 158, 824, 569, 888, 58, 138, 157, 888, 61, 138, 157, 888, 70, 138, 157, 574, 888, 56, 138, 158, 888, 61, 138, 158, 888, 68, 138, 158, 824, 584, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 588, 888, 52, 138, 157, 888, 55, 138, 157, 888, 64, 138, 157, 593, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 598, 888, 56, 138, 158, 888, 60, 138, 158, 888, 68, 138, 158, 824, 608, 888, 54, 138, 157, 888, 66, 138, 157, 613, 888, 51, 138, 161, 888, 63, 138, 161, 828, 579, 888, 51, 138, 157, 888, 55, 138, 157, 888, 63, 138, 157, 584, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 588, 888, 55, 138, 157, 888, 58, 138, 157, 888, 67, 138, 157, 593, 888, 56, 138, 157, 888, 60, 138, 157, 888, 68, 138, 157, 598, 888, 58, 138, 158, 888, 61, 138, 158, 888, 70, 138, 158, 824, 608, 888, 60, 138, 158, 888, 63, 138, 158, 888, 72, 138, 158, 824, 618, 888, 59, 138, 157, 888, 62, 138, 157, 888, 71, 138, 157, 4, 897, 559, 888, 60, 138, 157, 888, 63, 138, 157, 888, 72, 138, 157, 564, 888, 56, 138, 158, 888, 60, 138, 158, 888, 68, 138, 158, 824, 574, 888, 56, 138, 157, 888, 60, 138, 157, 888, 68, 138, 157, 579, 888, 58, 138, 157, 888, 61, 138, 157, 888, 70, 138, 157, 584, 888, 60, 138, 157, 888, 63, 138, 157, 888, 72, 138, 157, 588, 888, 61, 138, 158, 888, 65, 138, 158, 888, 73, 138, 158, 824, 598, 888, 58, 138, 157, 888, 61, 138, 157, 888, 70, 138, 157, 603, 888, 56, 138, 158, 888, 61, 138, 158, 888, 68, 138, 158, 824, 613, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 618, 888, 52, 138, 157, 888, 55, 138, 157, 888, 64, 138, 157, 4, 897, 559, 888, 53, 138, 157, 888, 56, 138, 157, 888, 65, 138, 157, 564, 888, 56, 138, 158, 888, 60, 138, 158, 888, 68, 138, 158, 824, 574, 888, 54, 138, 157, 888, 66, 138, 157, 579, 888, 51, 138, 161, 888, 63, 138, 161, 830, 613, 888, 56, 138, 157, 888, 66, 138, 157, 888, 68, 138, 157, 618, 888, 58, 138, 158, 888, 66, 138, 158, 888, 70, 138, 158, 824, 564, 888, 56, 138, 157, 888, 66, 138, 157, 888, 68, 138, 157, 569, 888, 58, 138, 158, 888, 66, 138, 158, 888, 70, 138, 158, 824, 579, 888, 53, 138, 161, 888, 61, 138, 161, 888, 65, 138, 161, 833, 559, 888, 55, 138, 157, 564, 888, 58, 138, 157, 569, 888, 61, 138, 157, 574, 888, 64, 138, 157, 579, 888, 61, 138, 157, 584, 888, 58, 138, 157, 588, 888, 55, 138, 157, 593, 888, 56, 138, 157, 888, 68, 138, 157, 598, 888, 65, 138, 157, 603, 888, 61, 138, 157, 608, 888, 56, 138, 158, 824, 618, 888, 43, 138, 157, 4, 897, 559, 888, 46, 138, 157, 564, 888, 49, 138, 157, 569, 888, 53, 138, 157, 574, 888, 43, 138, 157, 579, 888, 49, 138, 157, 584, 888, 43, 138, 158, 888, 53, 138, 158, 593, 888, 49, 138, 157, 598, 888, 42, 138, 158, 888, 48, 138, 158, 888, 51, 138, 158, 824, 608, 888, 41, 138, 159, 888, 44, 138, 159, 888, 49, 138, 159, 829, 569, 888, 56, 138]\n",
      "left tokens [4, 897, 811, 888, 49, 149, 158, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 588, 888, 616, 888, 888, 888, 888, 888, 888, 888, 888, 610, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 42, 888, 615, 888, 888, 611, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 603, 888, 888, 888, 888, 888, 888, 888, 888, 888, 149, 888, 888, 888, 888, 888, 811, 888, 615, 888, 888, 888, 888, 888, 888, 43, 888, 888, 27, 888, 888, 888, 888, 888, 42, 888, 888, 888, 888, 888, 42, 888, 888, 888, 888, 888, 41, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 41, 888, 888, 888, 888, 888, 42, 888, 888, 27, 888, 30, 149, 888, 888, 888, 888, 34, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 43, 888, 888, 888, 29, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 42, 888, 888, 888, 40, 888, 888, 615, 888, 888, 888, 888, 888, 888, 41, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 603, 888, 888, 888, 888, 888, 888, 888, 888, 40, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 615, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 41, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 29, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 42, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 36, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 610, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 44, 888, 888, 888, 811, 888, 888, 888, 35, 888, 846, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 40, 603, 888, 888, 888, 888, 27, 129, 168, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 599, 888, 836, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 615, 888, 888, 833, 599, 888, 34, 615, 888, 888, 888, 60, 149, 161, 888, 42, 888, 811, 888, 888, 888, 888, 615, 888, 888, 810, 888, 888, 888, 42, 888, 888, 888, 888, 888, 888, 888, 888, 836, 888, 888, 888, 888, 888, 615, 888, 603, 888, 888, 888, 27, 615, 888, 888, 615, 888, 888, 615, 888, 888, 34, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 42, 615, 888, 888, 888, 888, 615, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 27, 615, 888, 888, 39, 888, 888, 34, 827, 888, 846, 615, 888, 888, 888, 888, 888, 888, 888, 613, 888, 39, 615, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 132, 888, 615, 888, 888, 888, 888, 888, 888, 603, 888, 888, 615, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 41, 888, 888, 615, 888, 888, 888, 888, 888, 888, 41, 149, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 603, 888, 824, 888, 888, 811, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 38, 888, 615, 888, 615, 888, 888, 888, 888, 888, 588, 888, 888, 29, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 27, 888, 888, 888, 888, 888, 888, 888, 888, 888, 599, 888, 888, 40, 603, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 603, 888, 888, 888, 888, 888, 888, 888, 39, 599, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 603, 888, 615, 888, 888, 888, 888, 888, 603, 888, 888, 888, 888, 888, 888, 888, 888, 888, 599, 888, 888, 888, 888, 888, 615, 888, 888, 168, 888, 888, 888, 888, 888, 888, 888, 41, 149, 888, 888, 888, 36, 615, 888, 599, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 824, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 27, 615, 888, 888, 888, 888, 888, 888, 888, 811, 888, 888, 615, 888, 888, 888, 888, 888, 888, 888, 846, 888, 888, 888, 811, 888, 888, 888, 888, 27, 615, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 888, 41, 888, 888, 888, 888]\n",
      "right score: Score(ttype=Tick, tpq=32, begin=0, end=1534, tracks=1, notes=219, time_sig=1, key_sig=0, markers=0)\n",
      "left score: Score(ttype=Tick, tpq=32, begin=0, end=796, tracks=1, notes=3, time_sig=1, key_sig=0, markers=0)\n",
      "left midi ticks per beat: 480\n",
      "max tick: 0\n",
      "tempo changes: 0\n",
      "time sig: 0\n",
      "key sig: 0\n",
      "markers: 0\n",
      "lyrics: False\n",
      "instruments: 1\n",
      "Saved MIDI to generated_ragtime.mid\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "right_hand_sample = sample_batch[0][0]  # First sample of the right-hand batch\n",
    "left_hand_sample = sample_batch[1][0] \n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "#vocab_size = len(valid_token_id_set)\n",
    "\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load(Path('best_model.pth'), weights_only=True))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "# Call the generation function\n",
    "generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens=right_hand_sample,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=\"generated_ragtime.mid\",\n",
    "    device=device,\n",
    "    valid_token_ids=valid_token_ids,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
