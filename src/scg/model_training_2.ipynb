{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Load miditok tokenizer\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI.from_pretrained(\"tokenizer.json\")\n",
    "\n",
    "# ----- Dataset -----\n",
    "class PairedMIDIDataset(Dataset):\n",
    "    def __init__(self, right_dir: Path, left_dir: Path, max_len=1024):\n",
    "        self.right_files = sorted(right_dir.glob(\"*.json\"))\n",
    "        self.left_files = sorted(left_dir.glob(\"*.json\"))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.right_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.right_files[idx]) as f:\n",
    "            right = json.load(f)[:self.max_len]\n",
    "        with open(self.left_files[idx]) as f:\n",
    "            left = json.load(f)[:self.max_len]\n",
    "\n",
    "        return torch.tensor(right), torch.tensor(left)\n",
    "\n",
    "# ----- Collate function -----\n",
    "def collate_fn(batch):\n",
    "    right_batch, left_batch = zip(*batch)\n",
    "    right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
    "    left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n",
    "\n",
    "    pad_token_id = tokenizer[\"PAD_None\"]  # Use string-based access here\n",
    "    right_padded = nn.utils.rnn.pad_sequence(right_batch, batch_first=True, padding_value=pad_token_id)\n",
    "    left_padded = nn.utils.rnn.pad_sequence(left_batch, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return right_padded, left_padded\n",
    "\n",
    "\n",
    "\n",
    "# ----- Dataloader -----\n",
    "right_json_dir = Path(\"tokenized_json/right_hand\")\n",
    "left_json_dir = Path(\"tokenized_json/left_hand\")\n",
    "\n",
    "dataset = PairedMIDIDataset(right_json_dir, left_json_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ----- Model: Simple Transformer -----\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        memory = self.encoder(src_emb.transpose(0, 1), src_mask)\n",
    "        out = self.decoder(tgt_emb.transpose(0, 1), memory, tgt_mask)\n",
    "        logits = self.fc_out(out.transpose(0, 1))\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID in dataset: 14997\n",
      "Vocab size from tokenizer: 15000\n",
      "Num valid token IDs: 9176\n"
     ]
    }
   ],
   "source": [
    "all_ids = []\n",
    "for right, left in dataloader.dataset:\n",
    "    all_ids.extend(right)\n",
    "    all_ids.extend(left)\n",
    "\n",
    "print(f\"Max token ID in dataset: {max(all_ids)}\")\n",
    "print(f\"Vocab size from tokenizer: {len(tokenizer)}\")\n",
    "\n",
    "valid_token_id_set = valid_ids = set(t.item() for t in all_ids)\n",
    "valid_token_ids = torch.tensor(list(valid_token_id_set), dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Num valid token IDs: {len(valid_token_id_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_24510/1637518399.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_24510/1637518399.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 8.4670\n",
      "Epoch 2 - Loss: 7.0742\n",
      "Epoch 3 - Loss: 6.4148\n",
      "Epoch 4 - Loss: 6.0486\n",
      "Epoch 5 - Loss: 5.7395\n",
      "Epoch 6 - Loss: 5.4808\n",
      "Epoch 7 - Loss: 5.2749\n",
      "Epoch 8 - Loss: 5.1350\n",
      "Epoch 9 - Loss: 4.9894\n",
      "Epoch 10 - Loss: 4.8614\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "pad_token = tokenizer.vocab.get(\"PAD_None\", -100)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])  # teacher forcing\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"music_transformer_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "def score_to_midi(score_tick):\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Correctly access instruments/tracks from ScoreTick\n",
    "    try:\n",
    "        for track in score_tick.tracks:  # ← this is the fix\n",
    "            midi_instr = Instrument(\n",
    "                program=track.program,\n",
    "                is_drum=track.is_drum,\n",
    "                name=track.name\n",
    "            )\n",
    "            for note in track.notes:\n",
    "                midi_instr.notes.append(Note(\n",
    "                    pitch=note.pitch,\n",
    "                    start=note.start,\n",
    "                    end=note.end,\n",
    "                    velocity=note.velocity\n",
    "                ))\n",
    "            midi.instruments.append(midi_instr)\n",
    "    except AttributeError as e:\n",
    "        raise ValueError(\"Provided object does not contain valid MIDI track info\") from e\n",
    "\n",
    "    return midi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    output_path,\n",
    "    max_len=512,\n",
    "    device=\"cpu\",\n",
    "    valid_token_ids=None,  # ← NEW: pass a list or set of allowed IDs\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure right_hand_tokens is a batched tensor\n",
    "    if isinstance(right_hand_tokens, list):\n",
    "        input_ids = torch.tensor([right_hand_tokens], dtype=torch.long, device=device)\n",
    "    elif isinstance(right_hand_tokens, torch.Tensor):\n",
    "        if right_hand_tokens.ndim == 1:\n",
    "            input_ids = right_hand_tokens.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            input_ids = right_hand_tokens.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"right_hand_tokens must be a list of ints or a torch.Tensor\")\n",
    "\n",
    "    bos_token_id = tokenizer.vocab.get(\"BOS_None\", tokenizer.vocab.get(\"BOS\", 0))\n",
    "    eos_token_id = tokenizer.vocab.get(\"EOS_None\", tokenizer.vocab.get(\"EOS\", -1))\n",
    "\n",
    "    decoder_input = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    mask_tensor = torch.full((vocab_size,), float('-inf'), device=device)\n",
    "\n",
    "    mask_tensor[valid_token_ids] = 0.0\n",
    "\n",
    "\n",
    "    # Autoregressive generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_ids, decoder_input)  # (batch, seq, vocab)\n",
    "            next_token_logits = output[:, -1, :]      # (batch, vocab)\n",
    "\n",
    "            # Apply mask\n",
    "            next_token_logits = next_token_logits + mask_tensor\n",
    "\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    left_hand_tokens = decoder_input.squeeze(0).tolist()\n",
    "    left_hand_tokens[0:2] = input_ids.squeeze(0).tolist()[0:2]\n",
    "\n",
    "    # Decode to Score objects\n",
    "    print('right tokens', input_ids.squeeze(0).tolist())\n",
    "    print('left tokens', left_hand_tokens)\n",
    "    \n",
    "\n",
    "    right_score = tokenizer.decode(input_ids.squeeze(0).tolist())\n",
    "    left_score = tokenizer.decode(left_hand_tokens)\n",
    "    \n",
    "    print('right score:', right_score)\n",
    "    print('left score:', left_score)\n",
    "\n",
    "    # Convert to MIDI\n",
    "    right_midi = score_to_midi(right_score)\n",
    "    left_midi = score_to_midi(left_score)\n",
    "\n",
    "    #print(right_midi.instruments[0])\n",
    "\n",
    "\n",
    "    print('left midi', left_midi)\n",
    "    # Create MIDI\n",
    "    # Create new MIDI and combine tracks\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Append notes from decoded MIDI objects\n",
    "    for track, program, name in zip([right_midi, left_midi], [0, 32], [\"Right Hand\", \"Left Hand\"]):\n",
    "        inst = Instrument(program=program, is_drum=False, name=name)\n",
    "        # Take notes from the first instrument in the decoded track\n",
    "        decoded_inst = track.instruments[0]\n",
    "        inst.notes.extend(decoded_inst.notes)\n",
    "        midi.instruments.append(inst)\n",
    "\n",
    "    midi.dump(str(output_path))\n",
    "\n",
    "    print(f\"✅ Saved MIDI to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_24510/1637518399.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_24510/1637518399.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n",
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "right_hand_sample = sample_batch[0][0]  # First sample of the right-hand batch\n",
    "left_hand_sample = sample_batch[1][0] \n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load(Path('music_transformer_weights.pth'), weights_only=True))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "# Call the generation function\n",
    "generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens=right_hand_sample,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=\"generated_ragtime.mid\",\n",
    "    device=device,\n",
    "    valid_token_ids=valid_token_ids,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
