{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU via MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU via CUDA:', torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('Using GPU via MPS (Apple Silicon)')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')\n",
    "\n",
    "# Use device like this:\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "from mido import MidiFile\n",
    "from symusic import Score\n",
    "\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "\n",
    "import miditoolkit\n",
    "from miditoolkit import MidiFile\n",
    "import json\n",
    "\n",
    "import pretty_midi\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Load miditok tokenizer\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI.from_pretrained(\"tokenizer.json\")\n",
    "\n",
    "# ----- Dataset -----\n",
    "class PairedMIDIDataset(Dataset):\n",
    "    def __init__(self, right_dir: Path, left_dir: Path, max_len=1024):\n",
    "        self.right_files = sorted(right_dir.glob(\"*.json\"))\n",
    "        self.left_files = sorted(left_dir.glob(\"*.json\"))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.right_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.right_files[idx]) as f:\n",
    "            right = json.load(f)[:self.max_len]\n",
    "        with open(self.left_files[idx]) as f:\n",
    "            left = json.load(f)[:self.max_len]\n",
    "\n",
    "        return torch.tensor(right), torch.tensor(left)\n",
    "\n",
    "# ----- Collate function -----\n",
    "def collate_fn(batch):\n",
    "    right_batch, left_batch = zip(*batch)\n",
    "    right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
    "    left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n",
    "\n",
    "    pad_token_id = tokenizer[\"PAD_None\"]  # Use string-based access here\n",
    "    right_padded = nn.utils.rnn.pad_sequence(right_batch, batch_first=True, padding_value=pad_token_id)\n",
    "    left_padded = nn.utils.rnn.pad_sequence(left_batch, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return right_padded, left_padded\n",
    "\n",
    "\n",
    "\n",
    "# ----- Dataloader -----\n",
    "right_json_dir = Path(\"tokenized_json/right_hand\")\n",
    "left_json_dir = Path(\"tokenized_json/left_hand\")\n",
    "\n",
    "dataset = PairedMIDIDataset(right_json_dir, left_json_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ----- Model: Simple Transformer -----\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        memory = self.encoder(src_emb.transpose(0, 1), src_mask)\n",
    "        out = self.decoder(tgt_emb.transpose(0, 1), memory, tgt_mask)\n",
    "        logits = self.fc_out(out.transpose(0, 1))\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_35757/1637518399.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
      "/var/folders/5v/y__8mmrj0s93px4wxmvty5z00000gn/T/ipykernel_35757/1637518399.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n"
     ]
    }
   ],
   "source": [
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "def score_to_midi(score_tick):\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Correctly access instruments/tracks from ScoreTick\n",
    "    try:\n",
    "        for track in score_tick.tracks:  # ← this is the fix\n",
    "            midi_instr = Instrument(\n",
    "                program=track.program,\n",
    "                is_drum=track.is_drum,\n",
    "                name=track.name\n",
    "            )\n",
    "            for note in track.notes:\n",
    "                midi_instr.notes.append(Note(\n",
    "                    pitch=note.pitch,\n",
    "                    start=note.start,\n",
    "                    end=note.end,\n",
    "                    velocity=note.velocity\n",
    "                ))\n",
    "            midi.instruments.append(midi_instr)\n",
    "    except AttributeError as e:\n",
    "        raise ValueError(\"Provided object does not contain valid MIDI track info\") from e\n",
    "\n",
    "    return midi\n",
    "from miditoolkit import MidiFile, Instrument\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    output_path,\n",
    "    max_len=1024,\n",
    "    device=device,\n",
    "    valid_token_ids=None,  # ← NEW: pass a list or set of allowed IDs\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Ensure right_hand_tokens is a batched tensor\n",
    "    if isinstance(right_hand_tokens, list):\n",
    "        input_ids = torch.tensor([right_hand_tokens], dtype=torch.long, device=device)\n",
    "    elif isinstance(right_hand_tokens, torch.Tensor):\n",
    "        if right_hand_tokens.ndim == 1:\n",
    "            input_ids = right_hand_tokens.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            input_ids = right_hand_tokens.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"right_hand_tokens must be a list of ints or a torch.Tensor\")\n",
    "\n",
    "    bos_token_id = tokenizer.vocab.get(\"BOS_None\", tokenizer.vocab.get(\"BOS\", 0))\n",
    "    eos_token_id = tokenizer.vocab.get(\"EOS_None\", tokenizer.vocab.get(\"EOS\", -1))\n",
    "\n",
    "    decoder_input = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    mask_tensor = torch.full((vocab_size,), float('-inf'), device=device)\n",
    "\n",
    "    mask_tensor[valid_token_ids] = 0.0\n",
    "\n",
    "\n",
    "    # Autoregressive generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_ids, decoder_input)  # (batch, seq, vocab)\n",
    "            next_token_logits = output[:, -1, :]      # (batch, vocab)\n",
    "\n",
    "            # Apply mask\n",
    "            next_token_logits = next_token_logits + mask_tensor\n",
    "\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    left_hand_tokens = decoder_input.squeeze(0).tolist()\n",
    "    left_hand_tokens[0:2] = input_ids.squeeze(0).tolist()[0:2]\n",
    "\n",
    "    # Decode to Score objects\n",
    "    print('right tokens', input_ids.squeeze(0).tolist())\n",
    "    print('left tokens', left_hand_tokens)\n",
    "    \n",
    "\n",
    "    right_score = tokenizer.decode(input_ids.squeeze(0).tolist())\n",
    "    left_score = tokenizer.decode(left_hand_tokens)\n",
    "    \n",
    "    print('right score:', right_score)\n",
    "    print('left score:', left_score)\n",
    "\n",
    "    # Convert to MIDI\n",
    "    right_midi = score_to_midi(right_score)\n",
    "    left_midi = score_to_midi(left_score)\n",
    "\n",
    "    #print(right_midi.instruments[0])\n",
    "\n",
    "\n",
    "    print('left midi', left_midi)\n",
    "    # Create MIDI\n",
    "    # Create new MIDI and combine tracks\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Append notes from decoded MIDI objects\n",
    "    for track, program, name in zip([right_midi, left_midi], [0, 0], [\"RH-1\", \"LH-1\"]):\n",
    "        inst = Instrument(program=program, is_drum=False, name=name)\n",
    "        # Take notes from the first instrument in the decoded track\n",
    "        decoded_inst = track.instruments[0]\n",
    "        inst.notes.extend(decoded_inst.notes)\n",
    "        midi.instruments.append(inst)\n",
    "\n",
    "    midi.dump(str(output_path))\n",
    "\n",
    "    print(f\"Saved MIDI to {output_path}\")\n",
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "right_hand_sample = sample_batch[0][0]  # First sample of the right-hand batch\n",
    "left_hand_sample = sample_batch[1][0] \n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "all_ids = []\n",
    "for right, left in dataloader.dataset:\n",
    "    all_ids.extend(right)\n",
    "    all_ids.extend(left)\n",
    "\n",
    "print(f\"Max token ID in dataset: {max(all_ids)}\")\n",
    "print(f\"Vocab size from tokenizer: {len(tokenizer)}\")\n",
    "\n",
    "valid_token_id_set = valid_ids = set(t.item() for t in all_ids)\n",
    "valid_token_ids = torch.tensor(list(valid_token_id_set), dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Num valid token IDs: {len(valid_token_id_set)}\")\n",
    "\n",
    "\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load(Path('best_model.pth'), weights_only=True))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "# Call the generation function\n",
    "generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens=right_hand_sample,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=\"generated_ragtime.mid\",\n",
    "    device=device,\n",
    "    valid_token_ids=valid_token_ids,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 480\n",
       "max tick: 4018\n",
       "tempo changes: 1\n",
       "time sig: 1\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using miditoolkit\n",
    "path = os.path.join('generated_ragtime.mid')\n",
    "midi_obj = miditoolkit.midi.parser.MidiFile(path)\n",
    "midi_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instrument(program=0, is_drum=False, name=RH-1) - 477 notes,\n",
       " Instrument(program=0, is_drum=False, name=LH-1) - 649 notes]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_obj.instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3173",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Map first few token IDs to strings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m token_strs \u001b[38;5;241m=\u001b[39m [id_to_token[token_id] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m token_ids[:\u001b[38;5;241m20\u001b[39m]]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_strs)\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Map first few token IDs to strings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m token_strs \u001b[38;5;241m=\u001b[39m [\u001b[43mid_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m token_ids[:\u001b[38;5;241m20\u001b[39m]]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_strs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3173"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = REMI.from_pretrained(\"tokenizer.json\")\n",
    "# Invert the vocab dictionary: id → token\n",
    "id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "# Load token IDs\n",
    "import json\n",
    "with open(\"tokenized_json/right_hand/acsrnade.json\") as f:\n",
    "    token_ids = json.load(f)\n",
    "\n",
    "# Map first few token IDs to strings\n",
    "token_strs = [id_to_token[token_id] for token_id in token_ids[:20]]\n",
    "\n",
    "print(token_strs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD_None',\n",
       " 1: 'BOS_None',\n",
       " 2: 'EOS_None',\n",
       " 3: 'MASK_None',\n",
       " 4: 'Bar_None',\n",
       " 5: 'Pitch_21',\n",
       " 6: 'Pitch_22',\n",
       " 7: 'Pitch_23',\n",
       " 8: 'Pitch_24',\n",
       " 9: 'Pitch_25',\n",
       " 10: 'Pitch_26',\n",
       " 11: 'Pitch_27',\n",
       " 12: 'Pitch_28',\n",
       " 13: 'Pitch_29',\n",
       " 14: 'Pitch_30',\n",
       " 15: 'Pitch_31',\n",
       " 16: 'Pitch_32',\n",
       " 17: 'Pitch_33',\n",
       " 18: 'Pitch_34',\n",
       " 19: 'Pitch_35',\n",
       " 20: 'Pitch_36',\n",
       " 21: 'Pitch_37',\n",
       " 22: 'Pitch_38',\n",
       " 23: 'Pitch_39',\n",
       " 24: 'Pitch_40',\n",
       " 25: 'Pitch_41',\n",
       " 26: 'Pitch_42',\n",
       " 27: 'Pitch_43',\n",
       " 28: 'Pitch_44',\n",
       " 29: 'Pitch_45',\n",
       " 30: 'Pitch_46',\n",
       " 31: 'Pitch_47',\n",
       " 32: 'Pitch_48',\n",
       " 33: 'Pitch_49',\n",
       " 34: 'Pitch_50',\n",
       " 35: 'Pitch_51',\n",
       " 36: 'Pitch_52',\n",
       " 37: 'Pitch_53',\n",
       " 38: 'Pitch_54',\n",
       " 39: 'Pitch_55',\n",
       " 40: 'Pitch_56',\n",
       " 41: 'Pitch_57',\n",
       " 42: 'Pitch_58',\n",
       " 43: 'Pitch_59',\n",
       " 44: 'Pitch_60',\n",
       " 45: 'Pitch_61',\n",
       " 46: 'Pitch_62',\n",
       " 47: 'Pitch_63',\n",
       " 48: 'Pitch_64',\n",
       " 49: 'Pitch_65',\n",
       " 50: 'Pitch_66',\n",
       " 51: 'Pitch_67',\n",
       " 52: 'Pitch_68',\n",
       " 53: 'Pitch_69',\n",
       " 54: 'Pitch_70',\n",
       " 55: 'Pitch_71',\n",
       " 56: 'Pitch_72',\n",
       " 57: 'Pitch_73',\n",
       " 58: 'Pitch_74',\n",
       " 59: 'Pitch_75',\n",
       " 60: 'Pitch_76',\n",
       " 61: 'Pitch_77',\n",
       " 62: 'Pitch_78',\n",
       " 63: 'Pitch_79',\n",
       " 64: 'Pitch_80',\n",
       " 65: 'Pitch_81',\n",
       " 66: 'Pitch_82',\n",
       " 67: 'Pitch_83',\n",
       " 68: 'Pitch_84',\n",
       " 69: 'Pitch_85',\n",
       " 70: 'Pitch_86',\n",
       " 71: 'Pitch_87',\n",
       " 72: 'Pitch_88',\n",
       " 73: 'Pitch_89',\n",
       " 74: 'Pitch_90',\n",
       " 75: 'Pitch_91',\n",
       " 76: 'Pitch_92',\n",
       " 77: 'Pitch_93',\n",
       " 78: 'Pitch_94',\n",
       " 79: 'Pitch_95',\n",
       " 80: 'Pitch_96',\n",
       " 81: 'Pitch_97',\n",
       " 82: 'Pitch_98',\n",
       " 83: 'Pitch_99',\n",
       " 84: 'Pitch_100',\n",
       " 85: 'Pitch_101',\n",
       " 86: 'Pitch_102',\n",
       " 87: 'Pitch_103',\n",
       " 88: 'Pitch_104',\n",
       " 89: 'Pitch_105',\n",
       " 90: 'Pitch_106',\n",
       " 91: 'Pitch_107',\n",
       " 92: 'Pitch_108',\n",
       " 93: 'Velocity_3',\n",
       " 94: 'Velocity_7',\n",
       " 95: 'Velocity_11',\n",
       " 96: 'Velocity_15',\n",
       " 97: 'Velocity_19',\n",
       " 98: 'Velocity_23',\n",
       " 99: 'Velocity_27',\n",
       " 100: 'Velocity_31',\n",
       " 101: 'Velocity_35',\n",
       " 102: 'Velocity_39',\n",
       " 103: 'Velocity_43',\n",
       " 104: 'Velocity_47',\n",
       " 105: 'Velocity_51',\n",
       " 106: 'Velocity_55',\n",
       " 107: 'Velocity_59',\n",
       " 108: 'Velocity_63',\n",
       " 109: 'Velocity_67',\n",
       " 110: 'Velocity_71',\n",
       " 111: 'Velocity_75',\n",
       " 112: 'Velocity_79',\n",
       " 113: 'Velocity_83',\n",
       " 114: 'Velocity_87',\n",
       " 115: 'Velocity_91',\n",
       " 116: 'Velocity_95',\n",
       " 117: 'Velocity_99',\n",
       " 118: 'Velocity_103',\n",
       " 119: 'Velocity_107',\n",
       " 120: 'Velocity_111',\n",
       " 121: 'Velocity_115',\n",
       " 122: 'Velocity_119',\n",
       " 123: 'Velocity_123',\n",
       " 124: 'Velocity_127',\n",
       " 125: 'Duration_0.1.8',\n",
       " 126: 'Duration_0.2.8',\n",
       " 127: 'Duration_0.3.8',\n",
       " 128: 'Duration_0.4.8',\n",
       " 129: 'Duration_0.5.8',\n",
       " 130: 'Duration_0.6.8',\n",
       " 131: 'Duration_0.7.8',\n",
       " 132: 'Duration_1.0.8',\n",
       " 133: 'Duration_1.1.8',\n",
       " 134: 'Duration_1.2.8',\n",
       " 135: 'Duration_1.3.8',\n",
       " 136: 'Duration_1.4.8',\n",
       " 137: 'Duration_1.5.8',\n",
       " 138: 'Duration_1.6.8',\n",
       " 139: 'Duration_1.7.8',\n",
       " 140: 'Duration_2.0.8',\n",
       " 141: 'Duration_2.1.8',\n",
       " 142: 'Duration_2.2.8',\n",
       " 143: 'Duration_2.3.8',\n",
       " 144: 'Duration_2.4.8',\n",
       " 145: 'Duration_2.5.8',\n",
       " 146: 'Duration_2.6.8',\n",
       " 147: 'Duration_2.7.8',\n",
       " 148: 'Duration_3.0.8',\n",
       " 149: 'Duration_3.1.8',\n",
       " 150: 'Duration_3.2.8',\n",
       " 151: 'Duration_3.3.8',\n",
       " 152: 'Duration_3.4.8',\n",
       " 153: 'Duration_3.5.8',\n",
       " 154: 'Duration_3.6.8',\n",
       " 155: 'Duration_3.7.8',\n",
       " 156: 'Duration_4.0.8',\n",
       " 157: 'Duration_4.1.8',\n",
       " 158: 'Duration_4.2.8',\n",
       " 159: 'Duration_4.3.8',\n",
       " 160: 'Duration_4.4.8',\n",
       " 161: 'Duration_4.5.8',\n",
       " 162: 'Duration_4.6.8',\n",
       " 163: 'Duration_4.7.8',\n",
       " 164: 'Duration_5.0.8',\n",
       " 165: 'Duration_5.1.8',\n",
       " 166: 'Duration_5.2.8',\n",
       " 167: 'Duration_5.3.8',\n",
       " 168: 'Duration_5.4.8',\n",
       " 169: 'Duration_5.5.8',\n",
       " 170: 'Duration_5.6.8',\n",
       " 171: 'Duration_5.7.8',\n",
       " 172: 'Duration_6.0.8',\n",
       " 173: 'Duration_6.1.8',\n",
       " 174: 'Duration_6.2.8',\n",
       " 175: 'Duration_6.3.8',\n",
       " 176: 'Duration_6.4.8',\n",
       " 177: 'Duration_6.5.8',\n",
       " 178: 'Duration_6.6.8',\n",
       " 179: 'Duration_6.7.8',\n",
       " 180: 'Duration_7.0.8',\n",
       " 181: 'Duration_7.1.8',\n",
       " 182: 'Duration_7.2.8',\n",
       " 183: 'Duration_7.3.8',\n",
       " 184: 'Duration_7.4.8',\n",
       " 185: 'Duration_7.5.8',\n",
       " 186: 'Duration_7.6.8',\n",
       " 187: 'Duration_7.7.8',\n",
       " 188: 'Duration_8.0.8',\n",
       " 189: 'Duration_8.1.8',\n",
       " 190: 'Duration_8.2.8',\n",
       " 191: 'Duration_8.3.8',\n",
       " 192: 'Duration_8.4.8',\n",
       " 193: 'Duration_8.5.8',\n",
       " 194: 'Duration_8.6.8',\n",
       " 195: 'Duration_8.7.8',\n",
       " 196: 'Duration_9.0.8',\n",
       " 197: 'Duration_9.1.8',\n",
       " 198: 'Duration_9.2.8',\n",
       " 199: 'Duration_9.3.8',\n",
       " 200: 'Duration_9.4.8',\n",
       " 201: 'Duration_9.5.8',\n",
       " 202: 'Duration_9.6.8',\n",
       " 203: 'Duration_9.7.8',\n",
       " 204: 'Duration_10.0.8',\n",
       " 205: 'Duration_10.1.8',\n",
       " 206: 'Duration_10.2.8',\n",
       " 207: 'Duration_10.3.8',\n",
       " 208: 'Duration_10.4.8',\n",
       " 209: 'Duration_10.5.8',\n",
       " 210: 'Duration_10.6.8',\n",
       " 211: 'Duration_10.7.8',\n",
       " 212: 'Duration_11.0.8',\n",
       " 213: 'Duration_11.1.8',\n",
       " 214: 'Duration_11.2.8',\n",
       " 215: 'Duration_11.3.8',\n",
       " 216: 'Duration_11.4.8',\n",
       " 217: 'Duration_11.5.8',\n",
       " 218: 'Duration_11.6.8',\n",
       " 219: 'Duration_11.7.8',\n",
       " 220: 'Duration_12.0.8',\n",
       " 221: 'Duration_12.1.8',\n",
       " 222: 'Duration_12.2.8',\n",
       " 223: 'Duration_12.3.8',\n",
       " 224: 'Duration_12.4.8',\n",
       " 225: 'Duration_12.5.8',\n",
       " 226: 'Duration_12.6.8',\n",
       " 227: 'Duration_12.7.8',\n",
       " 228: 'Duration_13.0.8',\n",
       " 229: 'Duration_13.1.8',\n",
       " 230: 'Duration_13.2.8',\n",
       " 231: 'Duration_13.3.8',\n",
       " 232: 'Duration_13.4.8',\n",
       " 233: 'Duration_13.5.8',\n",
       " 234: 'Duration_13.6.8',\n",
       " 235: 'Duration_13.7.8',\n",
       " 236: 'Duration_14.0.8',\n",
       " 237: 'Duration_14.1.8',\n",
       " 238: 'Duration_14.2.8',\n",
       " 239: 'Duration_14.3.8',\n",
       " 240: 'Duration_14.4.8',\n",
       " 241: 'Duration_14.5.8',\n",
       " 242: 'Duration_14.6.8',\n",
       " 243: 'Duration_14.7.8',\n",
       " 244: 'Duration_15.0.8',\n",
       " 245: 'Duration_15.1.8',\n",
       " 246: 'Duration_15.2.8',\n",
       " 247: 'Duration_15.3.8',\n",
       " 248: 'Duration_15.4.8',\n",
       " 249: 'Duration_15.5.8',\n",
       " 250: 'Duration_15.6.8',\n",
       " 251: 'Duration_15.7.8',\n",
       " 252: 'Duration_16.0.8',\n",
       " 253: 'Duration_16.1.8',\n",
       " 254: 'Duration_16.2.8',\n",
       " 255: 'Duration_16.3.8',\n",
       " 256: 'Duration_16.4.8',\n",
       " 257: 'Duration_16.5.8',\n",
       " 258: 'Duration_16.6.8',\n",
       " 259: 'Duration_16.7.8',\n",
       " 260: 'Duration_17.0.8',\n",
       " 261: 'Duration_17.1.8',\n",
       " 262: 'Duration_17.2.8',\n",
       " 263: 'Duration_17.3.8',\n",
       " 264: 'Duration_17.4.8',\n",
       " 265: 'Duration_17.5.8',\n",
       " 266: 'Duration_17.6.8',\n",
       " 267: 'Duration_17.7.8',\n",
       " 268: 'Duration_18.0.8',\n",
       " 269: 'Duration_18.1.8',\n",
       " 270: 'Duration_18.2.8',\n",
       " 271: 'Duration_18.3.8',\n",
       " 272: 'Duration_18.4.8',\n",
       " 273: 'Duration_18.5.8',\n",
       " 274: 'Duration_18.6.8',\n",
       " 275: 'Duration_18.7.8',\n",
       " 276: 'Duration_19.0.8',\n",
       " 277: 'Duration_19.1.8',\n",
       " 278: 'Duration_19.2.8',\n",
       " 279: 'Duration_19.3.8',\n",
       " 280: 'Duration_19.4.8',\n",
       " 281: 'Duration_19.5.8',\n",
       " 282: 'Duration_19.6.8',\n",
       " 283: 'Duration_19.7.8',\n",
       " 284: 'Duration_20.0.8',\n",
       " 285: 'Duration_20.1.8',\n",
       " 286: 'Duration_20.2.8',\n",
       " 287: 'Duration_20.3.8',\n",
       " 288: 'Duration_20.4.8',\n",
       " 289: 'Duration_20.5.8',\n",
       " 290: 'Duration_20.6.8',\n",
       " 291: 'Duration_20.7.8',\n",
       " 292: 'Duration_21.0.8',\n",
       " 293: 'Duration_21.1.8',\n",
       " 294: 'Duration_21.2.8',\n",
       " 295: 'Duration_21.3.8',\n",
       " 296: 'Duration_21.4.8',\n",
       " 297: 'Duration_21.5.8',\n",
       " 298: 'Duration_21.6.8',\n",
       " 299: 'Duration_21.7.8',\n",
       " 300: 'Duration_22.0.8',\n",
       " 301: 'Duration_22.1.8',\n",
       " 302: 'Duration_22.2.8',\n",
       " 303: 'Duration_22.3.8',\n",
       " 304: 'Duration_22.4.8',\n",
       " 305: 'Duration_22.5.8',\n",
       " 306: 'Duration_22.6.8',\n",
       " 307: 'Duration_22.7.8',\n",
       " 308: 'Duration_23.0.8',\n",
       " 309: 'Duration_23.1.8',\n",
       " 310: 'Duration_23.2.8',\n",
       " 311: 'Duration_23.3.8',\n",
       " 312: 'Duration_23.4.8',\n",
       " 313: 'Duration_23.5.8',\n",
       " 314: 'Duration_23.6.8',\n",
       " 315: 'Duration_23.7.8',\n",
       " 316: 'Duration_24.0.8',\n",
       " 317: 'Duration_24.1.8',\n",
       " 318: 'Duration_24.2.8',\n",
       " 319: 'Duration_24.3.8',\n",
       " 320: 'Duration_24.4.8',\n",
       " 321: 'Duration_24.5.8',\n",
       " 322: 'Duration_24.6.8',\n",
       " 323: 'Duration_24.7.8',\n",
       " 324: 'Duration_25.0.8',\n",
       " 325: 'Duration_25.1.8',\n",
       " 326: 'Duration_25.2.8',\n",
       " 327: 'Duration_25.3.8',\n",
       " 328: 'Duration_25.4.8',\n",
       " 329: 'Duration_25.5.8',\n",
       " 330: 'Duration_25.6.8',\n",
       " 331: 'Duration_25.7.8',\n",
       " 332: 'Duration_26.0.8',\n",
       " 333: 'Duration_26.1.8',\n",
       " 334: 'Duration_26.2.8',\n",
       " 335: 'Duration_26.3.8',\n",
       " 336: 'Duration_26.4.8',\n",
       " 337: 'Duration_26.5.8',\n",
       " 338: 'Duration_26.6.8',\n",
       " 339: 'Duration_26.7.8',\n",
       " 340: 'Duration_27.0.8',\n",
       " 341: 'Duration_27.1.8',\n",
       " 342: 'Duration_27.2.8',\n",
       " 343: 'Duration_27.3.8',\n",
       " 344: 'Duration_27.4.8',\n",
       " 345: 'Duration_27.5.8',\n",
       " 346: 'Duration_27.6.8',\n",
       " 347: 'Duration_27.7.8',\n",
       " 348: 'Duration_28.0.8',\n",
       " 349: 'Duration_28.1.8',\n",
       " 350: 'Duration_28.2.8',\n",
       " 351: 'Duration_28.3.8',\n",
       " 352: 'Duration_28.4.8',\n",
       " 353: 'Duration_28.5.8',\n",
       " 354: 'Duration_28.6.8',\n",
       " 355: 'Duration_28.7.8',\n",
       " 356: 'Duration_29.0.8',\n",
       " 357: 'Duration_29.1.8',\n",
       " 358: 'Duration_29.2.8',\n",
       " 359: 'Duration_29.3.8',\n",
       " 360: 'Duration_29.4.8',\n",
       " 361: 'Duration_29.5.8',\n",
       " 362: 'Duration_29.6.8',\n",
       " 363: 'Duration_29.7.8',\n",
       " 364: 'Duration_30.0.8',\n",
       " 365: 'Duration_30.1.8',\n",
       " 366: 'Duration_30.2.8',\n",
       " 367: 'Duration_30.3.8',\n",
       " 368: 'Duration_30.4.8',\n",
       " 369: 'Duration_30.5.8',\n",
       " 370: 'Duration_30.6.8',\n",
       " 371: 'Duration_30.7.8',\n",
       " 372: 'Duration_31.0.8',\n",
       " 373: 'Duration_31.1.8',\n",
       " 374: 'Duration_31.2.8',\n",
       " 375: 'Duration_31.3.8',\n",
       " 376: 'Duration_31.4.8',\n",
       " 377: 'Duration_31.5.8',\n",
       " 378: 'Duration_31.6.8',\n",
       " 379: 'Duration_31.7.8',\n",
       " 380: 'Duration_32.0.8',\n",
       " 381: 'Position_0',\n",
       " 382: 'Position_1',\n",
       " 383: 'Position_2',\n",
       " 384: 'Position_3',\n",
       " 385: 'Position_4',\n",
       " 386: 'Position_5',\n",
       " 387: 'Position_6',\n",
       " 388: 'Position_7',\n",
       " 389: 'Position_8',\n",
       " 390: 'Position_9',\n",
       " 391: 'Position_10',\n",
       " 392: 'Position_11',\n",
       " 393: 'Position_12',\n",
       " 394: 'Position_13',\n",
       " 395: 'Position_14',\n",
       " 396: 'Position_15',\n",
       " 397: 'Position_16',\n",
       " 398: 'Position_17',\n",
       " 399: 'Position_18',\n",
       " 400: 'Position_19',\n",
       " 401: 'Position_20',\n",
       " 402: 'Position_21',\n",
       " 403: 'Position_22',\n",
       " 404: 'Position_23',\n",
       " 405: 'Position_24',\n",
       " 406: 'Position_25',\n",
       " 407: 'Position_26',\n",
       " 408: 'Position_27',\n",
       " 409: 'Position_28',\n",
       " 410: 'Position_29',\n",
       " 411: 'Position_30',\n",
       " 412: 'Position_31',\n",
       " 413: 'Position_32',\n",
       " 414: 'Position_33',\n",
       " 415: 'Position_34',\n",
       " 416: 'Position_35',\n",
       " 417: 'Position_36',\n",
       " 418: 'Position_37',\n",
       " 419: 'Position_38',\n",
       " 420: 'Position_39',\n",
       " 421: 'Position_40',\n",
       " 422: 'Position_41',\n",
       " 423: 'Position_42',\n",
       " 424: 'Position_43',\n",
       " 425: 'Position_44',\n",
       " 426: 'Position_45',\n",
       " 427: 'Position_46',\n",
       " 428: 'Position_47',\n",
       " 429: 'Position_48',\n",
       " 430: 'Position_49',\n",
       " 431: 'Position_50',\n",
       " 432: 'Position_51',\n",
       " 433: 'Position_52',\n",
       " 434: 'Position_53',\n",
       " 435: 'Position_54',\n",
       " 436: 'Position_55',\n",
       " 437: 'Position_56',\n",
       " 438: 'Position_57',\n",
       " 439: 'Position_58',\n",
       " 440: 'Position_59',\n",
       " 441: 'Position_60',\n",
       " 442: 'Position_61',\n",
       " 443: 'Position_62',\n",
       " 444: 'Position_63',\n",
       " 445: 'Position_64',\n",
       " 446: 'Position_65',\n",
       " 447: 'Position_66',\n",
       " 448: 'Position_67',\n",
       " 449: 'Position_68',\n",
       " 450: 'Position_69',\n",
       " 451: 'Position_70',\n",
       " 452: 'Position_71',\n",
       " 453: 'Position_72',\n",
       " 454: 'Position_73',\n",
       " 455: 'Position_74',\n",
       " 456: 'Position_75',\n",
       " 457: 'Position_76',\n",
       " 458: 'Position_77',\n",
       " 459: 'Position_78',\n",
       " 460: 'Position_79',\n",
       " 461: 'Position_80',\n",
       " 462: 'Position_81',\n",
       " 463: 'Position_82',\n",
       " 464: 'Position_83',\n",
       " 465: 'Position_84',\n",
       " 466: 'Position_85',\n",
       " 467: 'Position_86',\n",
       " 468: 'Position_87',\n",
       " 469: 'Position_88',\n",
       " 470: 'Position_89',\n",
       " 471: 'Position_90',\n",
       " 472: 'Position_91',\n",
       " 473: 'Position_92',\n",
       " 474: 'Position_93',\n",
       " 475: 'Position_94',\n",
       " 476: 'Position_95',\n",
       " 477: 'PitchDrum_27',\n",
       " 478: 'PitchDrum_28',\n",
       " 479: 'PitchDrum_29',\n",
       " 480: 'PitchDrum_30',\n",
       " 481: 'PitchDrum_31',\n",
       " 482: 'PitchDrum_32',\n",
       " 483: 'PitchDrum_33',\n",
       " 484: 'PitchDrum_34',\n",
       " 485: 'PitchDrum_35',\n",
       " 486: 'PitchDrum_36',\n",
       " 487: 'PitchDrum_37',\n",
       " 488: 'PitchDrum_38',\n",
       " 489: 'PitchDrum_39',\n",
       " 490: 'PitchDrum_40',\n",
       " 491: 'PitchDrum_41',\n",
       " 492: 'PitchDrum_42',\n",
       " 493: 'PitchDrum_43',\n",
       " 494: 'PitchDrum_44',\n",
       " 495: 'PitchDrum_45',\n",
       " 496: 'PitchDrum_46',\n",
       " 497: 'PitchDrum_47',\n",
       " 498: 'PitchDrum_48',\n",
       " 499: 'PitchDrum_49',\n",
       " 500: 'PitchDrum_50',\n",
       " 501: 'PitchDrum_51',\n",
       " 502: 'PitchDrum_52',\n",
       " 503: 'PitchDrum_53',\n",
       " 504: 'PitchDrum_54',\n",
       " 505: 'PitchDrum_55',\n",
       " 506: 'PitchDrum_56',\n",
       " 507: 'PitchDrum_57',\n",
       " 508: 'PitchDrum_58',\n",
       " 509: 'PitchDrum_59',\n",
       " 510: 'PitchDrum_60',\n",
       " 511: 'PitchDrum_61',\n",
       " 512: 'PitchDrum_62',\n",
       " 513: 'PitchDrum_63',\n",
       " 514: 'PitchDrum_64',\n",
       " 515: 'PitchDrum_65',\n",
       " 516: 'PitchDrum_66',\n",
       " 517: 'PitchDrum_67',\n",
       " 518: 'PitchDrum_68',\n",
       " 519: 'PitchDrum_69',\n",
       " 520: 'PitchDrum_70',\n",
       " 521: 'PitchDrum_71',\n",
       " 522: 'PitchDrum_72',\n",
       " 523: 'PitchDrum_73',\n",
       " 524: 'PitchDrum_74',\n",
       " 525: 'PitchDrum_75',\n",
       " 526: 'PitchDrum_76',\n",
       " 527: 'PitchDrum_77',\n",
       " 528: 'PitchDrum_78',\n",
       " 529: 'PitchDrum_79',\n",
       " 530: 'PitchDrum_80',\n",
       " 531: 'PitchDrum_81',\n",
       " 532: 'PitchDrum_82',\n",
       " 533: 'PitchDrum_83',\n",
       " 534: 'PitchDrum_84',\n",
       " 535: 'PitchDrum_85',\n",
       " 536: 'PitchDrum_86',\n",
       " 537: 'PitchDrum_87',\n",
       " 538: 'Chord_min',\n",
       " 539: 'Chord_maj',\n",
       " 540: 'Chord_dim',\n",
       " 541: 'Chord_aug',\n",
       " 542: 'Chord_sus2',\n",
       " 543: 'Chord_sus4',\n",
       " 544: 'Chord_7dom',\n",
       " 545: 'Chord_7min',\n",
       " 546: 'Chord_7maj',\n",
       " 547: 'Chord_7halfdim',\n",
       " 548: 'Chord_7dim',\n",
       " 549: 'Chord_7aug',\n",
       " 550: 'Chord_9maj',\n",
       " 551: 'Chord_9min',\n",
       " 552: 'Tempo_40.0',\n",
       " 553: 'Tempo_46.77',\n",
       " 554: 'Tempo_53.55',\n",
       " 555: 'Tempo_60.32',\n",
       " 556: 'Tempo_67.1',\n",
       " 557: 'Tempo_73.87',\n",
       " 558: 'Tempo_80.65',\n",
       " 559: 'Tempo_87.42',\n",
       " 560: 'Tempo_94.19',\n",
       " 561: 'Tempo_100.97',\n",
       " 562: 'Tempo_107.74',\n",
       " 563: 'Tempo_114.52',\n",
       " 564: 'Tempo_121.29',\n",
       " 565: 'Tempo_128.06',\n",
       " 566: 'Tempo_134.84',\n",
       " 567: 'Tempo_141.61',\n",
       " 568: 'Tempo_148.39',\n",
       " 569: 'Tempo_155.16',\n",
       " 570: 'Tempo_161.94',\n",
       " 571: 'Tempo_168.71',\n",
       " 572: 'Tempo_175.48',\n",
       " 573: 'Tempo_182.26',\n",
       " 574: 'Tempo_189.03',\n",
       " 575: 'Tempo_195.81',\n",
       " 576: 'Tempo_202.58',\n",
       " 577: 'Tempo_209.35',\n",
       " 578: 'Tempo_216.13',\n",
       " 579: 'Tempo_222.9',\n",
       " 580: 'Tempo_229.68',\n",
       " 581: 'Tempo_236.45',\n",
       " 582: 'Tempo_243.23',\n",
       " 583: 'Tempo_250.0',\n",
       " 584: 'Program_0',\n",
       " 585: 'TimeSig_3/8',\n",
       " 586: 'TimeSig_12/8',\n",
       " 587: 'TimeSig_6/8',\n",
       " 588: 'TimeSig_5/4',\n",
       " 589: 'TimeSig_6/4',\n",
       " 590: 'TimeSig_3/4',\n",
       " 591: 'TimeSig_2/4',\n",
       " 592: 'TimeSig_1/4',\n",
       " 593: 'TimeSig_4/4'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trained tokenizer with BPE/multi-vocab setup\n",
    "id_to_token = {v: k for k, v in tokenizer._vocab_base.items()}  # try this first\n",
    "\n",
    "# Fallback if that doesn't work\n",
    "if not id_to_token:\n",
    "    print('doesnt work')\n",
    "    id_to_token = {v: k for k, v in tokenizer.vocab_bpe.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
