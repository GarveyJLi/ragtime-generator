{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from mido import MidiFile\n",
    "from symusic import Score\n",
    "\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "\n",
    "import miditoolkit\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "import pretty_midi\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_rh_lh(midi_path: Path, rh_out_dir: Path, lh_out_dir: Path):\n",
    "    pm = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "\n",
    "    # Check minimum tracks\n",
    "    if len(pm.instruments) != 2:\n",
    "        print(f\"Skipping {midi_path.name}, doesn't contain 2 tracks\")\n",
    "        return\n",
    "\n",
    "    # Extract right hand (track 0)\n",
    "    pm_rh = pretty_midi.PrettyMIDI()\n",
    "    pm_rh.instruments.append(pm.instruments[0])\n",
    "\n",
    "    # Extract left hand (track 1)\n",
    "    pm_lh = pretty_midi.PrettyMIDI()\n",
    "    pm_lh.instruments.append(pm.instruments[1])\n",
    "\n",
    "    # Save RH MIDI\n",
    "    rh_out_path = rh_out_dir / midi_path.name\n",
    "    pm_rh.write(str(rh_out_path))\n",
    "\n",
    "    # Save LH MIDI\n",
    "    lh_out_path = lh_out_dir / midi_path.name\n",
    "    pm_lh.write(str(lh_out_path))\n",
    "\n",
    "    print(f\"Saved RH: {rh_out_path.name} | LH: {lh_out_path.name}\")\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    input_dir: Path,\n",
    "    rh_out_dir: Path,\n",
    "    lh_out_dir: Path\n",
    "):\n",
    "    rh_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    lh_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    midi_files = list(input_dir.glob(\"**/*.mid\"))\n",
    "    print(f\"Found {len(midi_files)} MIDI files.\")\n",
    "\n",
    "    for midi_path in midi_files:\n",
    "        try:\n",
    "            extract_and_save_rh_lh(midi_path, rh_out_dir, lh_out_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_path.name}: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../../data\").resolve()\n",
    "right_hand_dir = Path(\"data_right_hand\").resolve()\n",
    "left_hand_dir = Path(\"data_left_hand\").resolve()\n",
    "\n",
    "preprocess_dataset(base_dir, right_hand_dir, left_hand_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from miditok import REMI\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "class MIDIPairedConditionalDataset(Dataset):\n",
    "    def __init__(self, rh_paths, lh_paths, tokenizer: REMI, max_seq_len=1024):\n",
    "        assert len(rh_paths) == len(lh_paths), \"Mismatch in dataset size\"\n",
    "        self.rh_paths = sorted(rh_paths)\n",
    "        self.lh_paths = sorted(lh_paths)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.bos = tokenizer[\"BOS_None\"]\n",
    "        self.sep = tokenizer[\"SEP_None\"]\n",
    "        self.eos = tokenizer[\"EOS_None\"]\n",
    "        self.pad = tokenizer[\"PAD_None\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rh_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rh_tokens = self.tokenizer.encode(self.rh_paths[idx])\n",
    "        lh_tokens = self.tokenizer.encode(self.lh_paths[idx])\n",
    "\n",
    "        # Compose input: [BOS] RH [SEP] LH [EOS]\n",
    "        input_ids = [self.bos] + rh_tokens + [self.sep] + lh_tokens + [self.eos]\n",
    "\n",
    "        # Mask RH + BOS + SEP with -100 in the labels\n",
    "        labels = [-100] * (len(rh_tokens) + 2) + lh_tokens + [self.eos]\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_seq_len:\n",
    "            input_ids = input_ids[:self.max_seq_len]\n",
    "            labels = labels[:self.max_seq_len]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained tokenizer exists. Loading...\n",
      "Tokenizing right-hand MIDIs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 425/425 [00:07<00:00, 58.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing left-hand MIDIs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 425/425 [00:06<00:00, 64.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To initialize/load\n",
    "tokenizer = None\n",
    "tokenizer_path = Path(\"tokenizer.json\")\n",
    "\n",
    "# Load or train tokenizer\n",
    "if tokenizer_path.exists():\n",
    "    print('Pre-trained tokenizer exists. Loading...')\n",
    "    tokenizer = REMI.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    print('No tokenizer found. Training...')\n",
    "    #config = TokenizerConfig(num_velocities=32, use_chords=True, use_programs=True)\n",
    "\n",
    "    config = TokenizerConfig(\n",
    "        pitch_range=(21, 109),\n",
    "        beat_res={(0, 4): 4, (4, 12): 8, (12, 32): 16}, \n",
    "        num_velocities=64,\n",
    "        special_tokens=[\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "        encode_ids_split=\"no\",\n",
    "        use_velocities=True,\n",
    "        use_note_duration_programs=[0],  \n",
    "        use_chords=True,\n",
    "        use_rests=True,  # True if you want to model silences\n",
    "        use_tempos=True,\n",
    "        use_time_signatures=True,\n",
    "        use_sustain_pedals=False,\n",
    "        use_pitch_bends=False,\n",
    "        use_programs=True,  \n",
    "        programs=[0],\n",
    "        use_pitchdrum_tokens=True,\n",
    "        remove_duplicated_notes=False,\n",
    "        one_token_stream_for_programs=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = REMI(config)\n",
    "    midi_paths = list(Path(\"../../data\").rglob(\"*.mid\"))\n",
    "    #tokenizer.train(vocab_size=30000, files_paths=midi_paths)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "\n",
    "# Input and output paths\n",
    "out_dir = Path(\"tokenized_json\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "right_midi_dir = Path(\"data_right_hand\")\n",
    "left_midi_dir = Path(\"data_left_hand\")\n",
    "\n",
    "(right_json_dir := out_dir / \"right_hand\").mkdir(parents=True, exist_ok=True)\n",
    "(left_json_dir := out_dir / \"left_hand\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def tokenize_and_save(midi_path: Path, out_path: Path):\n",
    "    tokens = tokenizer.encode(midi_path)\n",
    "    with open(out_path.with_suffix(\".json\"), \"w\") as f:\n",
    "        json.dump(tokens.ids, f)\n",
    "\n",
    "\n",
    "# Tokenize right-hand tracks\n",
    "print(\"Tokenizing right-hand MIDIs...\")\n",
    "for midi_file in tqdm(sorted(right_midi_dir.glob(\"*.mid\"))):\n",
    "    out_path = right_json_dir / midi_file.stem\n",
    "    tokenize_and_save(midi_file, out_path)\n",
    "\n",
    "# Tokenize left-hand tracks\n",
    "print(\"Tokenizing left-hand MIDIs...\")\n",
    "for midi_file in tqdm(sorted(left_midi_dir.glob(\"*.mid\"))):\n",
    "    out_path = left_json_dir / midi_file.stem\n",
    "    tokenize_and_save(midi_file, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 MIDI files.\n",
      "Saved RH: symbolic_conditional_2.mid | LH: symbolic_conditional_2.mid\n",
      "Saved RH: symbolic_conditional_3.mid | LH: symbolic_conditional_3.mid\n",
      "Saved RH: symbolic_conditioned.mid | LH: symbolic_conditioned.mid\n",
      "Saved RH: symbolic_conditioned_1.mid | LH: symbolic_conditioned_1.mid\n",
      "Saved RH: symbolic_conditioned_10.mid | LH: symbolic_conditioned_10.mid\n",
      "Saved RH: symbolic_conditioned_11.mid | LH: symbolic_conditioned_11.mid\n",
      "Saved RH: symbolic_conditioned_12.mid | LH: symbolic_conditioned_12.mid\n",
      "Saved RH: symbolic_conditioned_13.mid | LH: symbolic_conditioned_13.mid\n",
      "Saved RH: symbolic_conditioned_14.mid | LH: symbolic_conditioned_14.mid\n",
      "Saved RH: symbolic_conditioned_15.mid | LH: symbolic_conditioned_15.mid\n",
      "Saved RH: symbolic_conditioned_16.mid | LH: symbolic_conditioned_16.mid\n",
      "Saved RH: symbolic_conditioned_17.mid | LH: symbolic_conditioned_17.mid\n",
      "Saved RH: symbolic_conditioned_18.mid | LH: symbolic_conditioned_18.mid\n",
      "Saved RH: symbolic_conditioned_19.mid | LH: symbolic_conditioned_19.mid\n",
      "Saved RH: symbolic_conditioned_2.mid | LH: symbolic_conditioned_2.mid\n",
      "Saved RH: symbolic_conditioned_20.mid | LH: symbolic_conditioned_20.mid\n",
      "Saved RH: symbolic_conditioned_21.mid | LH: symbolic_conditioned_21.mid\n",
      "Saved RH: symbolic_conditioned_22.mid | LH: symbolic_conditioned_22.mid\n",
      "Saved RH: symbolic_conditioned_23.mid | LH: symbolic_conditioned_23.mid\n",
      "Saved RH: symbolic_conditioned_24.mid | LH: symbolic_conditioned_24.mid\n",
      "Saved RH: symbolic_conditioned_25.mid | LH: symbolic_conditioned_25.mid\n",
      "Saved RH: symbolic_conditioned_26.mid | LH: symbolic_conditioned_26.mid\n",
      "Saved RH: symbolic_conditioned_27.mid | LH: symbolic_conditioned_27.mid\n",
      "Saved RH: symbolic_conditioned_28.mid | LH: symbolic_conditioned_28.mid\n",
      "Saved RH: symbolic_conditioned_29.mid | LH: symbolic_conditioned_29.mid\n",
      "Saved RH: symbolic_conditioned_3.mid | LH: symbolic_conditioned_3.mid\n",
      "Saved RH: symbolic_conditioned_30.mid | LH: symbolic_conditioned_30.mid\n",
      "Saved RH: symbolic_conditioned_31.mid | LH: symbolic_conditioned_31.mid\n",
      "Saved RH: symbolic_conditioned_32.mid | LH: symbolic_conditioned_32.mid\n",
      "Saved RH: symbolic_conditioned_33.mid | LH: symbolic_conditioned_33.mid\n",
      "Saved RH: symbolic_conditioned_34.mid | LH: symbolic_conditioned_34.mid\n",
      "Saved RH: symbolic_conditioned_35.mid | LH: symbolic_conditioned_35.mid\n",
      "Saved RH: symbolic_conditioned_36.mid | LH: symbolic_conditioned_36.mid\n",
      "Saved RH: symbolic_conditioned_37.mid | LH: symbolic_conditioned_37.mid\n",
      "Saved RH: symbolic_conditioned_38.mid | LH: symbolic_conditioned_38.mid\n",
      "Saved RH: symbolic_conditioned_39.mid | LH: symbolic_conditioned_39.mid\n",
      "Saved RH: symbolic_conditioned_4.mid | LH: symbolic_conditioned_4.mid\n",
      "Saved RH: symbolic_conditioned_40.mid | LH: symbolic_conditioned_40.mid\n",
      "Saved RH: symbolic_conditioned_41.mid | LH: symbolic_conditioned_41.mid\n",
      "Saved RH: symbolic_conditioned_42.mid | LH: symbolic_conditioned_42.mid\n",
      "Saved RH: symbolic_conditioned_43.mid | LH: symbolic_conditioned_43.mid\n",
      "Saved RH: symbolic_conditioned_44.mid | LH: symbolic_conditioned_44.mid\n",
      "Saved RH: symbolic_conditioned_45.mid | LH: symbolic_conditioned_45.mid\n",
      "Saved RH: symbolic_conditioned_46.mid | LH: symbolic_conditioned_46.mid\n",
      "Saved RH: symbolic_conditioned_47.mid | LH: symbolic_conditioned_47.mid\n",
      "Saved RH: symbolic_conditioned_48.mid | LH: symbolic_conditioned_48.mid\n",
      "Saved RH: symbolic_conditioned_49.mid | LH: symbolic_conditioned_49.mid\n",
      "Saved RH: symbolic_conditioned_5.mid | LH: symbolic_conditioned_5.mid\n",
      "Saved RH: symbolic_conditioned_50.mid | LH: symbolic_conditioned_50.mid\n",
      "Saved RH: symbolic_conditioned_6.mid | LH: symbolic_conditioned_6.mid\n",
      "Saved RH: symbolic_conditioned_7.mid | LH: symbolic_conditioned_7.mid\n",
      "Saved RH: symbolic_conditioned_8.mid | LH: symbolic_conditioned_8.mid\n",
      "Saved RH: symbolic_conditioned_9.mid | LH: symbolic_conditioned_9.mid\n"
     ]
    }
   ],
   "source": [
    "generated_base_dir = Path(\"symbolic_conditional\").resolve()\n",
    "generated_right_hand_dir = Path(\"generated_data_right_hand\").resolve()\n",
    "generated_left_hand_dir = Path(\"generated_data_left_hand\").resolve()\n",
    "\n",
    "preprocess_dataset(generated_base_dir, generated_right_hand_dir, generated_left_hand_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
