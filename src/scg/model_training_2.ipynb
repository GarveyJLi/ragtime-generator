{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU via MPS (Apple Silicon)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU via CUDA:', torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('Using GPU via MPS (Apple Silicon)')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')\n",
    "\n",
    "# Use device like this:\n",
    "# model.to(device)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Load miditok tokenizer\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI.from_pretrained(\"tokenizer.json\")\n",
    "\n",
    "# ----- Dataset -----\n",
    "class PairedMIDIDataset(Dataset):\n",
    "    def __init__(self, right_dir: Path, left_dir: Path, max_len=1024):\n",
    "        self.right_files = sorted(right_dir.glob(\"*.json\"))\n",
    "        self.left_files = sorted(left_dir.glob(\"*.json\"))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.right_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.right_files[idx]) as f:\n",
    "            right = json.load(f)[:self.max_len]\n",
    "        with open(self.left_files[idx]) as f:\n",
    "            left = json.load(f)[:self.max_len]\n",
    "\n",
    "        return torch.tensor(right), torch.tensor(left)\n",
    "\n",
    "# ----- Collate function -----\n",
    "def collate_fn(batch):\n",
    "    right_batch, left_batch = zip(*batch)\n",
    "    #right_batch = [torch.tensor(seq, dtype=torch.long) for seq in right_batch]\n",
    "    #left_batch = [torch.tensor(seq, dtype=torch.long) for seq in left_batch]\n",
    "    right_batch = [\n",
    "        seq.clone().detach() if isinstance(seq, torch.Tensor) else torch.tensor(seq, dtype=torch.long)\n",
    "        for seq in right_batch\n",
    "    ]\n",
    "    left_batch = [\n",
    "        seq.clone().detach() if isinstance(seq, torch.Tensor) else torch.tensor(seq, dtype=torch.long)\n",
    "        for seq in left_batch\n",
    "    ]\n",
    "\n",
    "    pad_token_id = tokenizer[\"PAD_None\"]  # Use string-based access here\n",
    "    right_padded = nn.utils.rnn.pad_sequence(right_batch, batch_first=True, padding_value=pad_token_id)\n",
    "    left_padded = nn.utils.rnn.pad_sequence(left_batch, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return right_padded, left_padded\n",
    "\n",
    "\n",
    "\n",
    "# ----- Dataloader -----\n",
    "right_json_dir = Path(\"tokenized_json/right_hand\")\n",
    "left_json_dir = Path(\"tokenized_json/left_hand\")\n",
    "\n",
    "dataset = PairedMIDIDataset(right_json_dir, left_json_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ----- Model: Simple Transformer -----\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        memory = self.encoder(src_emb.transpose(0, 1), src_mask)\n",
    "        out = self.decoder(tgt_emb.transpose(0, 1), memory, tgt_mask)\n",
    "        logits = self.fc_out(out.transpose(0, 1))\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID in dataset: 897\n",
      "Vocab size from tokenizer: 898\n",
      "Num valid token IDs: 249\n"
     ]
    }
   ],
   "source": [
    "all_ids = []\n",
    "for right, left in dataloader.dataset:\n",
    "    all_ids.extend(right)\n",
    "    all_ids.extend(left)\n",
    "\n",
    "print(f\"Max token ID in dataset: {max(all_ids)}\")\n",
    "print(f\"Vocab size from tokenizer: {len(tokenizer)}\")\n",
    "\n",
    "valid_token_id_set = set(t.item() for t in all_ids)\n",
    "valid_token_ids = torch.tensor(list(valid_token_id_set), dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Num valid token IDs: {len(valid_token_id_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 4.1180\n",
      "Loss improved from inf to 4.1180. Saving model...\n",
      "Epoch 2 - Loss: 2.5016\n",
      "Loss improved from 4.1180 to 2.5016. Saving model...\n",
      "Epoch 3 - Loss: 2.0830\n",
      "Loss improved from 2.5016 to 2.0830. Saving model...\n",
      "Epoch 4 - Loss: 1.9177\n",
      "Loss improved from 2.0830 to 1.9177. Saving model...\n",
      "Epoch 5 - Loss: 1.8210\n",
      "Loss improved from 1.9177 to 1.8210. Saving model...\n",
      "Epoch 6 - Loss: 1.7517\n",
      "Loss improved from 1.8210 to 1.7517. Saving model...\n",
      "Epoch 7 - Loss: 1.6809\n",
      "Loss improved from 1.7517 to 1.6809. Saving model...\n",
      "Epoch 8 - Loss: 1.6677\n",
      "Loss improved from 1.6809 to 1.6677. Saving model...\n",
      "Epoch 9 - Loss: 1.6440\n",
      "Loss improved from 1.6677 to 1.6440. Saving model...\n",
      "Epoch 10 - Loss: 1.6152\n",
      "Loss improved from 1.6440 to 1.6152. Saving model...\n",
      "Epoch 11 - Loss: 1.5966\n",
      "Loss improved from 1.6152 to 1.5966. Saving model...\n",
      "Epoch 12 - Loss: 1.5770\n",
      "Loss improved from 1.5966 to 1.5770. Saving model...\n",
      "Epoch 13 - Loss: 1.5686\n",
      "Loss improved from 1.5770 to 1.5686. Saving model...\n",
      "Epoch 14 - Loss: 1.5273\n",
      "Loss improved from 1.5686 to 1.5273. Saving model...\n",
      "Epoch 15 - Loss: 1.5477\n",
      "Epoch 16 - Loss: 1.5469\n",
      "Epoch 17 - Loss: 1.5150\n",
      "Loss improved from 1.5273 to 1.5150. Saving model...\n",
      "Epoch 18 - Loss: 1.5078\n",
      "Loss improved from 1.5150 to 1.5078. Saving model...\n",
      "Epoch 19 - Loss: 1.4999\n",
      "Loss improved from 1.5078 to 1.4999. Saving model...\n",
      "Epoch 20 - Loss: 1.4955\n",
      "Loss improved from 1.4999 to 1.4955. Saving model...\n",
      "Epoch 21 - Loss: 1.4859\n",
      "Loss improved from 1.4955 to 1.4859. Saving model...\n",
      "Epoch 22 - Loss: 1.4722\n",
      "Loss improved from 1.4859 to 1.4722. Saving model...\n",
      "Epoch 23 - Loss: 1.4589\n",
      "Loss improved from 1.4722 to 1.4589. Saving model...\n",
      "Epoch 24 - Loss: 1.4587\n",
      "Loss improved from 1.4589 to 1.4587. Saving model...\n",
      "Epoch 25 - Loss: 1.4529\n",
      "Loss improved from 1.4587 to 1.4529. Saving model...\n",
      "Epoch 26 - Loss: 1.4419\n",
      "Loss improved from 1.4529 to 1.4419. Saving model...\n",
      "Epoch 27 - Loss: 1.4268\n",
      "Loss improved from 1.4419 to 1.4268. Saving model...\n",
      "Epoch 28 - Loss: 1.4294\n",
      "Epoch 29 - Loss: 1.4185\n",
      "Loss improved from 1.4268 to 1.4185. Saving model...\n",
      "Epoch 30 - Loss: 1.3984\n",
      "Loss improved from 1.4185 to 1.3984. Saving model...\n",
      "Epoch 31 - Loss: 1.3913\n",
      "Loss improved from 1.3984 to 1.3913. Saving model...\n",
      "Epoch 32 - Loss: 1.3892\n",
      "Loss improved from 1.3913 to 1.3892. Saving model...\n",
      "Epoch 33 - Loss: 1.3762\n",
      "Loss improved from 1.3892 to 1.3762. Saving model...\n",
      "Epoch 34 - Loss: 1.3820\n",
      "Epoch 35 - Loss: 1.3712\n",
      "Loss improved from 1.3762 to 1.3712. Saving model...\n",
      "Epoch 36 - Loss: 1.3417\n",
      "Loss improved from 1.3712 to 1.3417. Saving model...\n",
      "Epoch 37 - Loss: 1.3370\n",
      "Loss improved from 1.3417 to 1.3370. Saving model...\n",
      "Epoch 38 - Loss: 1.3167\n",
      "Loss improved from 1.3370 to 1.3167. Saving model...\n",
      "Epoch 39 - Loss: 1.3047\n",
      "Loss improved from 1.3167 to 1.3047. Saving model...\n",
      "Epoch 40 - Loss: 1.2929\n",
      "Loss improved from 1.3047 to 1.2929. Saving model...\n",
      "Epoch 41 - Loss: 1.2733\n",
      "Loss improved from 1.2929 to 1.2733. Saving model...\n",
      "Epoch 42 - Loss: 1.2593\n",
      "Loss improved from 1.2733 to 1.2593. Saving model...\n",
      "Epoch 43 - Loss: 1.2344\n",
      "Loss improved from 1.2593 to 1.2344. Saving model...\n",
      "Epoch 44 - Loss: 1.2027\n",
      "Loss improved from 1.2344 to 1.2027. Saving model...\n",
      "Epoch 45 - Loss: 1.1912\n",
      "Loss improved from 1.2027 to 1.1912. Saving model...\n",
      "Epoch 46 - Loss: 1.1435\n",
      "Loss improved from 1.1912 to 1.1435. Saving model...\n",
      "Epoch 47 - Loss: 1.1379\n",
      "Loss improved from 1.1435 to 1.1379. Saving model...\n",
      "Epoch 48 - Loss: 1.1211\n",
      "Loss improved from 1.1379 to 1.1211. Saving model...\n",
      "Epoch 49 - Loss: 1.1085\n",
      "Loss improved from 1.1211 to 1.1085. Saving model...\n",
      "Epoch 50 - Loss: 1.0807\n",
      "Loss improved from 1.1085 to 1.0807. Saving model...\n",
      "Epoch 51 - Loss: 1.0526\n",
      "Loss improved from 1.0807 to 1.0526. Saving model...\n",
      "Epoch 52 - Loss: 1.0419\n",
      "Loss improved from 1.0526 to 1.0419. Saving model...\n",
      "Epoch 53 - Loss: 1.0321\n",
      "Loss improved from 1.0419 to 1.0321. Saving model...\n",
      "Epoch 54 - Loss: 1.0098\n",
      "Loss improved from 1.0321 to 1.0098. Saving model...\n",
      "Epoch 55 - Loss: 0.9985\n",
      "Loss improved from 1.0098 to 0.9985. Saving model...\n",
      "Epoch 56 - Loss: 0.9834\n",
      "Loss improved from 0.9985 to 0.9834. Saving model...\n",
      "Epoch 57 - Loss: 0.9669\n",
      "Loss improved from 0.9834 to 0.9669. Saving model...\n",
      "Epoch 58 - Loss: 0.9494\n",
      "Loss improved from 0.9669 to 0.9494. Saving model...\n",
      "Epoch 59 - Loss: 0.9423\n",
      "Loss improved from 0.9494 to 0.9423. Saving model...\n",
      "Epoch 60 - Loss: 0.9331\n",
      "Loss improved from 0.9423 to 0.9331. Saving model...\n",
      "Epoch 61 - Loss: 0.9126\n",
      "Loss improved from 0.9331 to 0.9126. Saving model...\n",
      "Epoch 62 - Loss: 0.9049\n",
      "Loss improved from 0.9126 to 0.9049. Saving model...\n",
      "Epoch 63 - Loss: 0.9010\n",
      "Loss improved from 0.9049 to 0.9010. Saving model...\n",
      "Epoch 64 - Loss: 0.8856\n",
      "Loss improved from 0.9010 to 0.8856. Saving model...\n",
      "Epoch 65 - Loss: 0.8748\n",
      "Loss improved from 0.8856 to 0.8748. Saving model...\n",
      "Epoch 66 - Loss: 0.8682\n",
      "Loss improved from 0.8748 to 0.8682. Saving model...\n",
      "Epoch 67 - Loss: 0.8549\n",
      "Loss improved from 0.8682 to 0.8549. Saving model...\n",
      "Epoch 68 - Loss: 0.8473\n",
      "Loss improved from 0.8549 to 0.8473. Saving model...\n",
      "Epoch 69 - Loss: 0.8317\n",
      "Loss improved from 0.8473 to 0.8317. Saving model...\n",
      "Epoch 70 - Loss: 0.8264\n",
      "Loss improved from 0.8317 to 0.8264. Saving model...\n",
      "Epoch 71 - Loss: 0.8183\n",
      "Loss improved from 0.8264 to 0.8183. Saving model...\n",
      "Epoch 72 - Loss: 0.8047\n",
      "Loss improved from 0.8183 to 0.8047. Saving model...\n",
      "Epoch 73 - Loss: 0.7987\n",
      "Loss improved from 0.8047 to 0.7987. Saving model...\n",
      "Epoch 74 - Loss: 0.7855\n",
      "Loss improved from 0.7987 to 0.7855. Saving model...\n",
      "Epoch 75 - Loss: 0.7779\n",
      "Loss improved from 0.7855 to 0.7779. Saving model...\n",
      "Epoch 76 - Loss: 0.7785\n",
      "Epoch 77 - Loss: 0.7664\n",
      "Loss improved from 0.7779 to 0.7664. Saving model...\n",
      "Epoch 78 - Loss: 0.7546\n",
      "Loss improved from 0.7664 to 0.7546. Saving model...\n",
      "Epoch 79 - Loss: 0.7499\n",
      "Loss improved from 0.7546 to 0.7499. Saving model...\n",
      "Epoch 80 - Loss: 0.7388\n",
      "Loss improved from 0.7499 to 0.7388. Saving model...\n",
      "Epoch 81 - Loss: 0.7379\n",
      "Loss improved from 0.7388 to 0.7379. Saving model...\n",
      "Epoch 82 - Loss: 0.7309\n",
      "Loss improved from 0.7379 to 0.7309. Saving model...\n",
      "Epoch 83 - Loss: 0.7199\n",
      "Loss improved from 0.7309 to 0.7199. Saving model...\n",
      "Epoch 84 - Loss: 0.7170\n",
      "Loss improved from 0.7199 to 0.7170. Saving model...\n",
      "Epoch 85 - Loss: 0.6905\n",
      "Loss improved from 0.7170 to 0.6905. Saving model...\n",
      "Epoch 86 - Loss: 0.6905\n",
      "Loss improved from 0.6905 to 0.6905. Saving model...\n",
      "Epoch 87 - Loss: 0.7060\n",
      "Epoch 88 - Loss: 0.6877\n",
      "Loss improved from 0.6905 to 0.6877. Saving model...\n",
      "Epoch 89 - Loss: 0.6758\n",
      "Loss improved from 0.6877 to 0.6758. Saving model...\n",
      "Epoch 90 - Loss: 0.6720\n",
      "Loss improved from 0.6758 to 0.6720. Saving model...\n",
      "Epoch 91 - Loss: 0.6597\n",
      "Loss improved from 0.6720 to 0.6597. Saving model...\n",
      "Epoch 92 - Loss: 0.6602\n",
      "Epoch 93 - Loss: 0.6482\n",
      "Loss improved from 0.6597 to 0.6482. Saving model...\n",
      "Epoch 94 - Loss: 0.6458\n",
      "Loss improved from 0.6482 to 0.6458. Saving model...\n",
      "Epoch 95 - Loss: 0.6415\n",
      "Loss improved from 0.6458 to 0.6415. Saving model...\n",
      "Epoch 96 - Loss: 0.6425\n",
      "Epoch 97 - Loss: 0.6276\n",
      "Loss improved from 0.6415 to 0.6276. Saving model...\n",
      "Epoch 98 - Loss: 0.6235\n",
      "Loss improved from 0.6276 to 0.6235. Saving model...\n",
      "Epoch 99 - Loss: 0.6229\n",
      "Loss improved from 0.6235 to 0.6229. Saving model...\n",
      "Epoch 100 - Loss: 0.6094\n",
      "Loss improved from 0.6229 to 0.6094. Saving model...\n",
      "Epoch 101 - Loss: 0.6121\n",
      "Epoch 102 - Loss: 0.5988\n",
      "Loss improved from 0.6094 to 0.5988. Saving model...\n",
      "Epoch 103 - Loss: 0.6028\n",
      "Epoch 104 - Loss: 0.5914\n",
      "Loss improved from 0.5988 to 0.5914. Saving model...\n",
      "Epoch 105 - Loss: 0.5915\n",
      "Epoch 106 - Loss: 0.5811\n",
      "Loss improved from 0.5914 to 0.5811. Saving model...\n",
      "Epoch 107 - Loss: 0.5746\n",
      "Loss improved from 0.5811 to 0.5746. Saving model...\n",
      "Epoch 108 - Loss: 0.5706\n",
      "Loss improved from 0.5746 to 0.5706. Saving model...\n",
      "Epoch 109 - Loss: 0.5660\n",
      "Loss improved from 0.5706 to 0.5660. Saving model...\n",
      "Epoch 110 - Loss: 0.5652\n",
      "Loss improved from 0.5660 to 0.5652. Saving model...\n",
      "Epoch 111 - Loss: 0.5545\n",
      "Loss improved from 0.5652 to 0.5545. Saving model...\n",
      "Epoch 112 - Loss: 0.5521\n",
      "Loss improved from 0.5545 to 0.5521. Saving model...\n",
      "Epoch 113 - Loss: 0.5494\n",
      "Loss improved from 0.5521 to 0.5494. Saving model...\n",
      "Epoch 114 - Loss: 0.5436\n",
      "Loss improved from 0.5494 to 0.5436. Saving model...\n",
      "Epoch 115 - Loss: 0.5434\n",
      "Loss improved from 0.5436 to 0.5434. Saving model...\n",
      "Epoch 116 - Loss: 0.5360\n",
      "Loss improved from 0.5434 to 0.5360. Saving model...\n",
      "Epoch 117 - Loss: 0.5345\n",
      "Loss improved from 0.5360 to 0.5345. Saving model...\n",
      "Epoch 118 - Loss: 0.5304\n",
      "Loss improved from 0.5345 to 0.5304. Saving model...\n",
      "Epoch 119 - Loss: 0.5353\n",
      "Epoch 120 - Loss: 0.5279\n",
      "Loss improved from 0.5304 to 0.5279. Saving model...\n",
      "Epoch 121 - Loss: 0.5372\n",
      "Epoch 122 - Loss: 0.5393\n",
      "Epoch 123 - Loss: 0.5147\n",
      "Loss improved from 0.5279 to 0.5147. Saving model...\n",
      "Epoch 124 - Loss: 0.5175\n",
      "Epoch 125 - Loss: 0.5224\n",
      "Epoch 126 - Loss: 0.5019\n",
      "Loss improved from 0.5147 to 0.5019. Saving model...\n",
      "Epoch 127 - Loss: 0.4945\n",
      "Loss improved from 0.5019 to 0.4945. Saving model...\n",
      "Epoch 128 - Loss: 0.4906\n",
      "Loss improved from 0.4945 to 0.4906. Saving model...\n",
      "Epoch 129 - Loss: 0.4831\n",
      "Loss improved from 0.4906 to 0.4831. Saving model...\n",
      "Epoch 130 - Loss: 0.4790\n",
      "Loss improved from 0.4831 to 0.4790. Saving model...\n",
      "Epoch 131 - Loss: 0.4766\n",
      "Loss improved from 0.4790 to 0.4766. Saving model...\n",
      "Epoch 132 - Loss: 0.4770\n",
      "Epoch 133 - Loss: 0.4728\n",
      "Loss improved from 0.4766 to 0.4728. Saving model...\n",
      "Epoch 134 - Loss: 0.4695\n",
      "Loss improved from 0.4728 to 0.4695. Saving model...\n",
      "Epoch 135 - Loss: 0.4644\n",
      "Loss improved from 0.4695 to 0.4644. Saving model...\n",
      "Epoch 136 - Loss: 0.4579\n",
      "Loss improved from 0.4644 to 0.4579. Saving model...\n",
      "Epoch 137 - Loss: 0.4551\n",
      "Loss improved from 0.4579 to 0.4551. Saving model...\n",
      "Epoch 138 - Loss: 0.4502\n",
      "Loss improved from 0.4551 to 0.4502. Saving model...\n",
      "Epoch 139 - Loss: 0.4536\n",
      "Epoch 140 - Loss: 0.4470\n",
      "Loss improved from 0.4502 to 0.4470. Saving model...\n",
      "Epoch 141 - Loss: 0.4431\n",
      "Loss improved from 0.4470 to 0.4431. Saving model...\n",
      "Epoch 142 - Loss: 0.4383\n",
      "Loss improved from 0.4431 to 0.4383. Saving model...\n",
      "Epoch 143 - Loss: 0.4391\n",
      "Epoch 144 - Loss: 0.4497\n",
      "Epoch 145 - Loss: 0.4682\n",
      "Epoch 146 - Loss: 0.4336\n",
      "Loss improved from 0.4383 to 0.4336. Saving model...\n",
      "Epoch 147 - Loss: 0.4204\n",
      "Loss improved from 0.4336 to 0.4204. Saving model...\n",
      "Epoch 148 - Loss: 0.4182\n",
      "Loss improved from 0.4204 to 0.4182. Saving model...\n",
      "Epoch 149 - Loss: 0.4143\n",
      "Loss improved from 0.4182 to 0.4143. Saving model...\n",
      "Epoch 150 - Loss: 0.4089\n",
      "Loss improved from 0.4143 to 0.4089. Saving model...\n",
      "Epoch 151 - Loss: 0.4100\n",
      "Epoch 152 - Loss: 0.4120\n",
      "Epoch 153 - Loss: 0.4057\n",
      "Loss improved from 0.4089 to 0.4057. Saving model...\n",
      "Epoch 154 - Loss: 0.4001\n",
      "Loss improved from 0.4057 to 0.4001. Saving model...\n",
      "Epoch 155 - Loss: 0.4010\n",
      "Epoch 156 - Loss: 0.3945\n",
      "Loss improved from 0.4001 to 0.3945. Saving model...\n",
      "Epoch 157 - Loss: 0.3947\n",
      "Epoch 158 - Loss: 0.3917\n",
      "Loss improved from 0.3945 to 0.3917. Saving model...\n",
      "Epoch 159 - Loss: 0.3885\n",
      "Loss improved from 0.3917 to 0.3885. Saving model...\n",
      "Epoch 160 - Loss: 0.3837\n",
      "Loss improved from 0.3885 to 0.3837. Saving model...\n",
      "Epoch 161 - Loss: 0.3829\n",
      "Loss improved from 0.3837 to 0.3829. Saving model...\n",
      "Epoch 162 - Loss: 0.3793\n",
      "Loss improved from 0.3829 to 0.3793. Saving model...\n",
      "Epoch 163 - Loss: 0.3826\n",
      "Epoch 164 - Loss: 0.3798\n",
      "Epoch 165 - Loss: 0.3780\n",
      "Loss improved from 0.3793 to 0.3780. Saving model...\n",
      "Epoch 166 - Loss: 0.3698\n",
      "Loss improved from 0.3780 to 0.3698. Saving model...\n",
      "Epoch 167 - Loss: 0.3695\n",
      "Loss improved from 0.3698 to 0.3695. Saving model...\n",
      "Epoch 168 - Loss: 0.3718\n",
      "Epoch 169 - Loss: 0.3629\n",
      "Loss improved from 0.3695 to 0.3629. Saving model...\n",
      "Epoch 170 - Loss: 0.3551\n",
      "Loss improved from 0.3629 to 0.3551. Saving model...\n",
      "Epoch 171 - Loss: 0.3523\n",
      "Loss improved from 0.3551 to 0.3523. Saving model...\n",
      "Epoch 172 - Loss: 0.3546\n",
      "Epoch 173 - Loss: 0.3523\n",
      "Epoch 174 - Loss: 0.3614\n",
      "Epoch 175 - Loss: 0.3573\n",
      "Epoch 176 - Loss: 0.3433\n",
      "Loss improved from 0.3523 to 0.3433. Saving model...\n",
      "Epoch 177 - Loss: 0.3395\n",
      "Loss improved from 0.3433 to 0.3395. Saving model...\n",
      "Epoch 178 - Loss: 0.3416\n",
      "Epoch 179 - Loss: 0.3431\n",
      "Epoch 180 - Loss: 0.3374\n",
      "Loss improved from 0.3395 to 0.3374. Saving model...\n",
      "Epoch 181 - Loss: 0.3341\n",
      "Loss improved from 0.3374 to 0.3341. Saving model...\n",
      "Epoch 182 - Loss: 0.3374\n",
      "Epoch 183 - Loss: 0.3300\n",
      "Loss improved from 0.3341 to 0.3300. Saving model...\n",
      "Epoch 184 - Loss: 0.3298\n",
      "Loss improved from 0.3300 to 0.3298. Saving model...\n",
      "Epoch 185 - Loss: 0.3238\n",
      "Loss improved from 0.3298 to 0.3238. Saving model...\n",
      "Epoch 186 - Loss: 0.3203\n",
      "Loss improved from 0.3238 to 0.3203. Saving model...\n",
      "Epoch 187 - Loss: 0.3211\n",
      "Epoch 188 - Loss: 0.3186\n",
      "Loss improved from 0.3203 to 0.3186. Saving model...\n",
      "Epoch 189 - Loss: 0.3164\n",
      "Loss improved from 0.3186 to 0.3164. Saving model...\n",
      "Epoch 190 - Loss: 0.3189\n",
      "Epoch 191 - Loss: 0.3142\n",
      "Loss improved from 0.3164 to 0.3142. Saving model...\n",
      "Epoch 192 - Loss: 0.3121\n",
      "Loss improved from 0.3142 to 0.3121. Saving model...\n",
      "Epoch 193 - Loss: 0.3066\n",
      "Loss improved from 0.3121 to 0.3066. Saving model...\n",
      "Epoch 194 - Loss: 0.3092\n",
      "Epoch 195 - Loss: 0.3024\n",
      "Loss improved from 0.3066 to 0.3024. Saving model...\n",
      "Epoch 196 - Loss: 0.2983\n",
      "Loss improved from 0.3024 to 0.2983. Saving model...\n",
      "Epoch 197 - Loss: 0.2937\n",
      "Loss improved from 0.2983 to 0.2937. Saving model...\n",
      "Epoch 198 - Loss: 0.2929\n",
      "Loss improved from 0.2937 to 0.2929. Saving model...\n",
      "Epoch 199 - Loss: 0.2959\n",
      "Epoch 200 - Loss: 0.2941\n",
      "Epoch 201 - Loss: 0.2927\n",
      "Loss improved from 0.2929 to 0.2927. Saving model...\n",
      "Epoch 202 - Loss: 0.2915\n",
      "Loss improved from 0.2927 to 0.2915. Saving model...\n",
      "Epoch 203 - Loss: 0.2879\n",
      "Loss improved from 0.2915 to 0.2879. Saving model...\n",
      "Epoch 204 - Loss: 0.2812\n",
      "Loss improved from 0.2879 to 0.2812. Saving model...\n",
      "Epoch 205 - Loss: 0.2796\n",
      "Loss improved from 0.2812 to 0.2796. Saving model...\n",
      "Epoch 206 - Loss: 0.2909\n",
      "Epoch 207 - Loss: 0.2925\n",
      "Epoch 208 - Loss: 0.2797\n",
      "Epoch 209 - Loss: 0.2769\n",
      "Loss improved from 0.2796 to 0.2769. Saving model...\n",
      "Epoch 210 - Loss: 0.2752\n",
      "Loss improved from 0.2769 to 0.2752. Saving model...\n",
      "Epoch 211 - Loss: 0.2718\n",
      "Loss improved from 0.2752 to 0.2718. Saving model...\n",
      "Epoch 212 - Loss: 0.2825\n",
      "Epoch 213 - Loss: 0.2817\n",
      "Epoch 214 - Loss: 0.2675\n",
      "Loss improved from 0.2718 to 0.2675. Saving model...\n",
      "Epoch 215 - Loss: 0.2755\n",
      "Epoch 216 - Loss: 0.2711\n",
      "Epoch 217 - Loss: 0.2668\n",
      "Loss improved from 0.2675 to 0.2668. Saving model...\n",
      "Epoch 218 - Loss: 0.2673\n",
      "Epoch 219 - Loss: 0.2616\n",
      "Loss improved from 0.2668 to 0.2616. Saving model...\n",
      "Epoch 220 - Loss: 0.2567\n",
      "Loss improved from 0.2616 to 0.2567. Saving model...\n",
      "Epoch 221 - Loss: 0.2529\n",
      "Loss improved from 0.2567 to 0.2529. Saving model...\n",
      "Epoch 222 - Loss: 0.2584\n",
      "Epoch 223 - Loss: 0.2582\n",
      "Epoch 224 - Loss: 0.2518\n",
      "Loss improved from 0.2529 to 0.2518. Saving model...\n",
      "Epoch 225 - Loss: 0.2470\n",
      "Loss improved from 0.2518 to 0.2470. Saving model...\n",
      "Epoch 226 - Loss: 0.2479\n",
      "Epoch 227 - Loss: 0.2482\n",
      "Epoch 228 - Loss: 0.2507\n",
      "Epoch 229 - Loss: 0.2457\n",
      "Loss improved from 0.2470 to 0.2457. Saving model...\n",
      "Epoch 230 - Loss: 0.2445\n",
      "Loss improved from 0.2457 to 0.2445. Saving model...\n",
      "Epoch 231 - Loss: 0.2410\n",
      "Loss improved from 0.2445 to 0.2410. Saving model...\n",
      "Epoch 232 - Loss: 0.2375\n",
      "Loss improved from 0.2410 to 0.2375. Saving model...\n",
      "Epoch 233 - Loss: 0.2349\n",
      "Loss improved from 0.2375 to 0.2349. Saving model...\n",
      "Epoch 234 - Loss: 0.2433\n",
      "Epoch 235 - Loss: 0.2397\n",
      "Epoch 236 - Loss: 0.2381\n",
      "Epoch 237 - Loss: 0.2399\n",
      "Epoch 238 - Loss: 0.2322\n",
      "Loss improved from 0.2349 to 0.2322. Saving model...\n",
      "Epoch 239 - Loss: 0.2245\n",
      "Loss improved from 0.2322 to 0.2245. Saving model...\n",
      "Epoch 240 - Loss: 0.2229\n",
      "Loss improved from 0.2245 to 0.2229. Saving model...\n",
      "Epoch 241 - Loss: 0.2273\n",
      "Epoch 242 - Loss: 0.2213\n",
      "Loss improved from 0.2229 to 0.2213. Saving model...\n",
      "Epoch 243 - Loss: 0.2209\n",
      "Loss improved from 0.2213 to 0.2209. Saving model...\n",
      "Epoch 244 - Loss: 0.2212\n",
      "Epoch 245 - Loss: 0.2194\n",
      "Loss improved from 0.2209 to 0.2194. Saving model...\n",
      "Epoch 246 - Loss: 0.2202\n",
      "Epoch 247 - Loss: 0.2174\n",
      "Loss improved from 0.2194 to 0.2174. Saving model...\n",
      "Epoch 248 - Loss: 0.2164\n",
      "Loss improved from 0.2174 to 0.2164. Saving model...\n",
      "Epoch 249 - Loss: 0.2223\n",
      "Epoch 250 - Loss: 0.2242\n",
      "Epoch 251 - Loss: 0.2137\n",
      "Loss improved from 0.2164 to 0.2137. Saving model...\n",
      "Epoch 252 - Loss: 0.2147\n",
      "Epoch 253 - Loss: 0.2124\n",
      "Loss improved from 0.2137 to 0.2124. Saving model...\n",
      "Epoch 254 - Loss: 0.2173\n",
      "Epoch 255 - Loss: 0.2128\n",
      "Epoch 256 - Loss: 0.2094\n",
      "Loss improved from 0.2124 to 0.2094. Saving model...\n",
      "Epoch 257 - Loss: 0.2052\n",
      "Loss improved from 0.2094 to 0.2052. Saving model...\n",
      "Epoch 258 - Loss: 0.2094\n",
      "Epoch 259 - Loss: 0.2098\n",
      "Epoch 260 - Loss: 0.2067\n",
      "Epoch 261 - Loss: 0.2214\n",
      "Epoch 262 - Loss: 0.2067\n",
      "Epoch 263 - Loss: 0.2028\n",
      "Loss improved from 0.2052 to 0.2028. Saving model...\n",
      "Epoch 264 - Loss: 0.1981\n",
      "Loss improved from 0.2028 to 0.1981. Saving model...\n",
      "Epoch 265 - Loss: 0.1992\n",
      "Epoch 266 - Loss: 0.2032\n",
      "Epoch 267 - Loss: 0.1970\n",
      "Loss improved from 0.1981 to 0.1970. Saving model...\n",
      "Epoch 268 - Loss: 0.1985\n",
      "Epoch 269 - Loss: 0.1928\n",
      "Loss improved from 0.1970 to 0.1928. Saving model...\n",
      "Epoch 270 - Loss: 0.1936\n",
      "Epoch 271 - Loss: 0.1910\n",
      "Loss improved from 0.1928 to 0.1910. Saving model...\n",
      "Epoch 272 - Loss: 0.1877\n",
      "Loss improved from 0.1910 to 0.1877. Saving model...\n",
      "Epoch 273 - Loss: 0.1925\n",
      "Epoch 274 - Loss: 0.1885\n",
      "Epoch 275 - Loss: 0.1900\n",
      "Epoch 276 - Loss: 0.1963\n",
      "Epoch 277 - Loss: 0.1883\n",
      "Epoch 278 - Loss: 0.1868\n",
      "Loss improved from 0.1877 to 0.1868. Saving model...\n",
      "Epoch 279 - Loss: 0.1890\n",
      "Epoch 280 - Loss: 0.1886\n",
      "Epoch 281 - Loss: 0.1872\n",
      "Epoch 282 - Loss: 0.1846\n",
      "Loss improved from 0.1868 to 0.1846. Saving model...\n",
      "Epoch 283 - Loss: 0.1781\n",
      "Loss improved from 0.1846 to 0.1781. Saving model...\n",
      "Epoch 284 - Loss: 0.1746\n",
      "Loss improved from 0.1781 to 0.1746. Saving model...\n",
      "Epoch 285 - Loss: 0.1763\n",
      "Epoch 286 - Loss: 0.1766\n",
      "Epoch 287 - Loss: 0.1810\n",
      "Epoch 288 - Loss: 0.1842\n",
      "Epoch 289 - Loss: 0.1808\n",
      "Epoch 290 - Loss: 0.1781\n",
      "Epoch 291 - Loss: 0.1746\n",
      "Epoch 292 - Loss: 0.1771\n",
      "Epoch 293 - Loss: 0.1707\n",
      "Loss improved from 0.1746 to 0.1707. Saving model...\n",
      "Epoch 294 - Loss: 0.1658\n",
      "Loss improved from 0.1707 to 0.1658. Saving model...\n",
      "Epoch 295 - Loss: 0.1677\n",
      "Epoch 296 - Loss: 0.1718\n",
      "Epoch 297 - Loss: 0.1681\n",
      "Epoch 298 - Loss: 0.1665\n",
      "Epoch 299 - Loss: 0.1653\n",
      "Loss improved from 0.1658 to 0.1653. Saving model...\n",
      "Epoch 300 - Loss: 0.1724\n",
      "Epoch 301 - Loss: 0.1691\n",
      "Epoch 302 - Loss: 0.1687\n",
      "Epoch 303 - Loss: 0.1814\n",
      "Epoch 304 - Loss: 0.1672\n",
      "Epoch 305 - Loss: 0.1621\n",
      "Loss improved from 0.1653 to 0.1621. Saving model...\n",
      "Epoch 306 - Loss: 0.1630\n",
      "Epoch 307 - Loss: 0.1622\n",
      "Epoch 308 - Loss: 0.1673\n",
      "Epoch 309 - Loss: 0.1619\n",
      "Loss improved from 0.1621 to 0.1619. Saving model...\n",
      "Epoch 310 - Loss: 0.1561\n",
      "Loss improved from 0.1619 to 0.1561. Saving model...\n",
      "Epoch 311 - Loss: 0.1528\n",
      "Loss improved from 0.1561 to 0.1528. Saving model...\n",
      "Epoch 312 - Loss: 0.1533\n",
      "Epoch 313 - Loss: 0.1549\n",
      "Epoch 314 - Loss: 0.1517\n",
      "Loss improved from 0.1528 to 0.1517. Saving model...\n",
      "Epoch 315 - Loss: 0.1519\n",
      "Epoch 316 - Loss: 0.1497\n",
      "Loss improved from 0.1517 to 0.1497. Saving model...\n",
      "Epoch 317 - Loss: 0.1510\n",
      "Epoch 318 - Loss: 0.1523\n",
      "Epoch 319 - Loss: 0.1628\n",
      "Epoch 320 - Loss: 0.1545\n",
      "Epoch 321 - Loss: 0.1558\n",
      "Epoch 322 - Loss: 0.1508\n",
      "Epoch 323 - Loss: 0.1474\n",
      "Loss improved from 0.1497 to 0.1474. Saving model...\n",
      "Epoch 324 - Loss: 0.1489\n",
      "Epoch 325 - Loss: 0.1560\n",
      "Epoch 326 - Loss: 0.1571\n",
      "Epoch 327 - Loss: 0.1523\n",
      "Epoch 328 - Loss: 0.1562\n",
      "Epoch 329 - Loss: 0.1473\n",
      "Loss improved from 0.1474 to 0.1473. Saving model...\n",
      "Epoch 330 - Loss: 0.1422\n",
      "Loss improved from 0.1473 to 0.1422. Saving model...\n",
      "Epoch 331 - Loss: 0.1418\n",
      "Loss improved from 0.1422 to 0.1418. Saving model...\n",
      "Epoch 332 - Loss: 0.1379\n",
      "Loss improved from 0.1418 to 0.1379. Saving model...\n",
      "Epoch 333 - Loss: 0.1412\n",
      "Epoch 334 - Loss: 0.1389\n",
      "Epoch 335 - Loss: 0.1462\n",
      "Epoch 336 - Loss: 0.1455\n",
      "Epoch 337 - Loss: 0.1430\n",
      "Epoch 338 - Loss: 0.1394\n",
      "Epoch 339 - Loss: 0.1381\n",
      "Epoch 340 - Loss: 0.1317\n",
      "Loss improved from 0.1379 to 0.1317. Saving model...\n",
      "Epoch 341 - Loss: 0.1301\n",
      "Loss improved from 0.1317 to 0.1301. Saving model...\n",
      "Epoch 342 - Loss: 0.1339\n",
      "Epoch 343 - Loss: 0.1388\n",
      "Epoch 344 - Loss: 0.1360\n",
      "Epoch 345 - Loss: 0.1342\n",
      "Epoch 346 - Loss: 0.1332\n",
      "Epoch 347 - Loss: 0.1345\n",
      "Epoch 348 - Loss: 0.1310\n",
      "Epoch 349 - Loss: 0.1289\n",
      "Loss improved from 0.1301 to 0.1289. Saving model...\n",
      "Epoch 350 - Loss: 0.1309\n",
      "Epoch 351 - Loss: 0.1307\n",
      "Epoch 352 - Loss: 0.1272\n",
      "Loss improved from 0.1289 to 0.1272. Saving model...\n",
      "Epoch 353 - Loss: 0.1254\n",
      "Loss improved from 0.1272 to 0.1254. Saving model...\n",
      "Epoch 354 - Loss: 0.1271\n",
      "Epoch 355 - Loss: 0.1309\n",
      "Epoch 356 - Loss: 0.1362\n",
      "Epoch 357 - Loss: 0.1251\n",
      "Loss improved from 0.1254 to 0.1251. Saving model...\n",
      "Epoch 358 - Loss: 0.1260\n",
      "Epoch 359 - Loss: 0.1231\n",
      "Loss improved from 0.1251 to 0.1231. Saving model...\n",
      "Epoch 360 - Loss: 0.1234\n",
      "Epoch 361 - Loss: 0.1319\n",
      "Epoch 362 - Loss: 0.1255\n",
      "Epoch 363 - Loss: 0.1253\n",
      "Epoch 364 - Loss: 0.1248\n",
      "Epoch 365 - Loss: 0.1225\n",
      "Loss improved from 0.1231 to 0.1225. Saving model...\n",
      "Epoch 366 - Loss: 0.1235\n",
      "Epoch 367 - Loss: 0.1217\n",
      "Loss improved from 0.1225 to 0.1217. Saving model...\n",
      "Epoch 368 - Loss: 0.1281\n",
      "Epoch 369 - Loss: 0.1326\n",
      "Epoch 370 - Loss: 0.1285\n",
      "Epoch 371 - Loss: 0.1233\n",
      "Epoch 372 - Loss: 0.1168\n",
      "Loss improved from 0.1217 to 0.1168. Saving model...\n",
      "Epoch 373 - Loss: 0.1173\n",
      "Epoch 374 - Loss: 0.1213\n",
      "Epoch 375 - Loss: 0.1222\n",
      "Epoch 376 - Loss: 0.1198\n",
      "Epoch 377 - Loss: 0.1211\n",
      "Epoch 378 - Loss: 0.1175\n",
      "Epoch 379 - Loss: 0.1159\n",
      "Loss improved from 0.1168 to 0.1159. Saving model...\n",
      "Epoch 380 - Loss: 0.1135\n",
      "Loss improved from 0.1159 to 0.1135. Saving model...\n",
      "Epoch 381 - Loss: 0.1164\n",
      "Epoch 382 - Loss: 0.1113\n",
      "Loss improved from 0.1135 to 0.1113. Saving model...\n",
      "Epoch 383 - Loss: 0.1128\n",
      "Epoch 384 - Loss: 0.1155\n",
      "Epoch 385 - Loss: 0.1126\n",
      "Epoch 386 - Loss: 0.1158\n",
      "Epoch 387 - Loss: 0.1104\n",
      "Loss improved from 0.1113 to 0.1104. Saving model...\n",
      "Epoch 388 - Loss: 0.1099\n",
      "Loss improved from 0.1104 to 0.1099. Saving model...\n",
      "Epoch 389 - Loss: 0.1097\n",
      "Loss improved from 0.1099 to 0.1097. Saving model...\n",
      "Epoch 390 - Loss: 0.1116\n",
      "Epoch 391 - Loss: 0.1127\n",
      "Epoch 392 - Loss: 0.1096\n",
      "Loss improved from 0.1097 to 0.1096. Saving model...\n",
      "Epoch 393 - Loss: 0.1120\n",
      "Epoch 394 - Loss: 0.1099\n",
      "Epoch 395 - Loss: 0.1085\n",
      "Loss improved from 0.1096 to 0.1085. Saving model...\n",
      "Epoch 396 - Loss: 0.1109\n",
      "Epoch 397 - Loss: 0.1097\n",
      "Epoch 398 - Loss: 0.1095\n",
      "Epoch 399 - Loss: 0.1090\n",
      "Epoch 400 - Loss: 0.1068\n",
      "Loss improved from 0.1085 to 0.1068. Saving model...\n",
      "Epoch 401 - Loss: 0.1189\n",
      "Epoch 402 - Loss: 0.1094\n",
      "Epoch 403 - Loss: 0.1076\n",
      "Epoch 404 - Loss: 0.1041\n",
      "Loss improved from 0.1068 to 0.1041. Saving model...\n",
      "Epoch 405 - Loss: 0.1150\n",
      "Epoch 406 - Loss: 0.1095\n",
      "Epoch 407 - Loss: 0.1091\n",
      "Epoch 408 - Loss: 0.1065\n",
      "Epoch 409 - Loss: 0.1031\n",
      "Loss improved from 0.1041 to 0.1031. Saving model...\n",
      "Epoch 410 - Loss: 0.1055\n",
      "Epoch 411 - Loss: 0.1084\n",
      "Epoch 412 - Loss: 0.1025\n",
      "Loss improved from 0.1031 to 0.1025. Saving model...\n",
      "Epoch 413 - Loss: 0.0996\n",
      "Loss improved from 0.1025 to 0.0996. Saving model...\n",
      "Epoch 414 - Loss: 0.0981\n",
      "Loss improved from 0.0996 to 0.0981. Saving model...\n",
      "Epoch 415 - Loss: 0.1015\n",
      "Epoch 416 - Loss: 0.0983\n",
      "Epoch 417 - Loss: 0.1004\n",
      "Epoch 418 - Loss: 0.0972\n",
      "Loss improved from 0.0981 to 0.0972. Saving model...\n",
      "Epoch 419 - Loss: 0.1040\n",
      "Epoch 420 - Loss: 0.1045\n",
      "Epoch 421 - Loss: 0.0993\n",
      "Epoch 422 - Loss: 0.1009\n",
      "Epoch 423 - Loss: 0.0963\n",
      "Loss improved from 0.0972 to 0.0963. Saving model...\n",
      "Epoch 424 - Loss: 0.0955\n",
      "Loss improved from 0.0963 to 0.0955. Saving model...\n",
      "Epoch 425 - Loss: 0.0983\n",
      "Epoch 426 - Loss: 0.0950\n",
      "Loss improved from 0.0955 to 0.0950. Saving model...\n",
      "Epoch 427 - Loss: 0.0941\n",
      "Loss improved from 0.0950 to 0.0941. Saving model...\n",
      "Epoch 428 - Loss: 0.0935\n",
      "Loss improved from 0.0941 to 0.0935. Saving model...\n",
      "Epoch 429 - Loss: 0.0959\n",
      "Epoch 430 - Loss: 0.0977\n",
      "Epoch 431 - Loss: 0.0939\n",
      "Epoch 432 - Loss: 0.0893\n",
      "Loss improved from 0.0935 to 0.0893. Saving model...\n",
      "Epoch 433 - Loss: 0.0917\n",
      "Epoch 434 - Loss: 0.0929\n",
      "Epoch 435 - Loss: 0.1025\n",
      "Epoch 436 - Loss: 0.1080\n",
      "Epoch 437 - Loss: 0.0980\n",
      "Epoch 438 - Loss: 0.0978\n",
      "Epoch 439 - Loss: 0.1068\n",
      "Epoch 440 - Loss: 0.0944\n",
      "Epoch 441 - Loss: 0.0923\n",
      "Epoch 442 - Loss: 0.0928\n",
      "Epoch 443 - Loss: 0.0894\n",
      "Epoch 444 - Loss: 0.0873\n",
      "Loss improved from 0.0893 to 0.0873. Saving model...\n",
      "Epoch 445 - Loss: 0.0890\n",
      "Epoch 446 - Loss: 0.0903\n",
      "Epoch 447 - Loss: 0.0920\n",
      "Epoch 448 - Loss: 0.0927\n",
      "Epoch 449 - Loss: 0.0885\n",
      "Epoch 450 - Loss: 0.0866\n",
      "Loss improved from 0.0873 to 0.0866. Saving model...\n",
      "Epoch 451 - Loss: 0.0912\n",
      "Epoch 452 - Loss: 0.0945\n",
      "Epoch 453 - Loss: 0.0903\n",
      "Epoch 454 - Loss: 0.0990\n",
      "Epoch 455 - Loss: 0.0945\n",
      "Epoch 456 - Loss: 0.0868\n",
      "Epoch 457 - Loss: 0.0888\n",
      "Epoch 458 - Loss: 0.0837\n",
      "Loss improved from 0.0866 to 0.0837. Saving model...\n",
      "Epoch 459 - Loss: 0.0841\n",
      "Epoch 460 - Loss: 0.0830\n",
      "Loss improved from 0.0837 to 0.0830. Saving model...\n",
      "Epoch 461 - Loss: 0.0828\n",
      "Loss improved from 0.0830 to 0.0828. Saving model...\n",
      "Epoch 462 - Loss: 0.0811\n",
      "Loss improved from 0.0828 to 0.0811. Saving model...\n",
      "Epoch 463 - Loss: 0.0830\n",
      "Epoch 464 - Loss: 0.0847\n",
      "Epoch 465 - Loss: 0.0862\n",
      "Epoch 466 - Loss: 0.1027\n",
      "Epoch 467 - Loss: 0.0958\n",
      "Epoch 468 - Loss: 0.0890\n",
      "Epoch 469 - Loss: 0.0830\n",
      "Epoch 470 - Loss: 0.0840\n",
      "Epoch 471 - Loss: 0.0799\n",
      "Loss improved from 0.0811 to 0.0799. Saving model...\n",
      "Epoch 472 - Loss: 0.0836\n",
      "Epoch 473 - Loss: 0.0820\n",
      "Epoch 474 - Loss: 0.0808\n",
      "Epoch 475 - Loss: 0.0792\n",
      "Loss improved from 0.0799 to 0.0792. Saving model...\n",
      "Epoch 476 - Loss: 0.0773\n",
      "Loss improved from 0.0792 to 0.0773. Saving model...\n",
      "Epoch 477 - Loss: 0.0794\n",
      "Epoch 478 - Loss: 0.0849\n",
      "Epoch 479 - Loss: 0.0959\n",
      "Epoch 480 - Loss: 0.0915\n",
      "Epoch 481 - Loss: 0.1001\n",
      "Epoch 482 - Loss: 0.0824\n",
      "Epoch 483 - Loss: 0.0786\n",
      "Epoch 484 - Loss: 0.0762\n",
      "Loss improved from 0.0773 to 0.0762. Saving model...\n",
      "Epoch 485 - Loss: 0.0774\n",
      "Epoch 486 - Loss: 0.0749\n",
      "Loss improved from 0.0762 to 0.0749. Saving model...\n",
      "Epoch 487 - Loss: 0.0750\n",
      "Epoch 488 - Loss: 0.0761\n",
      "Epoch 489 - Loss: 0.0754\n",
      "Epoch 490 - Loss: 0.0755\n",
      "Epoch 491 - Loss: 0.0763\n",
      "Epoch 492 - Loss: 0.0792\n",
      "Epoch 493 - Loss: 0.0745\n",
      "Loss improved from 0.0749 to 0.0745. Saving model...\n",
      "Epoch 494 - Loss: 0.0745\n",
      "Loss improved from 0.0745 to 0.0745. Saving model...\n",
      "Epoch 495 - Loss: 0.0745\n",
      "Loss improved from 0.0745 to 0.0745. Saving model...\n",
      "Epoch 496 - Loss: 0.0729\n",
      "Loss improved from 0.0745 to 0.0729. Saving model...\n",
      "Epoch 497 - Loss: 0.0749\n",
      "Epoch 498 - Loss: 0.0740\n",
      "Epoch 499 - Loss: 0.0750\n",
      "Epoch 500 - Loss: 0.0755\n",
      "Epoch 501 - Loss: 0.0751\n",
      "Epoch 502 - Loss: 0.0785\n",
      "Epoch 503 - Loss: 0.0763\n",
      "Epoch 504 - Loss: 0.0795\n",
      "Epoch 505 - Loss: 0.0781\n",
      "Epoch 506 - Loss: 0.0732\n",
      "Epoch 507 - Loss: 0.0738\n",
      "Epoch 508 - Loss: 0.0800\n",
      "Epoch 509 - Loss: 0.0806\n",
      "Epoch 510 - Loss: 0.0787\n",
      "Epoch 511 - Loss: 0.0784\n",
      "Epoch 512 - Loss: 0.0732\n",
      "Epoch 513 - Loss: 0.0693\n",
      "Loss improved from 0.0729 to 0.0693. Saving model...\n",
      "Epoch 514 - Loss: 0.0709\n",
      "Epoch 515 - Loss: 0.0692\n",
      "Loss improved from 0.0693 to 0.0692. Saving model...\n",
      "Epoch 516 - Loss: 0.0701\n",
      "Epoch 517 - Loss: 0.0700\n",
      "Epoch 518 - Loss: 0.0750\n",
      "Epoch 519 - Loss: 0.0878\n",
      "Epoch 520 - Loss: 0.0792\n",
      "Epoch 521 - Loss: 0.0855\n",
      "Epoch 522 - Loss: 0.0789\n",
      "Epoch 523 - Loss: 0.0794\n",
      "Epoch 524 - Loss: 0.0790\n",
      "Epoch 525 - Loss: 0.0779\n",
      "Epoch 526 - Loss: 0.0694\n",
      "Epoch 527 - Loss: 0.0685\n",
      "Loss improved from 0.0692 to 0.0685. Saving model...\n",
      "Epoch 528 - Loss: 0.0663\n",
      "Loss improved from 0.0685 to 0.0663. Saving model...\n",
      "Epoch 529 - Loss: 0.0652\n",
      "Loss improved from 0.0663 to 0.0652. Saving model...\n",
      "Epoch 530 - Loss: 0.0642\n",
      "Loss improved from 0.0652 to 0.0642. Saving model...\n",
      "Epoch 531 - Loss: 0.0678\n",
      "Epoch 532 - Loss: 0.0658\n",
      "Epoch 533 - Loss: 0.0662\n",
      "Epoch 534 - Loss: 0.0683\n",
      "Epoch 535 - Loss: 0.0705\n",
      "Epoch 536 - Loss: 0.0718\n",
      "Epoch 537 - Loss: 0.0680\n",
      "Epoch 538 - Loss: 0.0642\n",
      "Loss improved from 0.0642 to 0.0642. Saving model...\n",
      "Epoch 539 - Loss: 0.0616\n",
      "Loss improved from 0.0642 to 0.0616. Saving model...\n",
      "Epoch 540 - Loss: 0.0649\n",
      "Epoch 541 - Loss: 0.0632\n",
      "Epoch 542 - Loss: 0.0639\n",
      "Epoch 543 - Loss: 0.0627\n",
      "Epoch 544 - Loss: 0.0642\n",
      "Epoch 545 - Loss: 0.0629\n",
      "Epoch 546 - Loss: 0.0641\n",
      "Epoch 547 - Loss: 0.0634\n",
      "Epoch 548 - Loss: 0.0680\n",
      "Epoch 549 - Loss: 0.0662\n",
      "Epoch 550 - Loss: 0.0708\n",
      "Epoch 551 - Loss: 0.0650\n",
      "Epoch 552 - Loss: 0.0666\n",
      "Epoch 553 - Loss: 0.0830\n",
      "Epoch 554 - Loss: 0.0688\n",
      "Epoch 555 - Loss: 0.0623\n",
      "Epoch 556 - Loss: 0.0603\n",
      "Loss improved from 0.0616 to 0.0603. Saving model...\n",
      "Epoch 557 - Loss: 0.0615\n",
      "Epoch 558 - Loss: 0.0642\n",
      "Epoch 559 - Loss: 0.0650\n",
      "Epoch 560 - Loss: 0.0619\n",
      "Epoch 561 - Loss: 0.0609\n",
      "Epoch 562 - Loss: 0.0590\n",
      "Loss improved from 0.0603 to 0.0590. Saving model...\n",
      "Epoch 563 - Loss: 0.0590\n",
      "Epoch 564 - Loss: 0.0630\n",
      "Epoch 565 - Loss: 0.0602\n",
      "Epoch 566 - Loss: 0.0593\n",
      "Epoch 567 - Loss: 0.0586\n",
      "Loss improved from 0.0590 to 0.0586. Saving model...\n",
      "Epoch 568 - Loss: 0.0593\n",
      "Epoch 569 - Loss: 0.0616\n",
      "Epoch 570 - Loss: 0.0583\n",
      "Loss improved from 0.0586 to 0.0583. Saving model...\n",
      "Epoch 571 - Loss: 0.0606\n",
      "Epoch 572 - Loss: 0.0586\n",
      "Epoch 573 - Loss: 0.0569\n",
      "Loss improved from 0.0583 to 0.0569. Saving model...\n",
      "Epoch 574 - Loss: 0.0613\n",
      "Epoch 575 - Loss: 0.0602\n",
      "Epoch 576 - Loss: 0.0635\n",
      "Epoch 577 - Loss: 0.0650\n",
      "Epoch 578 - Loss: 0.0659\n",
      "Epoch 579 - Loss: 0.0626\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#vocab_size = len(valid_token_id_set)\n",
    "vocab_size = len(tokenizer)\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "pad_token = tokenizer.vocab.get(\"PAD_None\", -100)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "\n",
    "best_loss = float(\"inf\")  # Initialize best loss to very large value\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])  # teacher forcing\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model if there is significant improvement\n",
    "    if avg_loss < best_loss:\n",
    "        print(f\"Loss improved from {best_loss:.4f} to {avg_loss:.4f}. Saving model...\")\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), \"music_transformer_weights.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"music_transformer_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "def score_to_midi(score_tick):\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Correctly access instruments/tracks from ScoreTick\n",
    "    try:\n",
    "        for track in score_tick.tracks:  # ← this is the fix\n",
    "            midi_instr = Instrument(\n",
    "                program=track.program,\n",
    "                is_drum=track.is_drum,\n",
    "                name=track.name\n",
    "            )\n",
    "            for note in track.notes:\n",
    "                midi_instr.notes.append(Note(\n",
    "                    pitch=note.pitch,\n",
    "                    start=note.start,\n",
    "                    end=note.end,\n",
    "                    velocity=note.velocity\n",
    "                ))\n",
    "            midi.instruments.append(midi_instr)\n",
    "    except AttributeError as e:\n",
    "        raise ValueError(\"Provided object does not contain valid MIDI track info\") from e\n",
    "\n",
    "    return midi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditoolkit import MidiFile, Instrument\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    output_path,\n",
    "    max_len=1024,\n",
    "    device=\"cpu\",\n",
    "    valid_token_ids=None,  # ← NEW: pass a list or set of allowed IDs\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Ensure right_hand_tokens is a batched tensor\n",
    "    if isinstance(right_hand_tokens, list):\n",
    "        input_ids = torch.tensor([right_hand_tokens], dtype=torch.long, device=device)\n",
    "    elif isinstance(right_hand_tokens, torch.Tensor):\n",
    "        if right_hand_tokens.ndim == 1:\n",
    "            input_ids = right_hand_tokens.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            input_ids = right_hand_tokens.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"right_hand_tokens must be a list of ints or a torch.Tensor\")\n",
    "\n",
    "    bos_token_id = tokenizer.vocab.get(\"BOS_None\", tokenizer.vocab.get(\"BOS\", 0))\n",
    "    eos_token_id = tokenizer.vocab.get(\"EOS_None\", tokenizer.vocab.get(\"EOS\", -1))\n",
    "\n",
    "    decoder_input = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    #vocab_size = len(valid_token_id_set)\n",
    "\n",
    "    mask_tensor = torch.full((vocab_size,), float('-inf'), device=device)\n",
    "\n",
    "    mask_tensor[valid_token_ids] = 0.0\n",
    "\n",
    "\n",
    "    # Autoregressive generation\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_ids, decoder_input)  # (batch, seq, vocab)\n",
    "            next_token_logits = output[:, -1, :]      # (batch, vocab)\n",
    "\n",
    "            # Apply mask\n",
    "            next_token_logits = next_token_logits + mask_tensor\n",
    "\n",
    "            # Top-k sampling (top 50)\n",
    "            top_k = 50\n",
    "            top_logits, top_indices = torch.topk(next_token_logits, k=top_k, dim=-1)  # (batch, top_k)\n",
    "            probs = torch.softmax(top_logits, dim=-1)\n",
    "            sampled = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
    "            next_token = top_indices.gather(1, sampled)        # (batch, 1)\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    left_hand_tokens = decoder_input.squeeze(0).tolist()\n",
    "    left_hand_tokens[0:2] = input_ids.squeeze(0).tolist()[0:2]\n",
    "\n",
    "    # Decode to Score objects\n",
    "    print('right tokens', input_ids.squeeze(0).tolist())\n",
    "    print('left tokens', left_hand_tokens)\n",
    "    \n",
    "\n",
    "    right_score = tokenizer.decode(input_ids.squeeze(0).tolist())\n",
    "    left_score = tokenizer.decode(left_hand_tokens)\n",
    "    \n",
    "    print('right score:', right_score)\n",
    "    print('left score:', left_score)\n",
    "\n",
    "    # Convert to MIDI\n",
    "    right_midi = score_to_midi(right_score)\n",
    "    left_midi = score_to_midi(left_score)\n",
    "\n",
    "    #print(right_midi.instruments[0])\n",
    "\n",
    "\n",
    "    print('left midi', left_midi)\n",
    "    # Create MIDI\n",
    "    # Create new MIDI and combine tracks\n",
    "    midi = MidiFile()\n",
    "\n",
    "    # Append notes from decoded MIDI objects\n",
    "    for track, program, name in zip([right_midi, left_midi], [0, 0], [\"RH-1\", \"LH-1\"]):\n",
    "        inst = Instrument(program=program, is_drum=False, name=name)\n",
    "        # Take notes from the first instrument in the decoded track\n",
    "        decoded_inst = track.instruments[0]\n",
    "        inst.notes.extend(decoded_inst.notes)\n",
    "        midi.instruments.append(inst)\n",
    "\n",
    "    midi.dump(str(output_path))\n",
    "\n",
    "    print(f\"Saved MIDI to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsc/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right tokens [4, 897, 557, 868, 835, 824, 592, 888, 51, 156, 157, 594, 888, 53, 156, 157, 595, 888, 55, 156, 157, 597, 888, 56, 156, 158, 824, 607, 888, 56, 146, 157, 612, 888, 51, 146, 157, 617, 888, 53, 146, 157, 4, 897, 558, 888, 56, 146, 157, 563, 888, 53, 146, 157, 568, 888, 51, 146, 157, 571, 888, 39, 156, 157, 572, 888, 41, 156, 157, 888, 43, 156, 157, 573, 888, 44, 156, 158, 824, 583, 888, 44, 146, 157, 588, 888, 39, 146, 157, 593, 888, 41, 151, 157, 598, 888, 44, 146, 157, 603, 888, 41, 146, 157, 608, 888, 39, 146, 157, 613, 888, 39, 142, 159, 888, 44, 151, 157, 618, 888, 43, 146, 157, 4, 897, 559, 888, 44, 146, 157, 564, 888, 46, 146, 157, 569, 888, 48, 151, 158, 824, 579, 888, 48, 146, 158, 824, 589, 888, 44, 142, 157, 888, 47, 151, 157, 594, 888, 44, 138, 158, 888, 48, 146, 158, 824, 604, 888, 813, 888, 43, 144, 159, 888, 47, 144, 159, 888, 51, 154, 159, 827, 560, 888, 51, 142, 157, 562, 888, 53, 142, 157, 563, 888, 55, 142, 157, 565, 888, 48, 135, 158, 888, 56, 142, 158, 824, 575, 888, 48, 126, 157, 888, 56, 132, 157, 580, 888, 51, 132, 157, 585, 888, 48, 135, 157, 888, 53, 142, 157, 590, 888, 56, 132, 157, 595, 888, 48, 126, 157, 888, 53, 132, 157, 600, 888, 51, 132, 157, 605, 888, 48, 142, 158, 824, 615, 888, 48, 132, 157, 620, 888, 51, 132, 157, 4, 897, 561, 888, 44, 142, 159, 571, 888, 48, 126, 158, 888, 51, 126, 158, 888, 56, 132, 158, 824, 581, 888, 46, 135, 158, 888, 50, 135, 158, 888, 56, 135, 158, 888, 58, 142, 158, 824, 591, 888, 46, 135, 159, 888, 50, 135, 159, 888, 56, 135, 159, 888, 58, 142, 159, 827, 611, 888, 46, 135, 160, 888, 50, 135, 160, 888, 56, 135, 160, 888, 58, 142, 160, 825, 567, 888, 46, 132, 157, 572, 888, 50, 126, 157, 888, 53, 132, 157, 577, 888, 46, 142, 159, 587, 888, 50, 126, 158, 888, 53, 126, 158, 888, 58, 132, 158, 824, 597, 888, 51, 142, 157, 888, 61, 135, 157, 602, 888, 53, 132, 157, 888, 61, 126, 157, 607, 888, 55, 132, 157, 888, 61, 126, 157, 612, 888, 51, 132, 157, 888, 61, 126, 157, 617, 888, 53, 142, 157, 888, 61, 135, 157, 4, 897, 558, 888, 55, 132, 159, 888, 61, 126, 159, 824, 573, 888, 51, 142, 157, 888, 61, 135, 157, 578, 888, 53, 132, 157, 888, 61, 126, 157, 583, 888, 55, 132, 157, 888, 61, 126, 157, 588, 888, 51, 132, 157, 888, 61, 126, 157, 593, 888, 53, 142, 157, 888, 61, 135, 157, 598, 888, 55, 132, 159, 888, 61, 126, 159, 824, 613, 888, 44, 142, 157, 618, 888, 43, 132, 157, 4, 897, 559, 888, 44, 132, 157, 564, 888, 46, 132, 157, 569, 888, 44, 135, 158, 888, 48, 142, 158, 824, 579, 888, 44, 126, 158, 888, 48, 132, 158, 824, 589, 888, 44, 135, 157, 888, 47, 142, 157, 594, 888, 44, 126, 158, 888, 48, 132, 158, 824, 604, 888, 44, 142, 159, 827, 560, 888, 51, 142, 157, 562, 888, 53, 142, 157, 563, 888, 55, 142, 157, 565, 888, 48, 135, 158, 888, 56, 142, 158, 824, 575, 888, 48, 126, 157, 888, 56, 132, 157, 580, 888, 51, 132, 157, 585, 888, 48, 135, 157, 888, 53, 142, 157, 590, 888, 56, 132, 157, 595, 888, 48, 126, 157, 888, 53, 132, 157, 600, 888, 51, 132, 157, 605, 888, 48, 142, 158, 824, 615, 888, 48, 132, 157, 620, 888, 51, 132, 157, 4, 897, 561, 888, 44, 142, 159, 571, 888, 48, 126, 158, 888, 51, 126, 158, 888, 56, 132, 158, 824, 581, 888, 46, 135, 158, 888, 50, 135, 158, 888, 56, 135, 158, 888, 58, 142, 158, 824, 591, 888, 46, 135, 159, 888, 50, 135, 159, 888, 56, 135, 159, 888, 58, 142, 159, 827, 611, 888, 46, 135, 160, 888, 50, 135, 160, 888, 56, 135, 160, 888, 58, 142, 160, 825, 567, 888, 46, 132, 157, 572, 888, 50, 126, 157, 888, 53, 132, 157, 577, 888, 46, 142, 158, 824, 587, 888, 50, 126, 158, 888, 53, 126, 158, 888, 58, 132, 158, 824, 597, 888, 51, 142, 157, 888, 61, 135, 157, 602, 888, 53, 132, 157, 888, 61, 126, 157, 607, 888, 55, 132, 157, 888, 61, 126, 157, 612, 888, 51, 132, 157, 888, 61, 126, 157, 617, 888, 53, 142, 157, 888, 61, 135, 157, 4, 897, 558, 888, 55, 132, 159, 888, 61, 126, 159, 824, 573, 888, 51, 142, 157, 888, 61, 135, 157, 578, 888, 53, 132, 157, 888, 61, 126, 157, 583, 888, 55, 132, 157, 888, 61, 126, 157, 588, 888, 51, 132, 157, 888, 61, 126, 157, 593, 888, 53, 142, 157, 888, 61, 135, 157, 598, 888, 55, 132, 157, 888, 61, 126, 157, 603, 888, 51, 132, 157, 888, 61, 126, 157, 608, 888, 53, 132, 157, 888, 61, 126, 157, 613, 888, 55, 142, 157, 888, 61, 135, 157, 618, 888, 53, 142, 157, 888, 61, 135, 157, 4, 897, 559, 888, 51, 132, 157, 888, 61, 126, 157, 564, 888, 49, 142, 158, 888, 61, 135, 158, 824, 574, 888, 48, 132, 157, 888, 60, 126, 157, 579, 888, 46, 132, 158, 888, 58, 126, 158, 824, 589, 888, 44, 142, 158, 888, 56, 142, 158, 824, 599, 888, 811, 888, 44, 142, 158, 888, 48, 142, 158, 888, 51, 142, 158, 824, 609, 888, 41, 144, 158, 888, 48, 144, 158, 888, 51, 144, 158, 824, 619, 888, 42, 146, 158, 888, 48, 146, 158, 888, 51, 146, 158, 824, 565, 888, 43, 140, 157, 888, 50, 149, 157, 570, 888, 43, 132, 158, 888, 51, 139, 158, 824, 580, 888, 43, 136, 158, 888, 49, 144, 158, 824, 590, 888, 43, 132, 157, 888, 49, 139, 157, 595]\n",
      "left tokens [4, 897, 39, 590, 888, 43, 139, 158, 4, 897, 557, 888, 39, 139, 158, 580, 888, 36, 144, 158, 888, 48, 137, 158, 575, 888, 39, 139, 158, 611, 888, 27, 137, 158, 601, 888, 36, 132, 158, 824, 579, 888, 33, 132, 158, 888, 39, 132, 158, 589, 888, 32, 132, 158, 824, 583, 888, 42, 132, 158, 574, 888, 32, 132, 158, 824, 615, 888, 38, 132, 158, 888, 32, 132, 158, 604, 888, 32, 132, 158, 824, 567, 888, 39, 132, 158, 888, 46, 132, 158, 596, 888, 48, 132, 158, 888, 35, 132, 158, 617, 888, 30, 132, 158, 888, 36, 132, 158, 826, 610, 888, 39, 132, 158, 588, 888, 42, 132, 158, 888, 28, 123, 158, 888, 31, 132, 158, 616, 888, 41, 132, 158, 613, 888, 42, 132, 158, 824, 586, 888, 36, 132, 158, 888, 44, 132, 158, 888, 22, 132, 158, 824, 589, 888, 39, 134, 158, 888, 37, 132, 158, 593, 888, 46, 129, 158, 824, 586, 888, 37, 132, 158, 824, 588, 888, 22, 132, 158, 888, 34, 132, 158, 824, 559, 888, 44, 153, 158, 599, 888, 48, 132, 158, 888, 34, 132, 158, 603, 888, 36, 132, 158, 888, 39, 137, 158, 824, 605, 888, 44, 132, 158, 608, 888, 22, 132, 158, 824, 577, 888, 36, 132, 158, 588, 888, 41, 132, 158, 824, 605, 888, 39, 132, 158, 888, 27, 132, 158, 824, 570, 888, 44, 131, 158, 604, 888, 44, 139, 158, 601, 888, 39, 132, 158, 574, 888, 37, 153, 158, 588, 888, 24, 134, 158, 824, 578, 888, 33, 134, 158, 824, 611, 888, 36, 141, 158, 584, 888, 44, 132, 158, 824, 575, 888, 32, 126, 158, 824, 567, 888, 39, 132, 158, 888, 44, 132, 158, 568, 888, 34, 137, 158, 584, 888, 41, 137, 158, 824, 599, 888, 22, 134, 158, 888, 38, 129, 158, 824, 598, 888, 39, 137, 158, 824, 610, 888, 53, 132, 158, 824, 558, 888, 158, 824, 610, 888, 44, 132, 158, 589, 888, 27, 141, 158, 888, 44, 141, 158, 888, 41, 134, 160, 826, 559, 888, 39, 132, 158, 888, 44, 129, 158, 824, 599, 888, 25, 132, 158, 888, 39, 129, 158, 610, 888, 44, 132, 158, 824, 600, 888, 49, 132, 158, 824, 574, 888, 39, 159, 888, 39, 139, 162, 576, 888, 44, 132, 158, 888, 34, 132, 158, 888, 31, 132, 158, 888, 36, 139, 158, 830, 611, 888, 44, 132, 158, 888, 41, 139, 158, 888, 20, 132, 158, 824, 568, 888, 41, 132, 158, 888, 43, 132, 158, 888, 27, 829, 570, 888, 44, 131, 158, 888, 37, 139, 158, 824, 611, 888, 39, 139, 158, 888, 39, 129, 157, 570, 888, 22, 824, 600, 888, 39, 132, 158, 824, 596, 888, 30, 132, 158, 612, 888, 37, 152, 158, 4, 897, 580, 888, 34, 132, 158, 888, 34, 137, 158, 824, 587, 888, 27, 153, 158, 888, 31, 132, 158, 603, 888, 27, 132, 158, 598, 888, 38, 132, 158, 603, 888, 43, 132, 158, 824, 576, 888, 35, 146, 158, 824, 614, 888, 42, 132, 158, 888, 22, 132, 158, 601, 888, 36, 132, 158, 824, 578, 888, 37, 132, 158, 888, 42, 132, 158, 888, 46, 139, 158, 888, 32, 132, 158, 888, 159, 826, 604, 888, 27, 141, 158, 568, 888, 39, 132, 162, 888, 46, 129, 158, 603, 888, 44, 132, 158, 581, 888, 37, 139, 158, 888, 32, 136, 158, 824, 615, 824, 591, 888, 28, 139, 158, 824, 565, 888, 26, 132, 158, 616, 888, 39, 146, 158, 824, 614, 888, 21, 132, 158, 605, 888, 44, 129, 158, 824, 4, 897, 577, 888, 32, 144, 158, 610, 888, 37, 142, 158, 888, 26, 139, 158, 605, 827, 581, 888, 20, 132, 158, 4, 897, 560, 888, 25, 132, 158, 824, 597, 888, 29, 139, 158, 824, 612, 888, 44, 132, 158, 824, 561, 888, 37, 146, 158, 611, 888, 37, 146, 158, 824, 558, 888, 25, 151, 158, 4, 897, 566, 888, 32, 145, 158, 888, 29, 146, 158, 888, 25, 137, 158, 824, 563, 888, 34, 132, 158, 888, 27, 129, 158, 824, 591, 888, 38, 132, 158, 888, 39, 139, 158, 888, 22, 139, 158, 824, 586, 888, 37, 139, 158, 888, 43, 141, 158, 603, 888, 34, 146, 158, 596, 888, 42, 132, 158, 888, 47, 132, 158, 575, 888, 36, 134, 158, 824, 564, 888, 38, 139, 158, 888, 49, 129, 158, 888, 34, 146, 158, 824, 577, 888, 31, 129, 158, 824, 605, 888, 29, 126, 158, 824, 565, 888, 29, 149, 158, 824, 579, 888, 39, 139, 158, 618, 888, 27, 139, 158, 888, 41, 136, 158, 888, 42, 132, 158, 824, 603, 888, 41, 139, 158, 888, 26, 142, 158, 888, 30, 129, 158, 888, 810, 888, 30, 132, 158, 824, 577, 888, 39, 139, 161, 584, 888, 36, 139, 158, 824, 581, 888, 47, 132, 158, 888, 42, 137, 158, 824, 579, 888, 37, 139, 158, 824, 617, 888, 37, 134, 158, 824, 587, 888, 46, 126, 158, 888, 36, 146, 158, 824, 586, 888, 43, 139, 158, 824, 566, 888, 39, 126, 157, 569, 888, 37, 132, 158, 824, 597, 888, 41, 131, 158, 888, 36, 129, 158, 824, 614, 888, 43, 146, 158, 824, 589, 888, 31, 145, 158, 888, 43, 146, 157, 592, 888, 37, 146, 158, 888, 42, 142, 158, 888, 44, 146, 158, 824, 581, 888, 30, 139, 158, 888, 20, 133, 158, 824, 563, 888, 39, 139, 158, 888, 41, 132, 158, 824, 616, 888, 30, 150, 158, 888, 22, 146, 158, 888, 39, 146, 158, 888, 39, 146, 158, 888, 37, 146, 158, 888, 29, 126, 158, 824, 572, 888, 41, 126, 161, 605, 888, 43, 149, 158, 888, 26, 146, 158, 581, 888, 22, 144, 158, 888, 34, 130, 158, 826, 609, 888, 31, 137, 158, 824, 616, 888, 41, 137, 158, 888, 44, 146, 158, 888, 37, 146, 158, 888, 42, 139, 158, 888, 44, 139, 158, 888, 34, 141, 158, 608, 888, 32, 129, 158, 566, 888, 27, 139, 158, 824, 586, 888, 44]\n",
      "right score: Score(ttype=Tick, tpq=32, begin=0, end=1738, tracks=1, notes=210, time_sig=1, key_sig=0, markers=0)\n",
      "left score: Score(ttype=Tick, tpq=32, begin=0, end=3206, tracks=1, notes=198, time_sig=1, key_sig=0, markers=0)\n",
      "left midi ticks per beat: 480\n",
      "max tick: 0\n",
      "tempo changes: 0\n",
      "time sig: 0\n",
      "key sig: 0\n",
      "markers: 0\n",
      "lyrics: False\n",
      "instruments: 1\n",
      "Saved MIDI to generated_ragtime.mid\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "right_hand_sample = sample_batch[0][0]  # First sample of the right-hand batch\n",
    "left_hand_sample = sample_batch[1][0] \n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "#vocab_size = len(valid_token_id_set)\n",
    "\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load(Path('best_model.pth'), weights_only=True))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "# Call the generation function\n",
    "generate_left_hand_and_save_midi(\n",
    "    right_hand_tokens=right_hand_sample,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=\"generated_ragtime.mid\",\n",
    "    device=device,\n",
    "    valid_token_ids=valid_token_ids,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
